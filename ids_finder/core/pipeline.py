# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/00_ids_finder.ipynb.

# %% auto 0
__all__ = ['filter_indices', 'calc_candidate_mva_features', 'convert_to_dataframe', 'IDsPipeline', 'compress_data_by_cands',
           'sort_df', 'process_candidates', 'ids_finder', 'extract_features']

# %% ../../notebooks/00_ids_finder.ipynb 2
#| code-summary: "Import all the packages needed for the project"
from fastcore.utils import *
from fastcore.test import *
from ..utils.basic import *
import polars as pl
import xarray as xr
from .detection import *
from .propeties import *

try:
    import modin.pandas as pd
    import modin.pandas as mpd
    from modin.config import ProgressBar
    ProgressBar.enable()
except ImportError:
    import pandas as pd
import pandas
    
import numpy as np
from xarray_einstats import linalg

from datetime import timedelta

from loguru import logger

import pdpipe as pdp

from typing import Any, Callable

# %% ../../notebooks/00_ids_finder.ipynb 5
def filter_indices(
    df: pl.DataFrame | pl.LazyFrame,
    index_std_threshold=2,
    index_fluc_threshold=1,
    index_diff_threshold=0.1,
    sparse_num=15,
) -> pl.DataFrame | pl.LazyFrame:
    # filter indices to get possible IDs

    return df.filter(
        pl.col("index_std") > index_std_threshold,
        pl.col("index_fluctuation") > index_fluc_threshold,
        pl.col("index_diff") > index_diff_threshold,
        pl.col("index_std").is_finite(), # for cases where neighboring groups have std=0
        pl.col("count") > sparse_num, 
        pl.col("count_prev") > sparse_num, # filter out sparse intervals, which may give unreasonable results.
        pl.col("count_next") > sparse_num, # filter out sparse intervals, which may give unreasonable results.
    )

# %% ../../notebooks/00_ids_finder.ipynb 6
from pdpipe.util import out_of_place_col_insert

# %% ../../notebooks/00_ids_finder.ipynb 8
@patch
def _transform(self: pdp.ApplyToRows, X, verbose):
    new_cols = X.apply(self._func, axis=1)
    if isinstance(new_cols, (pd.Series, pandas.Series)):
        loc = len(X.columns)
        if self._follow_column:
            loc = X.columns.get_loc(self._follow_column) + 1
        return out_of_place_col_insert(
            X=X, series=new_cols, loc=loc, column_name=self._colname
        )
    if isinstance(new_cols, (mpd.DataFrame, pandas.DataFrame)):
        sorted_cols = sorted(list(new_cols.columns))
        new_cols = new_cols[sorted_cols]
        if self._follow_column:
            inter_X = X
            loc = X.columns.get_loc(self._follow_column) + 1
            for colname in new_cols.columns:
                inter_X = out_of_place_col_insert(
                    X=inter_X,
                    series=new_cols[colname],
                    loc=loc,
                    column_name=colname,
                )
                loc += 1
            return inter_X
        assign_map = {
            colname: new_cols[colname] for colname in new_cols.columns
        }
        return X.assign(**assign_map)
    raise TypeError(  # pragma: no cover
        "Unexpected type generated by applying a function to a DataFrame."
        " Only Series and DataFrame are allowed."
    )

# %% ../../notebooks/00_ids_finder.ipynb 9
def calc_candidate_mva_features(candidate, data: xr.DataArray):

    output_names = ["Vl_x", "Vl_y", "Vl_z", "eig0", "eig1", "eig2", 'Q_mva', 'b_mag', 'b_n', 'db_mag', 'bn_over_b', 'db_over_b', 'db_over_b_max', 'db_l', 'db_m', 'db_n']
    results = mva_features(
        data.sel(time=slice(candidate["d_tstart"], candidate["d_tstop"])).to_numpy()
    )
    
    return pandas.Series(results, output_names)

# %% ../../notebooks/00_ids_finder.ipynb 10
def convert_to_dataframe(
    data: pl.DataFrame | pl.LazyFrame # orignal Dataframe
)->pd.DataFrame:
    "convert data into a pandas/modin DataFrame"
    if isinstance(data, pl.LazyFrame):
        data = data.collect().to_pandas(use_pyarrow_extension_array=True)
    if isinstance(data, pl.DataFrame):
        data = data.to_pandas(use_pyarrow_extension_array=True)
    if not isinstance(data, pd.DataFrame):  # `modin` supports
        data = pd.DataFrame(data)
    return data

# %% ../../notebooks/00_ids_finder.ipynb 12
class IDsPipeline:
    def __init__(self):
        pass

    def calc_duration(self, sat_fgm: xr.DataArray):
        return pdp.ApplyToRows(
            lambda candidate: calc_candidate_duration(candidate, sat_fgm),
            func_desc="calculating duration parameters",
        )

    def calibrate_duration(self, sat_fgm, data_resolution):
        return pdp.ApplyToRows(
            lambda candidate: calibrate_candidate_duration(
                candidate, sat_fgm, data_resolution
            ),
            func_desc="calibrating duration parameters if needed",
        )

    def calc_mva_features(self, sat_fgm):
        return pdp.ApplyToRows(
            lambda candidate: calc_candidate_mva_features(candidate, sat_fgm),
            func_desc='calculating index "q_mva", "BnOverB" and "dBOverB"',
        )

    def calc_rotation_angle(self, sat_fgm):
        return pdp.ColByFrameFunc(
            "rotation_angle",
            lambda df: calc_candidate_rotation_angle(df, sat_fgm),
            func_desc="calculating rotation angle",
        )

# %% ../../notebooks/00_ids_finder.ipynb 14
def compress_data_by_cands(
    data: pl.DataFrame, candidates: pl.DataFrame, tau: timedelta
):
    """Compress the data for parallel processing"""
    ttstarts = candidates["tstart"] - tau
    ttstops = candidates["tstop"] + tau

    ttstarts_index = data["time"].search_sorted(ttstarts)
    ttstops_index = data["time"].search_sorted(ttstops)

    indices = np.concatenate(
        [
            np.arange(ttstart_index, ttstop_index + 1)
            for ttstart_index, ttstop_index in zip(ttstarts_index, ttstops_index)
        ]
    )  # faster than `pl.arange`
    indices_unique = (
        pl.Series(indices).unique().sort()
    )  # faster than `np.unique(index)`
    return data[indices_unique]


# %% ../../notebooks/00_ids_finder.ipynb 15
def sort_df(df: pl.DataFrame, col="time"):
    if df.get_column(col).is_sorted():
        return df.set_sorted(col)
    else:
        return df.sort(col)


def process_candidates(
    candidates_pl: pl.DataFrame,  # potential candidates DataFrame
    sat_fgm: xr.DataArray,  # satellite FGM data
    data_resolution: timedelta,  # time resolution of the data
) -> pl.DataFrame:
    "Process candidates DataFrame"
    
    candidates = convert_to_dataframe(candidates_pl)

    id_pipelines = IDsPipeline()
    candidates = id_pipelines.calc_duration(sat_fgm).apply(candidates)

    # calibrate duration
    temp_candidates = candidates.loc[
        lambda df: df["d_tstart"].isnull() | df["d_tstop"].isnull()
    ]  # temp_candidates = candidates.query('d_tstart.isnull() | d_tstop.isnull()') # not implemented in `modin`

    if not temp_candidates.empty:
        temp_candidates_updated = id_pipelines.calibrate_duration(
            sat_fgm, data_resolution
        ).apply(temp_candidates)
        candidates.update(temp_candidates_updated)

    ids = (
        id_pipelines.calc_mva_features(sat_fgm)
        + id_pipelines.calc_rotation_angle(sat_fgm)
    ).apply(
        candidates.dropna()  # Remove candidates with NaN values)
    )

    if isinstance(ids, mpd.DataFrame):
        ids = ids._to_pandas()
    if isinstance(ids, pandas.DataFrame):
        ids_pl = pl.DataFrame(ids)

    return ids_pl.pipe(sort_df, col="d_time")

# %% ../../notebooks/00_ids_finder.ipynb 17
def ids_finder(data: pl.LazyFrame, tau: float, params: dict):
    tau = timedelta(seconds=tau)
    ts = timedelta(seconds=params["time_resolution"])
    bcols = params.get("bcols", ["B_x", "B_y", "B_z"])
    data = data.sort("time").collect()

    # get candidates
    indices = compute_indices(data, tau, bcols)
    sparse_num = tau / ts // 3
    candidates = indices.pipe(filter_indices, sparse_num=sparse_num).pipe(
        pl_format_time, tau
    )

    data_c = compress_data_by_cands(data, candidates, tau)
    sat_fgm = df2ts(data_c, bcols)
    ids = process_candidates(candidates, sat_fgm, ts)
    return ids


def extract_features(
    partitioned_input: Dict[str, Callable], tau: float, params
) -> pl.DataFrame:
    ids = pl.concat(
        [
            ids_finder(partition_load(), tau, params)
            for partition_load in partitioned_input.values()
        ]
    )
    return ids.unique(["d_time", "d_tstart", "d_tstop"])
