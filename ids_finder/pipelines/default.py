# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/01_pipelines.ipynb.

# %% auto 0
__all__ = ['download_mag_data', 'load_mag_data', 'preprocess_mag_data', 'process_mag_data', 'extract_features',
           'create_mag_data_pipeline', 'download_state_data', 'load_state_data', 'preprocess_state_data',
           'process_state_data', 'create_state_data_pipeline', 'create_candidate_pipeline', 'create_pipeline',
           'combine_candidates']

# %% ../../notebooks/01_pipelines.ipynb 3
import polars as pl
from typing import Any, Dict, List, Tuple, Union

# %% ../../notebooks/01_pipelines.ipynb 4
# Kerdo
from kedro.pipeline import Pipeline, node
from kedro.pipeline.modular_pipeline import pipeline

# %% ../../notebooks/01_pipelines.ipynb 7
def download_mag_data(
    start: str = None,
    end: str = None,
    ts: str = None,  # time resolution
    probe: str = None,
    coord: str = None,
):
    """Downloading data
    """
    ...


def load_mag_data(
    start: str = None,
    end: str = None,
    ts: str = None,  # time resolution
    probe: str = None,
    coord: str = None,
):
    """Load data into a proper data structure, like dataframe.

    - Downloading data
    - Converting data structure
    """
    ...


# %% ../../notebooks/01_pipelines.ipynb 9
def preprocess_mag_data(
    raw_data: Any | pl.DataFrame = None,
    start: str = None,
    end: str = None,
    ts: str = None,  # time resolution
    coord: str = None,
) -> pl.DataFrame:
    """
    Preprocess the raw dataset (only minor transformations)

    - Applying naming conventions for columns
    - Parsing and typing data (like from string to datetime for time columns)
    - Structuring the data (like pivoting, unpivoting, etc.)
    - Changing storing format (like from `csv` to `parquet`)
    - Dropping null columns 
    - Dropping duplicate time
    - Resampling data to a given time resolution (better to do in the next stage)
    - ... other 'transformations' commonly performed at this stage.
    """
    pass


# %% ../../notebooks/01_pipelines.ipynb 11
def process_mag_data(
    raw_data: Any | pl.DataFrame,
    ts: str = None,  # time resolution
    coord: str = None,
) -> pl.DataFrame | Dict[str, pl.DataFrame]:
    """
    Corresponding to primary data layer, where source data models are transformed into domain data models

    - Transforming coordinate system if needed
    - Smoothing data
    - Resampling data to a given time resolution
    - Partitioning data, for the sake of memory
    """
    pass

def extract_features():
    pass

# %% ../../notebooks/01_pipelines.ipynb 13
def create_mag_data_pipeline(
    sat_id: str,  # satellite id, used for namespace
    ts: int = 1,  # time resolution,
    tau: str = '60s',  # time window
    **kwargs,
) -> Pipeline:
    
    ts_str = f"ts_{ts}s"
    
    node_load_data = node(
        load_mag_data,
        inputs=dict(
            start="params:start_date",
            end="params:end_date",
        ),
        outputs=f"raw_mag",
        name=f"load_{sat_id.upper()}_magnetic_field_data",
    )

    node_preprocess_data = node(
        preprocess_mag_data,
        inputs=dict(
            raw_data=f"raw_mag",
            start="params:start_date",
            end="params:end_date",
        ),
        outputs=f"inter_mag_{ts_str}",
        name=f"preprocess_{sat_id.upper()}_magnetic_field_data",
    )

    node_process_data = node(
        process_mag_data,
        inputs=f"inter_mag_{ts_str}",
        outputs=f"primary_mag_{ts_str}",
        name=f"process_{sat_id.upper()}_magnetic_field_data",
    )

    node_extract_features = node(
        extract_features,
        inputs=[f"primary_mag_{ts_str}", "params:tau", "params:extract_params"],
        outputs=f"feature_tau_{tau}",
        name=f"extract_{sat_id}_features",
    )

    nodes = [
        node_load_data,
        node_preprocess_data,
        node_process_data,
        node_extract_features,
    ]

    pipelines = pipeline(
        nodes,
        namespace=sat_id,
        parameters={
            "params:start_date": "params:jno_start_date",
            "params:end_date": "params:jno_end_date",
            "params:tau": tau,
        },
    )

    return pipelines

# %% ../../notebooks/01_pipelines.ipynb 17
def download_state_data(
    start: str = None,
    end: str = None,
    ts: str = None,  # time resolution
    probe: str = None,
    coord: str = None,
):
    ...


def load_state_data(
    start: str = None,
    end: str = None,
    ts: str = None,  # time resolution
    probe: str = None,
    coord: str = None,
):
    """Loading the raw dataset
    
    - Downloading data
    - Reading data into a proper data structure, like dataframe.
        - Parsing original data (dealing with delimiters, missing values, etc.)
    """
    ...


# %% ../../notebooks/01_pipelines.ipynb 19
def preprocess_state_data(
    raw_data: Any = None,
    start: str = None,
    end: str = None,
    ts: str = None,  # time resolution
    coord: str = None,
) -> pl.DataFrame:
    """
    Preprocess the raw dataset (only minor transformations)

    - Applying naming conventions for columns
    - Parsing and typing data (like from string to datetime for time columns)
    - Structuring the data (like pivoting, unpivoting, etc.)
    - Changing storing format (like from `csv` to `parquet`)
    - Dropping null columns
    - Resampling data to a given time resolution (better to do in the next stage)
    - ... other 'transformations' commonly performed at this stage.
    """
    pass


# %% ../../notebooks/01_pipelines.ipynb 21
def process_state_data(df: pl.DataFrame, columns=None,) -> pl.DataFrame:
    """
    Corresponding to primary data layer, where source data models are transformed into domain data models

    - Transforming data to RTN (Radial-Tangential-Normal) coordinate system
    - Discarding unnecessary columns
    - Smoothing data
    - Resampling data to a given time resolution
    - Partitioning data, for the sake of memory
    """
    pass

# %% ../../notebooks/01_pipelines.ipynb 23
def create_state_data_pipeline(
    sat_id,
    ts: str = '1h',  # time resolution
) -> Pipeline:
    
    node_load_data = node(
        load_state_data,
        inputs=dict(
            start="params:start_date",
            end="params:end_date",
        ),
        outputs="raw_state",
        name=f"download_{sat_id.upper()}_state_data",
    )
    
    node_preprocess_data = node(
        preprocess_state_data,
        inputs=dict(
            raw_data="raw_state",
            start="params:start_date",
            end="params:end_date",
        ),
        outputs=f"inter_state_{ts}",
        name=f"preprocess_{sat_id.upper()}_state_data",
    )
    
    node_process_data = node(
        process_state_data,
        inputs=f"inter_state_{ts}",
        outputs=f"primary_state_{ts}",
        name=f"process_{sat_id.upper()}_state_data",
    )

    nodes = [
        node_load_data,
        node_preprocess_data,
        node_process_data,
    ]

    pipelines = pipeline(
        nodes,
        namespace=sat_id,
        parameters={
            "params:start_date": "params:jno_start_date",
            "params:end_date": "params:jno_end_date",
        },
    )

    return pipelines

# %% ../../notebooks/01_pipelines.ipynb 25
from ..candidates import combine_features

# %% ../../notebooks/01_pipelines.ipynb 26
def create_candidate_pipeline(
    sat_id, 
    tau: int = 60,
    ts_mag: int = 1,
    ts_state: str = "1h",
    **kwargs) -> Pipeline:

    ts_mag_str = f"ts_{ts_mag}s"
    tau_str = f"tau_{tau}s"

    node_combine_features = node(
        combine_features,
        inputs=[
            f"{sat_id}.feature_{ts_mag_str}_{tau_str}",
            f"{sat_id}.primary_state_{ts_state}",
        ],
        outputs=f"candidates.{sat_id}_{ts_mag_str}_{tau_str}",
    )

    nodes = [node_combine_features]
    return pipeline(nodes)

# %% ../../notebooks/01_pipelines.ipynb 27
def create_pipeline(
    sat_id="sta",
    tau="60s",
    ts_mag="1s",  # time resolution of magnetic field data
    ts_state="1h",  # time resolution of state data
    **kwargs
) -> Pipeline:
    return (
        create_mag_data_pipeline(sat_id, ts=ts_mag, tau=tau)
        + create_state_data_pipeline(sat_id, ts=ts_state)
        + create_candidate_pipeline(sat_id, tau=tau, ts_state=ts_state)
    )

# %% ../../notebooks/01_pipelines.ipynb 29
def combine_candidates(dict):
    pass

# node_thm_extract_features = node(
#     extract_features,
#     inputs=["primary_thm_rtn_1s", "params:tau", "params:thm_1s_params"],
#     outputs="candidates_thm_rtn_1s",
#     name="extract_ARTEMIS_features",
# )

# node_combine_candidates = node(
#     combine_candidates,
#     inputs=dict(
#         sta_candidates="candidates_sta_rtn_1s",
#         jno_candidates="candidates_jno_ss_se_1s",
#         thm_candidates="candidates_thm_rtn_1s",
#     ),
#     outputs="candidates_all_1s",
#     name="combine_candidates",
# )
