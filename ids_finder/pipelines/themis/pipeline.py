# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../notebooks/02_themis.ipynb.

# %% auto 0
__all__ = ['download_mag_data', 'spz2parquet', 'preprocess_mag_data', 'process_mag_data', 'create_mag_data_pipeline',
           'download_state_data', 'preprocess_state_data', 'flow2gse', 'process_state_data',
           'create_state_data_pipeline', 'create_pipeline']

# %% ../../../notebooks/02_themis.ipynb 5
#| code-summary: import all the packages needed for the project
#| output: hide
from ...core import *
from fastcore.utils import *
from fastcore.test import *

import polars as pl
import pandas
import numpy as np
import xarray as xr


from datetime import timedelta
from loguru import logger


# %% ../../../notebooks/02_themis.ipynb 7
from kedro.pipeline import Pipeline, node
from kedro.pipeline.modular_pipeline import pipeline

# %% ../../../notebooks/02_themis.ipynb 14
def download_mag_data(
    start: str, end: str, probe: str = "b", datatype="fgs", coord="gse"
):
    import speasy as spz

    trange = [start, end]

    match probe:
        case "b":
            sat = "thb"

    product = f"cda/{sat.upper()}_L2_FGM/{sat}_{datatype}_{coord}"
    data = spz.get_data(product, trange, progress=True, disable_proxy=True)

    return data

# %% ../../../notebooks/02_themis.ipynb 16
def spz2parquet(raw_data):
    return pl.from_dataframe(raw_data.to_dataframe().reset_index()).rename({"index": "time"})


def preprocess_mag_data(
    raw_data,
    ts: str = None,  # time resolution
    coord: str = 'gse',
) -> pl.DataFrame:
    """
    Preprocess the raw dataset (only minor transformations)

    - Applying naming conventions for columns
    - Changing storing format to `parquet`
    """
    return spz2parquet(raw_data).rename({
        'Bx FGS-D': 'b_{coord}_x',
        'By FGS-D': 'b_{coord}_y',
        'Bz FGS-D': 'b_{coord}_z',
    })

# %% ../../../notebooks/02_themis.ipynb 18
from ...utils.basic import partition_data_by_year

# %% ../../../notebooks/02_themis.ipynb 19
def process_mag_data(
    raw_data: pl.DataFrame,
    ts: str = None,  # time resolution
) -> pl.DataFrame | Dict[str, pl.DataFrame]:
    """
    Partitioning data, for the sake of memory
    """
    return partition_data_by_year(raw_data)

# %% ../../../notebooks/02_themis.ipynb 21
def create_mag_data_pipeline(
    sat_id: str,  # satellite id, used for namespace
    ts: str = '4s',  # time resolution,
    tau: str = '60s',  # time window
    **kwargs,
) -> Pipeline:
    
    node_download_data = node(
        download_mag_data,
        inputs=dict(
            start="params:start_date",
            end="params:end_date",
        ),
        outputs=f"raw_mag",
        name=f"download_{sat_id.upper()}_magnetic_field_data",
    )

    node_preprocess_data = node(
        preprocess_mag_data,
        inputs=dict(
            raw_data=f"raw_mag",
        ),
        outputs=f"inter_mag_{ts}",
        name=f"preprocess_{sat_id.upper()}_magnetic_field_data",
    )

    node_process_data = node(
        process_mag_data,
        inputs=f"inter_mag_{ts}",
        outputs=f"primary_mag_{ts}",
        name=f"process_{sat_id.upper()}_magnetic_field_data",
    )

    node_extract_features = node(
        extract_features,
        inputs=[f"primary_mag_{ts}", "params:tau", "params:extract_params"],
        outputs=f"feature_tau_{tau}",
        name=f"extract_{sat_id}_features",
    )

    nodes = [
        node_download_data,
        node_preprocess_data,
        node_process_data,
        node_extract_features,
    ]

    pipelines = pipeline(
        nodes,
        namespace=sat_id,
        parameters={
            "params:start_date": "params:jno_start_date",
            "params:end_date": "params:jno_end_date",
            "params:tau": tau,
        },
    )

    return pipelines

# %% ../../../notebooks/02_themis.ipynb 25
def download_state_data(
    start: str = None,
    end: str = None,
    ts: str = None,  # time resolution
):
    import pyspedas
    
    trange = [start, end]
    files = pyspedas.omni.data(trange=trange, datatype='hour', downloadonly=True)
    return files


# %% ../../../notebooks/02_themis.ipynb 27
from ...utils.basic import cdf2pl, pmap

# %% ../../../notebooks/02_themis.ipynb 28
def preprocess_state_data(
    raw_data: List[str], # files
    vars: dict,
) -> pl.LazyFrame:
    """
    Preprocess the raw dataset (only minor transformations)

    - Applying naming conventions for columns
    - Extracting variables from `CDF` files, and convert them to DataFrame
    """
    
    columns_name_mapping = {key: value["COLNAME"] for key, value in vars.items()}
    df: pl.LazyFrame = pl.concat(raw_data | pmap(cdf2pl, var_names=list(vars)))

    return df.collect().rename(columns_name_mapping)

# %% ../../../notebooks/02_themis.ipynb 30
def flow2gse(df: pl.LazyFrame) -> pl.LazyFrame:
    """
    - Transforming data from `Quasi-GSE` coordinate to GSE coordinate system
    """
    sw_speed = pl.col("sw_speed")
    sw_theta = pl.col("sw_vel_theta")
    sw_phi = pl.col("sw_vel_phi")

    return df.with_columns(
        sw_vel_gse_x=-sw_speed * sw_theta.cos() * sw_phi.cos(),
        sw_vel_gse_y=+sw_speed * sw_theta.cos() * sw_phi.sin(),
        sw_vel_gse_z=+sw_speed * sw_theta.sin(),
    ).drop(["sw_theta", "sw_phi"])

def process_state_data(df: pl.LazyFrame) -> pl.LazyFrame:
    """
    - Transforming data to GSE coordinate system
    """

    return df.pipe(flow2gse)

# %% ../../../notebooks/02_themis.ipynb 32
def create_state_data_pipeline(
    sat_id,
    ts: str = '1h',  # time resolution
    **kwargs
) -> Pipeline:
    
    node_download_data = node(
        download_state_data,
        inputs=dict(
            start="params:start_date",
            end="params:end_date",
        ),
        outputs=f"raw_state_files",
        name=f"download_{sat_id.upper()}_state_data",
    )

    node_preprocess_data = node(
        preprocess_state_data,
        inputs=dict(
            raw_data=f"raw_state_files",
            vars="params:omni_vars",
        ),
        outputs=f"inter_state_{ts}",
        name=f"preprocess_{sat_id.upper()}_state_data",
    )
    
    node_process_data = node(
        process_state_data,
        inputs=f"inter_state_{ts}",
        outputs=f"primary_state_{ts}",
        name=f"process_{sat_id.upper()}_state_data",
    )
    
    nodes = [
        node_download_data,
        node_preprocess_data,
        node_process_data,
    ]
    pipelines = pipeline(
        nodes,
        namespace=sat_id,
        parameters={
            "params:omni_vars": "params:omni_vars",
            "params:start_date": "params:jno_start_date",
            "params:end_date": "params:jno_end_date",
        },
    )

    return pipelines

# %% ../../../notebooks/02_themis.ipynb 34
from ...candidates import create_candidate_pipeline

# %% ../../../notebooks/02_themis.ipynb 35
def create_pipeline(
    sat_id="thb",
    tau="60s",
    ts_state="1h",  # time resolution of state data
) -> Pipeline:
    return (
        create_mag_data_pipeline(sat_id, tau=tau)
        + create_state_data_pipeline(sat_id, ts=ts_state)
        + create_candidate_pipeline(sat_id, tau=tau, ts_state=ts_state)
    )
