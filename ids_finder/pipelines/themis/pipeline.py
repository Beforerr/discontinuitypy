# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../notebooks/12_themis.ipynb.

# %% auto 0
__all__ = ['download_mag_data', 'spz2df', 'load_mag_data', 'preprocess_mag_data', 'process_mag_data', 'create_mag_data_pipeline',
           'download_state_data', 'load_state_data', 'preprocess_state_data', 'preprocess_sw_state_data', 'flow2gse',
           'filter_tranges', 'add_state', 'process_state_data', 'create_state_data_pipeline', 'create_pipeline']

# %% ../../../notebooks/12_themis.ipynb 5
#| code-summary: import all the packages needed for the project
#| output: hide
from ...core import *
from fastcore.utils import *
from fastcore.test import *

import polars as pl
import pandas
import numpy as np
import xarray as xr


from datetime import timedelta
from loguru import logger


# %% ../../../notebooks/12_themis.ipynb 7
from kedro.pipeline import Pipeline, node
from kedro.pipeline.modular_pipeline import pipeline

# %% ../../../notebooks/12_themis.ipynb 14
import speasy as spz
from speasy import SpeasyVariable

# %% ../../../notebooks/12_themis.ipynb 15
def download_mag_data(
    trange, probe: str = "b", datatype="fgs", coord="gse"
) -> SpeasyVariable:
    match probe:
        case "b":
            sat = "thb"

    product = f"cda/{sat.upper()}_L2_FGM/{sat}_{datatype}_{coord}"
    data = spz.get_data(product, trange, disable_proxy=True)

    return data


def spz2df(raw_data: SpeasyVariable):
    return pl.from_dataframe(raw_data.to_dataframe().reset_index()).rename(
        {"index": "time"}
    )


def load_mag_data(start: str, end: str, probe: str = "b", datatype="fgs", coord="gse"):
    trange = [start, end]
    data = download_mag_data(trange, probe, datatype, coord)
    return spz2df(data)

# %% ../../../notebooks/12_themis.ipynb 17
def preprocess_mag_data(
    raw_data: pl.DataFrame,
    ts: str = None,  # time resolution
    coord: str = "gse",
) -> pl.DataFrame:
    """
    Preprocess the raw dataset (only minor transformations)

    - Applying naming conventions for columns
    - Dropping duplicate time
    - Changing storing format to `parquet`

    """
    name_mapping = {
        "Bx FGS-D": "BX",
        "By FGS-D": "BY",
        "Bz FGS-D": "BZ",
    }

    return raw_data.rename(name_mapping).sort("time").unique("time")

# %% ../../../notebooks/12_themis.ipynb 19
from ...utils.basic import partition_data_by_year

# %% ../../../notebooks/12_themis.ipynb 20
def process_mag_data(
    raw_data: pl.DataFrame,
    ts: str = None,  # time resolution
) -> pl.DataFrame | Dict[str, pl.DataFrame]:
    """
    Partitioning data, for the sake of memory
    """
    return partition_data_by_year(raw_data)

# %% ../../../notebooks/12_themis.ipynb 22
def create_mag_data_pipeline(
    sat_id: str,  # satellite id, used for namespace
    ts: str = '4s',  # time resolution,
    tau: str = '60s',  # time window
    **kwargs,
) -> Pipeline:
    
    node_load_data = node(
        load_mag_data,
        inputs=dict(
            start="params:start_date",
            end="params:end_date",
        ),
        outputs=f"raw_mag",
        name=f"load_{sat_id.upper()}_magnetic_field_data",
    )

    node_preprocess_data = node(
        preprocess_mag_data,
        inputs=dict(
            raw_data=f"raw_mag",
        ),
        outputs=f"inter_mag_{ts}",
        name=f"preprocess_{sat_id.upper()}_magnetic_field_data",
    )

    node_process_data = node(
        process_mag_data,
        inputs=f"inter_mag_{ts}",
        outputs=f"primary_mag_{ts}",
        name=f"process_{sat_id.upper()}_magnetic_field_data",
    )

    node_extract_features = node(
        extract_features,
        inputs=[f"primary_mag_{ts}", "params:tau", "params:extract_params"],
        outputs=f"feature_tau_{tau}",
        name=f"extract_{sat_id}_features",
    )

    nodes = [
        node_load_data,
        node_preprocess_data,
        node_process_data,
        node_extract_features,
    ]

    pipelines = pipeline(
        nodes,
        namespace=sat_id,
        parameters={
            "params:start_date": "params:jno_start_date",
            "params:end_date": "params:jno_end_date",
            "params:tau": "params:tau",
        },
    )

    return pipelines

# %% ../../../notebooks/12_themis.ipynb 29
from ...utils.basic import cdf2pl, pmap


# %% ../../../notebooks/12_themis.ipynb 30
def download_state_data(
    start: str = None,
    end: str = None,
    ts: str = None,  # time resolution
    probe: str = None,
    coord: str = None,
):
    import pyspedas
    
    trange = [start, end]
    files = pyspedas.omni.data(trange=trange, datatype='hour', downloadonly=True)
    return files


def load_state_data(
    start: str = None,
    end: str = None,
    ts: str = None,  # time resolution
    vars: dict = None,
):

    files = download_state_data(start, end, ts, vars)
    df: pl.LazyFrame = pl.concat(files | pmap(cdf2pl, var_names=list(vars)))
    return df


# %% ../../../notebooks/12_themis.ipynb 32
def preprocess_state_data(
    raw_data: pl.LazyFrame,
    vars: dict,
) -> pl.LazyFrame:
    """
    Preprocess the raw dataset (only minor transformations)

    - Applying naming conventions for columns
    - Extracting variables from `CDF` files, and convert them to DataFrame
    """

    columns_name_mapping = {key: value["COLNAME"] for key, value in vars.items()}

    return raw_data.rename(columns_name_mapping)

# %% ../../../notebooks/12_themis.ipynb 34
def preprocess_sw_state_data(
    raw_data: pandas.DataFrame,
) -> pl.LazyFrame:
    """
    - Applying naming conventions for columns
    - Parsing and typing data (like from string to datetime for time columns)
    """

    return pl.from_dataframe(raw_data).with_columns(
        # Note: For `polars`, please either specify both hour and minute, or neither.
        pl.concat_str(pl.col('start'), pl.lit(" 00")).str.to_datetime(format='%Y %j %H %M'),
        pl.concat_str(pl.col('end'), pl.lit(" 00")).str.to_datetime(format='%Y %j %H %M')
    )

# %% ../../../notebooks/12_themis.ipynb 36
def flow2gse(df: pl.LazyFrame) -> pl.LazyFrame:
    """
    - Transforming solar wind data from `Quasi-GSE` coordinate to GSE coordinate system
    """
    plasma_speed = pl.col("plasma_speed")
    sw_theta = pl.col("sw_vel_theta")
    sw_phi = pl.col("sw_vel_phi")

    return df.with_columns(
        sw_vel_gse_x=-plasma_speed * sw_theta.cos() * sw_phi.cos(),
        sw_vel_gse_y=+plasma_speed * sw_theta.cos() * sw_phi.sin(),
        sw_vel_gse_z=+plasma_speed * sw_theta.sin(),
    ).drop(["sw_theta", "sw_phi"])


def filter_tranges(time: pl.Series, tranges: Tuple[list, list]):
    """
    - Filter data by time ranges, return the indices of the time that are in the time ranges
    """

    starts = tranges[0]
    ends = tranges[1]

    start_indices = time.search_sorted(starts)
    end_indices = time.search_sorted(ends)

    return np.concatenate(
        [
            np.arange(start_index, end_index + 1)
            for start_index, end_index in zip(start_indices, end_indices)
        ]
    )


def add_state(l_df: pl.LazyFrame, l_state: pl.LazyFrame):
    state = l_state.collect()
    df = l_df.collect()

    start = state.get_column("start")
    end = state.get_column("end")

    time = df.get_column("time")
    
    indices = filter_tranges(time, (start, end))

    return (
        df.with_row_count()
        .with_columns(
            state=pl.when(pl.col("row_nr").is_in(indices)).then(1).otherwise(0)
        )
        .drop("row_nr")
    )


def process_state_data(df: pl.LazyFrame, state: pl.LazyFrame = None) -> pl.LazyFrame:
    """
    - Transforming data to GSE coordinate system
    - Combine state data with additional plasma state data
    """

    return (
        df.pipe(flow2gse)
        .pipe(add_state, state)
        .rename(
            {
                "sw_vel_gse_x": "v_x",
                "sw_vel_gse_y": "v_y",
                "sw_vel_gse_z": "v_z",
            }
        )
    )

# %% ../../../notebooks/12_themis.ipynb 38
def create_state_data_pipeline(
    sat_id,
    ts: str = "1h",  # time resolution
) -> Pipeline:
    node_load_data = node(
        load_state_data,
        inputs=dict(
            start="params:start_date",
            end="params:end_date",
            vars="params:omni_vars",
        ),
        outputs=f"raw_state",
        name=f"load_{sat_id.upper()}_state_data",
    )

    node_preprocess_data = node(
        preprocess_state_data,
        inputs=dict(
            raw_data=f"raw_state",
            vars="params:omni_vars",
        ),
        outputs=f"inter_state_{ts}",
        name=f"preprocess_{sat_id.upper()}_state_data",
    )

    node_preprocess_sw_state = node(
        preprocess_sw_state_data,
        inputs=f"raw_state_sw",
        outputs=f"inter_state_sw",
        name=f"preprocess_{sat_id.upper()}_solar_wind_state_data",
    )

    node_process_data = node(
        process_state_data,
        inputs=[f"inter_state_{ts}", f"inter_state_sw"],
        outputs=f"primary_state_{ts}",
        name=f"process_{sat_id.upper()}_state_data",
    )

    nodes = [
        node_load_data,
        node_preprocess_data,
        node_preprocess_sw_state,
        node_process_data,
    ]
    pipelines = pipeline(
        nodes,
        namespace=sat_id,
        parameters={
            "params:omni_vars": "params:omni_vars",
            "params:start_date": "params:jno_start_date",
            "params:end_date": "params:jno_end_date",
        },
    )

    return pipelines

# %% ../../../notebooks/12_themis.ipynb 41
from ...candidates import create_candidate_pipeline

# %% ../../../notebooks/12_themis.ipynb 42
def create_pipeline(
    sat_id="thb",
    tau="60s",
    ts_state="1h",  # time resolution of state data
) -> Pipeline:
    return (
        create_mag_data_pipeline(sat_id, tau=tau)
        + create_state_data_pipeline(sat_id, ts=ts_state)
        + create_candidate_pipeline(sat_id, tau=tau, ts_state=ts_state)
    )
