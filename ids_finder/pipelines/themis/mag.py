# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../notebooks/missions/themis/mag.ipynb.

# %% auto 0
__all__ = ['check_dataype', 'download_data', 'spz2df', 'load_data', 'preprocess_data', 'process_data', 'create_pipeline']

# %% ../../../notebooks/missions/themis/mag.ipynb 1
from datetime import timedelta
import polars as pl

from kedro.pipeline import Pipeline, node
from kedro.pipeline.modular_pipeline import pipeline

# %% ../../../notebooks/missions/themis/mag.ipynb 4
import speasy as spz
from speasy import SpeasyVariable

# %% ../../../notebooks/missions/themis/mag.ipynb 5
def check_dataype(ts: int):
    ts = timedelta(seconds=ts)
    fgs_ts = timedelta(seconds=3)
    fgl_ts = timedelta(seconds=0.1)

    if ts > fgs_ts:
        datatype = "fgs"
    elif ts > fgl_ts:
        datatype = "fgl"
    else:
        datatype = "fgh"
    return datatype

# %% ../../../notebooks/missions/themis/mag.ipynb 6
def download_data(
    trange, probe: str = "b", datatype="fgs", coord="gse"
) -> SpeasyVariable:
    match probe:
        case "b":
            sat = "thb"

    product = f"cda/{sat.upper()}_L2_FGM/{sat}_{datatype}_{coord}"
    data = spz.get_data(product, trange, disable_proxy=True)

    return data


def spz2df(raw_data: SpeasyVariable):
    return pl.from_dataframe(raw_data.to_dataframe().reset_index()).rename(
        {"index": "time"}
    )


def load_data(
    start: str,
    end: str,
    ts: int = None,  # time resolution
    probe: str = "b",
    coord="gse",
):
    trange = [start, end]

    datatype = check_dataype(ts)

    data = download_data(trange, probe, datatype, coord)
    return spz2df(data).lazy()

# %% ../../../notebooks/missions/themis/mag.ipynb 8
from ...utils.basic import resample

# %% ../../../notebooks/missions/themis/mag.ipynb 9
def preprocess_data(
    raw_data: pl.LazyFrame,
    datatype: str = None,
    coord: str = "gse",
) -> pl.LazyFrame:
    """
    Preprocess the raw dataset (only minor transformations)

    - Applying naming conventions for columns
    - Dropping duplicate time
    - Changing storing format to `parquet`

    """

    datatype = datatype.upper()
    name_mapping = {
        f"Bx {datatype}-D": "B_x",
        f"By {datatype}-D": "B_y",
        f"Bz {datatype}-D": "B_z",
    }

    return raw_data.sort("time").unique("time").rename(name_mapping)

# %% ../../../notebooks/missions/themis/mag.ipynb 11
from ...utils.basic import partition_data_by_year

# %% ../../../notebooks/missions/themis/mag.ipynb 12
def process_data(
    raw_data: pl.LazyFrame,
    ts: int = None,  # time resolution
) -> pl.DataFrame | dict[str, pl.DataFrame]:
    """
    Partitioning data, for the sake of memory
    """

    every = timedelta(seconds=ts)
    period = 2 * every

    return raw_data.pipe(resample, every=every, period=period).pipe(
        partition_data_by_year
    )

# %% ../../../notebooks/missions/themis/mag.ipynb 14
from ...core import extract_features

# %% ../../../notebooks/missions/themis/mag.ipynb 15
def create_pipeline(
    sat_id: str,  # satellite id, used for namespace
    ts: int = 1,  # time resolution, in seconds
    tau: str = "60s",  # time window
    **kwargs,
) -> Pipeline:

    datatype = check_dataype(
        ts
    )  # get the datatype from the time resolution, which is reasonable but not always is the case
    ts_str = f"ts_{ts}s"

    node_load_data = node(
        load_data,
        inputs=dict(
            start="params:start_date",
            end="params:end_date",
            ts="params:mag.time_resolution",
        ),
        outputs="raw_mag",
        name=f"load_{sat_id.upper()}_magnetic_field_data",
    )

    node_preprocess_data = node(
        preprocess_data,
        inputs=dict(
            raw_data="raw_mag",
            datatype=datatype,
        ),
        outputs=f"inter_mag_{datatype}",
        name=f"preprocess_{sat_id.upper()}_magnetic_field_data",
    )

    node_process_data = node(
        process_data,
        inputs=dict(
            raw_data=f"inter_mag_{datatype}",
            ts="params:mag.time_resolution",
        ),
        outputs=f"primary_mag_{ts_str}",
        name=f"process_{sat_id.upper()}_magnetic_field_data",
    )

    node_extract_features = node(
        extract_features,
        inputs=[f"primary_mag_{ts_str}", "params:tau", "params:mag"],
        outputs=f"feature_{ts_str}_tau_{tau}",
        name=f"extract_{sat_id}_features",
    )

    nodes = [
        node_load_data,
        node_preprocess_data,
        node_process_data,
        node_extract_features,
    ]

    pipelines = pipeline(
        nodes,
        namespace=sat_id,
        parameters={
            "params:start_date": "params:jno_start_date",
            "params:end_date": "params:jno_end_date",
            "params:tau": "params:tau",
        },
    )

    return pipelines
