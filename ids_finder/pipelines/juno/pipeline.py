# AUTOGENERATED! DO NOT EDIT! File to edit: ../../../notebooks/missions/01_juno.ipynb.

# %% auto 0
__all__ = ['time_resolutions', 'bcols_hgi', 'bcols_rtn', 'vcols_hgi', 'vcols_rtn', 'process_jno_index',
           'create_jno_index_pipeline', 'download_mag_data', 'load_mag_data', 'preprocess_mag_data', 'process_mag_data',
           'create_mag_data_pipeline', 'preprocess_state_data', 'hgi2rtn', 'process_state_data',
           'create_state_data_pipeline', 'create_pipeline']

# %% ../../../notebooks/missions/01_juno.ipynb 10
#| code-summary: import all the packages needed for the project
#| output: hide
from ...core import extract_features
from fastcore.utils import *
from fastcore.test import *

import polars as pl
import pandas as pd

from loguru import logger

from typing import Callable


# %% ../../../notebooks/missions/01_juno.ipynb 12
from kedro.pipeline import Pipeline, node
from kedro.pipeline.modular_pipeline import pipeline

# %% ../../../notebooks/missions/01_juno.ipynb 18
import pandas
import pdpipe as pdp

# %% ../../../notebooks/missions/01_juno.ipynb 19
def process_jno_index(df: pandas.DataFrame):
    
    _index_time_format = "%Y-%jT%H:%M:%S.%f"
    
    df.columns = df.columns.str.replace(" ", "")
    jno_index_pipeline = pdp.PdPipeline(
        [
            pdp.ColDrop(["PRODUCT_ID", "CR_DATE", "PRODUCT_LABEL_MD5CHECKSUM"]),
            pdp.ApplyByCols("SID", str.rstrip),
            pdp.ApplyByCols("FILE_SPECIFICATION_NAME", str.rstrip),
            pdp.ColByFrameFunc(
                "START_TIME",
                lambda df: pandas.to_datetime(df["START_TIME"], format=_index_time_format),
            ),
            pdp.ColByFrameFunc(
                "STOP_TIME",
                lambda df: pandas.to_datetime(df["STOP_TIME"], format=_index_time_format),
            ),
            # pdp.ApplyByCols(['START_TIME', 'STOP_TIME'], pandas.to_datetime, format=_index_time_format), # NOTE: This is slow
        ]
    )
    
    return jno_index_pipeline(df)


# %% ../../../notebooks/missions/01_juno.ipynb 21
from kedro.pipeline import pipeline, node

# %% ../../../notebooks/missions/01_juno.ipynb 22
def create_jno_index_pipeline():
    jno_index_pipeline = pipeline([
        node(process_jno_index, inputs="raw_JNO_SS_index", outputs="JNO_SS_index"),
        node(process_jno_index, inputs="raw_JNO_J_index", outputs="JNO_J_index"),
        node(lambda x1, x2: pandas.concat([x1, x2]), inputs=["JNO_SS_index", "JNO_J_index"], outputs="JNO_index")
    ])
    return jno_index_pipeline

# %% ../../../notebooks/missions/01_juno.ipynb 29
import pooch
from pooch import Unzip
from ...utils.basic import load_lbl, concat_partitions
from pipe import select, filter

# %% ../../../notebooks/missions/01_juno.ipynb 30
time_resolutions = ["1sec", "1min"]


def download_mag_data(
    start=None,
    end=None,
    ts: str = "1sec",  # time resolution
) -> list[str]:
    base_url = "https://pds-ppi.igpp.ucla.edu/ditdos/download?id=pds://PPI/JNO-SS-3-FGM-CAL-V1.0/DATA/CRUISE/SE"
    files = pooch.retrieve(
        url=f"{base_url}/{ts.upper()}",
        known_hash=None,
        path="../data/01_raw/",
        processor=Unzip(extract_dir=f"jno_ss_se_{ts}"),
    )
    return files


def load_mag_data(
    start: str | None = None,
    end: str | None = None,
    ts: str = "1sec",  # time resolution
) -> pl.DataFrame:
    files = download_mag_data(start, end, ts)

    data = pl.concat(
        files
        | filter(lambda x: x.endswith(".lbl"))
        | select(load_lbl)
        | select(pl.from_dataframe)
    )

    return data

# %% ../../../notebooks/missions/01_juno.ipynb 32
from ...utils.basic import concat_partitions

# %% ../../../notebooks/missions/01_juno.ipynb 33
def preprocess_mag_data(raw_data: pl.DataFrame) -> pl.DataFrame:
    """
    Preprocess the raw dataset (only minor transformations)

    - Applying naming conventions for columns
    - Parsing and typing data
    - Changing storing format (from `lbl` to `parquet`)
    - Dropping useless columns
    """

    df_pl = (
        raw_data
        .lazy()
        .with_columns(time=pl.col("SAMPLE UTC").str.to_datetime("%Y %j %H %M %S %f"))
        .drop(["SAMPLE UTC", "DECIMAL DAY", "INSTRUMENT RANGE"])
        .sort("time")
        .collect()
    )
    return df_pl

# %% ../../../notebooks/missions/01_juno.ipynb 35
from ...utils.basic import partition_data_by_year

# %% ../../../notebooks/missions/01_juno.ipynb 36
def process_mag_data(
    raw_data: pl.DataFrame,
    ts: str = None,  # time resolution
    coord: str = None,
) -> pl.DataFrame | Dict[str, pl.DataFrame]:
    """
    Partitioning data, for the sake of memory
    """
    return partition_data_by_year(raw_data)

# %% ../../../notebooks/missions/01_juno.ipynb 38
def create_mag_data_pipeline(
    sat_id,
    ts: str = "1s",  # time resolution,
    tau: str = "60s",  # time window
    **kwargs,
) -> Pipeline:

    node_download_data = node(
        load_mag_data,
        inputs=dict(
            start="params:start_date",
            end="params:end_date",
        ),
        outputs=f"raw_mag_files_{ts}",
        name=f"download_{sat_id.upper()}_magnetic_field_data",
    )

    node_preprocess_data = node(
        preprocess_mag_data,
        inputs=dict(
            raw_data=f"raw_mag_{ts}",
        ),
        outputs=f"inter_mag_{ts}",
        name=f"preprocess_{sat_id.upper()}_magnetic_field_data",
    )
    
    node_process_data = node(
        process_mag_data,
        inputs=f"inter_mag_{ts}",
        outputs=f"primary_mag_rtn_{ts}",
        name=f"process_{sat_id.upper()}_magnetic_field_data",
    )
    
    node_extract_features = node(
        extract_features,
        inputs=[f"primary_mag_rtn_{ts}", "params:tau", "params:extract_params"],
        outputs=f"feature_tau_{tau}",
        name=f"extract_{sat_id}_features",
    )

    nodes = [
        node_download_data,
        node_preprocess_data,
        node_process_data,
        node_extract_features,
    ]

    pipelines = pipeline(
        nodes,
        namespace=sat_id,
        parameters={
            "params:tau": "params:tau",
            "params:extract_params": "params:jno_1s_params",
            "params:start_date": "params:jno_start_date",
            "params:end_date": "params:jno_end_date",
        },
    )
    return pipelines

# %% ../../../notebooks/missions/01_juno.ipynb 42
def preprocess_state_data(
    raw_data: pandas.DataFrame,
    start: str = None,
    end: str = None,
    ts: str = None,  # time resolution
    coord: str = None,
) -> pl.DataFrame:
    """
    Preprocess the raw dataset (only minor transformations)

    - Parsing and typing data (like from string to datetime for time columns)
    - Changing storing format (like from `csv` to `parquet`)
    """
    df = (
        pl.from_pandas(raw_data)
        .lazy()
        .with_columns(
            time=pl.col("Date_Time").str.to_datetime(time_unit="ns"),
        )
        .sort("time")
        .drop(["Date_Time", "hour"])
    ).collect()
    return df

# %% ../../../notebooks/missions/01_juno.ipynb 44
bcols_hgi = ["bx", "by", "bz"]
bcols_rtn = ["b_r", "b_t", "b_n"]
vcols_hgi = ["ux", "uy", "uz"]
vcols_rtn = ["v_r", "v_t", "v_n"]


def hgi2rtn(df: pl.LazyFrame | pl.DataFrame):
    """Transform coordinates from HGI to RTN"""

    phi_rad = pl.col("phi_rad")
    ux = pl.col("ux")
    uy = pl.col("uy")
    uz = pl.col("uz")
    result = (
        df.with_columns(
            phi_rad=pl.col("phi").radians(),
        )
        .with_columns(
            b_r=pl.col("bx") * phi_rad.cos() + pl.col("by") * phi_rad.sin(),
            b_t=-pl.col("bx") * phi_rad.sin() + pl.col("by") * phi_rad.cos(),
            b_n=pl.col("bz"),
            v_r=ux * phi_rad.cos() + uy * phi_rad.sin(),
            v_t=-ux * phi_rad.sin() + uy * phi_rad.cos(),
            v_n=uz,
            plasma_speed=(ux**2 + uy**2 + uz**2).sqrt(),
        )
        .drop(["phi", "phi_rad"] + bcols_hgi + vcols_hgi)
    )
    return result

# %% ../../../notebooks/missions/01_juno.ipynb 45
def process_state_data(df: pl.DataFrame) -> pl.DataFrame:
    """
    Corresponding to primary data layer, where source data models are transformed into domain data models

    - Transforming data to RTN (Radial-Tangential-Normal) coordinate system
    - Applying naming conventions for columns
    """

    columns_name_mapping = {
        "r": "radial_distance",
        "v_r": "v_x",
        "v_t": "v_y",
        "v_n": "v_z",
        "b_r": "model_b_r",
        "b_n": "model_b_n",
        "b_t": "model_b_t",
        "Ti": "plasma_temperature",
        "rho": "plasma_density",
    }

    return df.pipe(hgi2rtn).rename(columns_name_mapping)

# %% ../../../notebooks/missions/01_juno.ipynb 47
def create_state_data_pipeline(
    sat_id,
    ts: str = '1h',  # time resolution
) -> Pipeline:
    
    node_preprocess_data = node(
        preprocess_state_data,
        inputs=dict(
            start="params:start_date",
            end="params:end_date",
            raw_data=f"raw_state",
        ),
        outputs=f"inter_state_{ts}",
        name=f"preprocess_{sat_id.upper()}_state_data",
    )
    
    node_process_data = node(
        process_state_data,
        inputs=f"inter_state_{ts}",
        outputs=f"primary_state_{ts}",
        name=f"process_{sat_id.upper()}_state_data",
    )
    
    nodes = [
        node_preprocess_data,
        node_process_data,
    ]
    pipelines = pipeline(
        nodes,
        namespace=sat_id,
        parameters={
            "params:start_date": "params:jno_start_date",
            "params:end_date": "params:jno_end_date",
        },
    )

    return pipelines

# %% ../../../notebooks/missions/01_juno.ipynb 49
from ..default import create_candidate_pipeline

# %% ../../../notebooks/missions/01_juno.ipynb 50
def create_pipeline(
    sat_id="jno",
    tau="60s",
    ts_mag="1s",  # time resolution of magnetic field data
    ts_state="1h",  # time resolution of state data
) -> Pipeline:
    return (
        create_mag_data_pipeline(sat_id, ts=ts_mag, tau=tau)
        + create_state_data_pipeline(sat_id, ts=ts_state)
        + create_candidate_pipeline(sat_id, tau=tau, ts_state=ts_state)
    )
