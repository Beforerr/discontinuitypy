# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/100_utils.ipynb.

# %% auto 0
__all__ = ['HTTP_PROTOCOLS', 'savefig', 'pmap', 'get_code', 'load_catalog', 'cdf2pl', 'pl_norm', 'partition_data_by_year',
           'concat_partitions', 'format_timedelta', 'resample', 'LblDataset', 'get_memory_usage', 'download_file',
           'check_fgm', 'col_renamer', 'df2ts', 'sat_get_fgm_from_df', 'juno_get_state', 'calc_vec_mag',
           'calc_time_diff']

# %% ../../notebooks/100_utils.ipynb 3
import matplotlib.pyplot as plt

# %% ../../notebooks/100_utils.ipynb 4
def savefig(name, **kwargs):
    plt.savefig(f"../figures/{name}.png", bbox_inches="tight", **kwargs)
    plt.savefig(f"../figures/{name}.pdf", bbox_inches="tight", **kwargs)

# %% ../../notebooks/100_utils.ipynb 6
import os
import requests

import polars as pl
import polars.selectors as cs

import pandas as pd
import xarray as xr

import pandas
import numpy as np
from xarray_einstats import linalg

from datetime import timedelta

from loguru import logger
from multipledispatch import dispatch

from xarray import DataArray
from typing import Union, Collection, Callable, Optional, Tuple
from typing import Any, Dict

# %% ../../notebooks/100_utils.ipynb 7
from pipe import select
from fastcore.utils import partial

# %% ../../notebooks/100_utils.ipynb 8
def pmap(func, *args, **kwargs):
    """
    map with `partial`
    """
    return select(partial(func, *args, **kwargs))

# %% ../../notebooks/100_utils.ipynb 9
def get_code(data, name):
    import lineapy
    "use lineapy to get code from data"
    lineapy.save(data, name)
    code = lineapy.get(name).get_code()
    print(code)
    return code

# %% ../../notebooks/100_utils.ipynb 12
from kedro.config import OmegaConfigLoader
from kedro.io import DataCatalog

def load_catalog(conf_source: str = "../conf", catalog_source: str = "catalog"):
    # Initialise a ConfigLoader
    conf_loader = OmegaConfigLoader(conf_source)

    # Load the data catalog configuration from catalog.yml
    conf_catalog = conf_loader.get(catalog_source)

    # Create the DataCatalog instance from the configuration
    catalog = DataCatalog.from_config(conf_catalog)
    
    return catalog

# %% ../../notebooks/100_utils.ipynb 14
from kedro.framework.session import KedroSession
from kedro.framework.startup import bootstrap_project
from kedro.ipython import _resolve_project_path

def load_catalog(project_path: str = '../'):
    project_path = _resolve_project_path(project_path)
    metadata = bootstrap_project(project_path)
    # configure_project(metadata.package_name)

    session = KedroSession.create(
        metadata.package_name, project_path,
    )
    context = session.load_context()
    catalog = context.catalog
    return catalog

# %% ../../notebooks/100_utils.ipynb 18
from fastcore.utils import patch

# %% ../../notebooks/100_utils.ipynb 19
import pycdfpp

# %% ../../notebooks/100_utils.ipynb 20
def cdf2pl(file_path: str, var_name: str) -> pl.LazyFrame:
    "Convert a CDF file to Polars Dataframe"
    cdf = pycdfpp.load(file_path)
    epoch_time = pycdfpp.to_datetime64(cdf["Epoch"])
    var_values = cdf[var_name].values

    # Dynamically create column names based on the shape of the field values
    columns = {
        "time": epoch_time,
        **{f"{var_name}_{i}": var_values[:, i] for i in range(var_values.shape[1])}
    }
    
    df = pl.DataFrame(columns).lazy()
    return df


# %% ../../notebooks/100_utils.ipynb 21
@patch
def plot(self:pl.DataFrame, *args, **kwargs):
    return self.to_pandas().plot(*args, **kwargs)

# %% ../../notebooks/100_utils.ipynb 22
def _expand_selectors(items: Any, *more_items: Any) -> list[Any]:
    """
    See `_expand_selectors` in `polars`.
    """
    expanded: list[Any] = []
    for item in (
        *(
            items
            if isinstance(items, Collection) and not isinstance(items, str)
            else [items]
        ),
        *more_items,
    ):
        expanded.append(item)
    return expanded

def pl_norm(columns, *more_columns) -> pl.Expr:
    """
    Computes the square root of the sum of squares for the given columns.

    Args:
    *columns (str): Names of the columns.

    Returns:
    pl.Expr: Expression representing the square root of the sum of squares.
    """
    all_columns = _expand_selectors(columns, *more_columns)
    squares = [pl.col(column).pow(2) for column in all_columns]

    return sum(squares).sqrt()

# %% ../../notebooks/100_utils.ipynb 24
def partition_data_by_year(df: pl.DataFrame) -> Dict[str, pl.DataFrame]:
    """Partition the dataset by year

    Args:
        df: Input DataFrame.

    Returns:
        Partitioned DataFrame.
    """
    return (
        df.with_columns(year=pl.col("time").dt.year().cast(pl.Utf8))
        .collect()
        .partition_by("year", include_key=False, as_dict=True)
    )

# %% ../../notebooks/100_utils.ipynb 25
def concat_partitions(partitioned_input: Dict[str, Callable]) -> pandas.DataFrame:
    """Concatenate input partitions into one pandas DataFrame.

    Args:
        partitioned_input: A dictionary with partition ids as keys and load functions as values.

    Returns:
        Pandas DataFrame representing a concatenation of all loaded partitions.
    """
    partitions_data = [
        partition_load_func() for partition_load_func in partitioned_input.values()
    ]  # load the actual partition data
    result = pandas.concat(partitions_data, ignore_index=True, sort=True)
    return result

# %% ../../notebooks/100_utils.ipynb 27
def format_timedelta(time):
    """Format timedelta to `timedelta`"""
    if isinstance(time, timedelta):
        return time
    elif isinstance(time, str):
        return pd.Timedelta(time)
    elif isinstance(time, int):
        return pd.Timedelta(seconds=time)
    else:
        raise TypeError(f"Unsupported type: {type(time)}")

def resample(
    df: pl.DataFrame | pl.LazyFrame, 
    every: timedelta | str | int, period: str | timedelta = None,
    time_column='time',
) -> pl.DataFrame | pl.LazyFrame:
    """Resample the DataFrame"""
    if period is None:
        period = every
    every = format_timedelta(every)
    period = format_timedelta(period)
    return (
        df.sort(time_column)
        .group_by_dynamic(time_column, every=every, period=period)
        .agg(cs.numeric().mean())
        .with_columns(
            (pl.col(time_column) + period / 2).dt.cast_time_unit("ns")
        )
    )

# %% ../../notebooks/100_utils.ipynb 29
from pathlib import PurePosixPath
import fsspec

from kedro.io import AbstractDataset
from kedro.io.core import get_filepath_str, get_protocol_and_path
from kedro.extras.datasets.pandas import CSVDataSet

import pdr

# %% ../../notebooks/100_utils.ipynb 30
HTTP_PROTOCOLS = ("http", "https")

class LblDataset(AbstractDataset):
    def __init__(
        self, filepath: str,
        load_type: str = 'table',
        metadata: Dict[str, Any] = None,
        ):
        # parse the path and protocol (e.g. file, http, s3, etc.)
        protocol, path = get_protocol_and_path(filepath)
        self._protocol = protocol
        self._filepath = PurePosixPath(path)
        
        self._fs = fsspec.filesystem(self._protocol)

        self.load_type = load_type
        self.metadata = metadata

    def _load(self):
        # using get_filepath_str ensures that the protocol and path are appended correctly for different filesystems
        load_path = get_filepath_str(self._filepath, self._protocol)

        if self._protocol in HTTP_PROTOCOLS:
            import pooch            
            local_fp = pooch.retrieve(load_path, known_hash=None)
        else:
            local_fp = load_path

        if self.load_type == 'table':
            df = pdr.read(local_fp).TABLE # NOTE: Support for read from url is not currently implemented.
        elif self.load_type == 'index':
            df = pandas.read_csv(local_fp, delimiter=",", quotechar='"')
            df.columns = df.columns.str.replace(" ", "")
            
        return df

    def _save(self):
        pass
    
    def _describe(self):
        """Returns a dict that describes the attributes of the dataset."""
        return dict(filepath=self._filepath, protocol=self._protocol)

# %% ../../notebooks/100_utils.ipynb 31
from humanize import naturalsize

# %% ../../notebooks/100_utils.ipynb 32
def get_memory_usage(data):
    datatype = type(data)
    match datatype:
        case pl.DataFrame:
            size = data.estimated_size()
        case pd.DataFrame:
            size = data.memory_usage().sum()
        case xr.DataArray:
            size = data.nbytes

    logger.info(f"{naturalsize(size)} ({datatype.__name__})")
    return size

# %% ../../notebooks/100_utils.ipynb 33
def download_file(url, local_dir="./", file_name=None):
    """
    Download a file from a URL and save it locally.

    Returns:
    file_path (str): Path to the downloaded file.
    """
    if file_name is None:
        file_name = url.split("/")[-1]

    file_path = os.path.join(local_dir, file_name)
    dir = os.path.dirname(file_path)
    if not os.path.isdir(dir):
        os.makedirs(dir)

    if not os.path.exists(file_path):
        logger.debug(f"Downloading from {url}")
        response = requests.get(url)
        with open(file_path, "wb") as f:
            f.write(response.content)
    return file_path

def check_fgm(vec):
    # check if time is monotonic increasing
    logger.info("Check if time is monotonic increasing")
    assert vec.time.to_series().is_monotonic_increasing
    # check available time difference
    logger.info(
        f"Available time delta: {vec.time.diff(dim='time').to_series().unique()}"
    )
    # data_array.time.diff(dim="time").plot(yscale="log")


def col_renamer(lbl: str):
    if lbl.startswith("BX"):
        return "BX"
    if lbl.startswith("BY"):
        return "BY"
    if lbl.startswith("BZ"):
        return "BZ"
    return lbl


def df2ts(
    df: Union[pandas.DataFrame, pl.DataFrame, pl.LazyFrame], cols, attrs=None, name=None
):
    for col in cols:
        if col not in df.columns:
            raise KeyError(f"Expected column {col} not found in the dataframe.")

    if isinstance(df, pl.LazyFrame):
        df = df.collect()

    # Prepare data
    data = df[cols]

    # Prepare coordinates
    time = df.index if isinstance(df, pandas.DataFrame) else df["time"]

    # Create the DataArray
    coords = {"time": time, "v_dim": cols}

    return xr.DataArray(data, coords=coords, attrs=attrs, name=name)


def sat_get_fgm_from_df(df: Union[pandas.DataFrame, pl.DataFrame, pl.LazyFrame]):
    attrs = {"coordinate_system": "se", "units": "nT"}

    return df2ts(df, cols=["BX", "BY", "BZ"], attrs=attrs, name="sat_fgm")


def juno_get_state(df: Union[pandas.DataFrame, pl.DataFrame, pl.LazyFrame]):
    attrs = {"coordinate_system": "se", "units": "km"}
    return df2ts(df, cols=["X", "Y", "Z"], attrs=attrs, name="sat_state")


def calc_vec_mag(vec) -> DataArray:
    return linalg.norm(vec, dims="v_dim")

# %% ../../notebooks/100_utils.ipynb 34
@dispatch(pl.DataFrame)
def calc_time_diff(data: pl.DataFrame): 
    return data.get_column('time').diff(null_behavior="drop").unique().sort()

@dispatch(pl.LazyFrame)
def calc_time_diff(
    data: pl.LazyFrame
) -> pl.Series: 
    return calc_time_diff(data.collect())
