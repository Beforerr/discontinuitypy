# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/utils/00_basic.ipynb.

# %% auto 0
__all__ = ['load_catalog', 'load_params', 'DF_TYPE', 'pmap', 'load_context', 'DataConfig', 'filter_tranges', 'filter_tranges_df',
           'pl_norm', 'partition_data_by_year', 'concat_df', 'concat_partitions', 'format_timedelta', 'resample',
           'get_memory_usage', 'check_fgm', 'col_renamer', 'df2ts', 'calc_vec_mag', 'calc_time_diff']

# %% ../../notebooks/utils/00_basic.ipynb 1
from .. import ROOT_DIR

from .cdf import cdf2pl

# %% ../../notebooks/utils/00_basic.ipynb 3
import polars as pl
import polars.selectors as cs

import pandas as pd
import xarray as xr

import pandas
import numpy as np
from xarray_einstats import linalg

from datetime import timedelta

from loguru import logger
from multipledispatch import dispatch

from xarray import DataArray
from typing import Union, Collection, Callable, Optional, Tuple
from typing import Any, Dict


# %% ../../notebooks/utils/00_basic.ipynb 4
from pipe import select
from fastcore.utils import partial

# %% ../../notebooks/utils/00_basic.ipynb 5
def pmap(func, *args, **kwargs):
    """
    map with `partial`
    """
    return select(partial(func, *args, **kwargs))

# %% ../../notebooks/utils/00_basic.ipynb 11
from kedro.framework.session import KedroSession
from kedro.framework.startup import bootstrap_project
from kedro.ipython import _resolve_project_path

def load_context(project_path: str = ROOT_DIR, params_only: bool = False, catalog_only: bool = False):
    project_path = _resolve_project_path(project_path)
    metadata = bootstrap_project(project_path)
    # configure_project(metadata.package_name)

    session = KedroSession.create(
        metadata.package_name, project_path,
    )
    context = session.load_context()

    if params_only:
        return context.params
    if catalog_only:
        return context.catalog
    else:
        return context

load_catalog = partial(load_context, catalog_only=True)
load_params = partial(load_context, params_only=True)

# %% ../../notebooks/utils/00_basic.ipynb 14
from pydantic import BaseModel
from datetime import datetime, timedelta
from pandas import Timedelta

class DataConfig(BaseModel):
    sat_id: str = None
    start: datetime = None
    end: datetime = None
    ts: timedelta = None
    coord: str = None

# %% ../../notebooks/utils/00_basic.ipynb 16
from fastcore.utils import patch

# %% ../../notebooks/utils/00_basic.ipynb 17
def filter_tranges(time: pl.Series, tranges: Tuple[list, list]):
    """
    - Filter data by time ranges, return the indices of the time that are in the time ranges (left inclusive, right exclusive)
    """

    starts = tranges[0]
    ends = tranges[1]

    start_indices = time.search_sorted(starts)
    end_indices = time.search_sorted(ends)

    return np.concatenate(
        [
            np.arange(start_index, end_index)
            for start_index, end_index in zip(start_indices, end_indices)
        ]
    )

def filter_tranges_df(df: pl.DataFrame, tranges: Tuple[list, list], time_col: str = "time"):
    """
    - Filter data by time ranges
    """

    time = df[time_col]
    filtered_indices = filter_tranges(time, tranges)
    return df[filtered_indices]

# %% ../../notebooks/utils/00_basic.ipynb 18
@patch
def plot(self:pl.DataFrame, *args, **kwargs):
    return self.to_pandas().plot(*args, **kwargs)

# %% ../../notebooks/utils/00_basic.ipynb 19
def _expand_selectors(items: Any, *more_items: Any) -> list[Any]:
    """
    See `_expand_selectors` in `polars`.
    """
    expanded: list[Any] = []
    for item in (
        *(
            items
            if isinstance(items, Collection) and not isinstance(items, str)
            else [items]
        ),
        *more_items,
    ):
        expanded.append(item)
    return expanded

def pl_norm(columns, *more_columns) -> pl.Expr:
    """
    Computes the square root of the sum of squares for the given columns.

    Args:
    *columns (str): Names of the columns.

    Returns:
    pl.Expr: Expression representing the square root of the sum of squares.
    """
    all_columns = _expand_selectors(columns, *more_columns)
    squares = [pl.col(column).pow(2) for column in all_columns]

    return sum(squares).sqrt()

# %% ../../notebooks/utils/00_basic.ipynb 21
def partition_data_by_year(df: pl.LazyFrame) -> Dict[str, pl.DataFrame]:
    """Partition the dataset by year

    Args:
        df: Input DataFrame.

    Returns:
        Partitioned DataFrame.
    """
    return (
        df.with_columns(year=pl.col("time").dt.year().cast(pl.Utf8))
        .collect()
        .partition_by("year", include_key=False, as_dict=True)
    )

# %% ../../notebooks/utils/00_basic.ipynb 22
DF_TYPE = Union[pl.DataFrame, pl.LazyFrame, pd.DataFrame]
def concat_df(dfs: list[DF_TYPE]) -> DF_TYPE:
    """Concatenate a list of DataFrames into one DataFrame.
    """
    
    match type(dfs[0]):
        case pl.DataFrame | pl.LazyFrame:
            concat_func = pl.concat
        case pandas.DataFrame:
            concat_func = pandas.concat
        case _:
            raise ValueError(f"Unsupported DataFrame type: {type(dfs[0])}")
    
    return concat_func(dfs)
                     
def concat_partitions(partitioned_input: Dict[str, Callable]):
    """Concatenate input partitions into one DataFrame.

    Args:
        partitioned_input: A dictionary with partition ids as keys and load functions as values.
    """
    partitions_data = [
        partition_load_func() for partition_load_func in partitioned_input.values()
    ]  # load the actual partition data
    
    result = concat_df(partitions_data)
    return result

# %% ../../notebooks/utils/00_basic.ipynb 24
def format_timedelta(time):
    """Format timedelta to `timedelta`"""
    if isinstance(time, timedelta):
        return time
    elif isinstance(time, str):
        return pd.Timedelta(time)
    elif isinstance(time, int):
        return pd.Timedelta(seconds=time)
    else:
        raise TypeError(f"Unsupported type: {type(time)}")

def resample(
    df: pl.DataFrame | pl.LazyFrame, 
    every: timedelta | str | int, period: str | timedelta = None,
    time_column='time',
) -> pl.DataFrame | pl.LazyFrame:
    """Resample the DataFrame"""
    if period is None:
        period = every
    every = format_timedelta(every)
    period = format_timedelta(period)
    return (
        df.sort(time_column)
        .group_by_dynamic(time_column, every=every, period=period)
        .agg(cs.numeric().mean())
        .with_columns(
            (pl.col(time_column) + period / 2).dt.cast_time_unit("ns")
        )
    )

# %% ../../notebooks/utils/00_basic.ipynb 25
from humanize import naturalsize

# %% ../../notebooks/utils/00_basic.ipynb 26
def get_memory_usage(data):
    datatype = type(data)
    match datatype:
        case pl.DataFrame:
            size = data.estimated_size()
        case pd.DataFrame:
            size = data.memory_usage().sum()
        case xr.DataArray:
            size = data.nbytes

    logger.info(f"{naturalsize(size)} ({datatype.__name__})")
    return size

# %% ../../notebooks/utils/00_basic.ipynb 27
def check_fgm(vec: xr.DataArray):
    # check if time is monotonic increasing
    logger.info("Check if time is monotonic increasing")
    assert vec.time.to_series().is_monotonic_increasing
    # check available time difference
    logger.info(
        f"Available time delta: {vec.time.diff(dim='time').to_series().unique()}"
    )


def col_renamer(lbl: str):
    if lbl.startswith("BX"):
        return "BX"
    if lbl.startswith("BY"):
        return "BY"
    if lbl.startswith("BZ"):
        return "BZ"
    return lbl


def df2ts(
    df: Union[pandas.DataFrame, pl.DataFrame, pl.LazyFrame], cols, attrs=None, name=None
):
    for col in cols:
        if col not in df.columns:
            raise KeyError(f"Expected column {col} not found in the dataframe.")

    if isinstance(df, pl.LazyFrame):
        df = df.collect()

    # Prepare data
    data = df[cols]

    # Prepare coordinates
    time = df.index if isinstance(df, pandas.DataFrame) else df["time"]

    # Create the DataArray
    coords = {"time": time, "v_dim": cols}

    return xr.DataArray(data, coords=coords, attrs=attrs, name=name)


def calc_vec_mag(vec) -> DataArray:
    return linalg.norm(vec, dims="v_dim")

# %% ../../notebooks/utils/00_basic.ipynb 28
@dispatch(pl.DataFrame)
def calc_time_diff(data: pl.DataFrame): 
    return data.get_column('time').diff(null_behavior="drop").unique().sort()

@dispatch(pl.LazyFrame)
def calc_time_diff(
    data: pl.LazyFrame
) -> pl.Series: 
    return calc_time_diff(data.collect())
