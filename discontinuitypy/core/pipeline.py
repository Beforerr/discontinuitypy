# AUTOGENERATED! DO NOT EDIT! File to edit: ../../notebooks/00_ids_finder.ipynb.

# %% auto 0
__all__ = ['compress_data_by_intervals', 'compress_dfs_by_intervals', 'compress_data_by_events', 'ids_finder', 'extract_features']

# %% ../../notebooks/00_ids_finder.ipynb 2
#| code-summary: "Import all the packages needed for the project"
import polars as pl
from .propeties import process_events
from .detection import detect_events
from ..utils.basic import df2ts
import numpy as np

from datetime import timedelta

from typing import Callable

# %% ../../notebooks/00_ids_finder.ipynb 5
def compress_data_by_intervals(
    data: pl.DataFrame, starts: list, stops: list
):
    """Compress the data for parallel processing"""

    starts_index = data["time"].search_sorted(starts)
    ends_index = data["time"].search_sorted(stops)

    ranges = [
        np.arange(start_index, end_index + 1)
        for start_index, end_index in zip(starts_index, ends_index)
        if start_index < end_index
    ]

    if not ranges:  # If `ranges` is empty
        # Handle the case, e.g., return an empty dataframe or subset of the original dataframe
        return data.filter(pl.lit(False))  # This will return an empty dataframe with the same columns
    else:
        indices = np.concatenate(ranges)  # faster than `pl.arange`

    indices_unique = (
        pl.Series(indices).unique().sort()
    )  # faster than `np.unique(index)`
    return data[indices_unique]

def compress_dfs_by_intervals(
    dfs: list[pl.LazyFrame], starts: list, stops: list
):
    """Compress dataframes for parallel processing"""
    return pl.concat(compress_data_by_intervals(df.collect(), starts, stops) for df in dfs)

def compress_data_by_events(
    data: pl.DataFrame, events: pl.DataFrame
):
    """Compress the data for parallel processing"""
    starts = events["tstart"]
    ends = events["tstop"]
    return compress_data_by_intervals(data, starts, ends)

# %% ../../notebooks/00_ids_finder.ipynb 6
def ids_finder(
    detection_df: pl.LazyFrame, # data used for anomaly dectection (typically low cadence data)
    tau: timedelta,
    ts: timedelta, 
    bcols = None,
    extract_df: pl.LazyFrame = None, # data used for feature extraction (typically high cadence data),
    **kwargs
):
    if extract_df is None:
        extract_df = detection_df
    if bcols is None:
        bcols = detection_df.columns
        bcols.remove("time")
    
    detection_df = detection_df.sort("time").with_columns(pl.col("time").dt.cast_time_unit("us")) # https://github.com/pola-rs/polars/issues/12023
    extract_df = extract_df.sort("time").with_columns(pl.col("time").dt.cast_time_unit("us"))

    events = detect_events(detection_df, tau, ts, bcols, **kwargs)
    
    data_c = compress_data_by_events(extract_df.collect(), events)
    sat_fgm = df2ts(data_c, bcols)
    ids = process_events(events, sat_fgm, ts, **kwargs)
    return ids

# %% ../../notebooks/00_ids_finder.ipynb 8
def extract_features(
    partitioned_input: dict[str, Callable[..., pl.LazyFrame]],
    tau: float,  # in seconds, yaml input
    ts: float,  # in seconds, yaml input
    **kwargs,
) -> pl.DataFrame:
    "wrapper function for partitioned input"

    _tau = timedelta(seconds=tau)
    _ts = timedelta(seconds=ts)

    ids = pl.concat(
        [
            ids_finder(partition_load(), _tau, _ts, **kwargs)
            for partition_load in partitioned_input.values()
        ]
    )
    return ids.unique(["d_time", "t.d_start", "t.d_end"])
