{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title:  ID identification\n",
    "subtitle: limited feature extraction / anomaly detection\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core/detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from datetime import timedelta\n",
    "import polars as pl\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "from ids_finder.utils.basic import *\n",
    "\n",
    "from multipledispatch import dispatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first index is $$ \\frac{\\sigma(B)}{Max(\\sigma(B_-),\\sigma(B_+))} $$\n",
    "The second index is $$ \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)} $$\n",
    "The ﬁrst two conditions guarantee that the ﬁeld changes of the IDs identiﬁed are large enough to be distinguished from the stochastic ﬂuctuations on magnetic ﬁelds, while the third is a supplementary condition toreduce the uncertainty of recognition.\n",
    "\n",
    "third index (relative field jump) is $$ \\frac{| \\Delta \\vec{B} |}{|B_{bg}|} $$ a supplementary condition to reduce the uncertainty of recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from ids_finder.utils.basic import _expand_selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# some helper functions\n",
    "def pl_format_time(df: pl.LazyFrame | pl.DataFrame, tau: timedelta):\n",
    "    return df.with_columns(\n",
    "        tstart=pl.col(\"time\"),\n",
    "        tstop=(pl.col(\"time\") + tau).dt.cast_time_unit(\"ns\"),\n",
    "        time=(pl.col(\"time\") + tau / 2).dt.cast_time_unit(\"ns\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def pl_dvec(columns, *more_columns):\n",
    "    all_columns = _expand_selectors(columns, *more_columns)\n",
    "    return [\n",
    "        (pl.col(column).first() - pl.col(column).last()).alias(f\"d{column}_vec\")\n",
    "        for column in all_columns\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compute_std(\n",
    "    df: pl.DataFrame | pl.LazyFrame, tau, b_cols=[\"BX\", \"BY\", \"BZ\"]\n",
    ") -> pl.DataFrame:\n",
    "    b_std_cols = [col_name + \"_std\" for col_name in b_cols]\n",
    "\n",
    "    std_df = (\n",
    "        df.group_by_dynamic(\"time\", every=tau / 2, period=tau)\n",
    "        .agg(\n",
    "            pl.count(),\n",
    "            pl.col(b_cols).std(ddof=0).map_alias(lambda col_name: col_name + \"_std\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl_norm(b_std_cols).alias(\"B_std\"),\n",
    "        )\n",
    "        .drop(b_std_cols)\n",
    "    )\n",
    "    return std_df\n",
    "\n",
    "\n",
    "def compute_combinded_std(\n",
    "    df: pl.DataFrame | pl.LazyFrame, tau, cols\n",
    ") -> pl.DataFrame:\n",
    "    combined_std_cols = [col_name + \"_combined_std\" for col_name in cols]\n",
    "    offsets = [0 * tau, tau / 2]\n",
    "    combined_std_dfs = []\n",
    "    for offset in offsets:\n",
    "        truncated_df = df.select(\n",
    "            (pl.col(\"time\") - offset).dt.truncate(tau, offset=offset).alias(\"time\"),\n",
    "            pl.col(cols),\n",
    "        )\n",
    "\n",
    "        prev_df = truncated_df.select(\n",
    "            (pl.col(\"time\") + tau).dt.cast_time_unit(\"ns\"),\n",
    "            pl.col(cols),\n",
    "        )\n",
    "\n",
    "        next_df = truncated_df.select(\n",
    "            (pl.col(\"time\") - tau).dt.cast_time_unit(\"ns\"),\n",
    "            pl.col(cols),\n",
    "        )\n",
    "\n",
    "        temp_combined_std_df = (\n",
    "            pl.concat([prev_df, next_df])\n",
    "            .group_by(\"time\")\n",
    "            .agg(\n",
    "                pl.col(cols)\n",
    "                .std(ddof=0)\n",
    "                .map_alias(lambda col_name: col_name + \"_combined_std\"),\n",
    "            )\n",
    "            .with_columns(pl_norm(combined_std_cols).alias(\"B_combined_std\"))\n",
    "            .drop(combined_std_cols)\n",
    "            .sort(\"time\")\n",
    "        )\n",
    "\n",
    "        combined_std_dfs.append(temp_combined_std_df)\n",
    "\n",
    "    combined_std_df = pl.concat(combined_std_dfs)\n",
    "    return combined_std_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dispatch(pl.LazyFrame, object)\n",
    "def compute_index_std(df: pl.LazyFrame, tau, join_strategy=\"inner\"):  # noqa: F811\n",
    "    \"\"\"\n",
    "    Compute the standard deviation index based on the given DataFrame and tau value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - df (pl.LazyFrame): The input DataFrame.\n",
    "    - tau (int): The time interval value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - pl.LazyFrame: DataFrame with calculated 'index_std' column.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> index_std_df = compute_index_std_pl(df, tau)\n",
    "    >>> index_std_df\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Simply shift to calculate index_std would not work correctly if data is missing, like `std_next = pl.col(\"B_std\").shift(-2)`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(tau, (int, float)):\n",
    "        tau = timedelta(seconds=tau)\n",
    "\n",
    "    if \"B_std\" in df.columns:\n",
    "        std_df = df\n",
    "    else:\n",
    "        # Compute standard deviations\n",
    "        std_df = compute_std(df, tau)\n",
    "\n",
    "    # Calculate the standard deviation index\n",
    "    prev_std_df = std_df.select(\n",
    "        (pl.col(\"time\") + tau).dt.cast_time_unit(\"ns\"),\n",
    "        pl.col(\"B_std\").alias(\"B_std_prev\"),\n",
    "        pl.col(\"count\").alias(\"count_prev\"),\n",
    "    )\n",
    "\n",
    "    next_std_df = std_df.select(\n",
    "        (pl.col(\"time\") - tau).dt.cast_time_unit(\"ns\"),\n",
    "        pl.col(\"B_std\").alias(\"B_std_next\"),\n",
    "        pl.col(\"count\").alias(\"count_next\")\n",
    "    )\n",
    "\n",
    "    index_std_df = (\n",
    "        std_df.join(prev_std_df, on=\"time\", how=join_strategy)\n",
    "        .join(next_std_df, on=\"time\", how=join_strategy)\n",
    "        .with_columns(\n",
    "            (pl.col(\"B_std\") / (pl.max_horizontal(\"B_std_prev\", \"B_std_next\"))).alias(\n",
    "                \"index_std\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return index_std_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of fluctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of the relative field jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_index_diff(\n",
    "    df: pl.DataFrame, \n",
    "    tau: timedelta,\n",
    "    cols\n",
    "    ):\n",
    "    db_cols = [\"d\" + col + \"_vec\" for col in cols]\n",
    "\n",
    "    index_diff = (\n",
    "        df.with_columns(pl_norm(cols).alias(\"B\"))\n",
    "        .group_by_dynamic(\"time\", every=tau / 2, period=tau)\n",
    "        .agg(\n",
    "            pl.col(\"B\").mean().alias(\"B_mean\"),\n",
    "            *pl_dvec(cols),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl_norm(db_cols).alias(\"dB_vec\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col(\"dB_vec\") / pl.col(\"B_mean\")).alias(\"index_diff\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return index_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _compute_indices(\n",
    "    df: pl.LazyFrame, tau: timedelta, cols: list[str] = [\"BX\", \"BY\", \"BZ\"]\n",
    ") -> pl.LazyFrame:\n",
    "    join_strategy = \"inner\"\n",
    "\n",
    "    std_df = compute_std(df, tau, cols)\n",
    "    combined_std_df = compute_combinded_std(df, tau, cols)\n",
    "\n",
    "    index_std = compute_index_std(std_df, tau)\n",
    "    index_diff = compute_index_diff(df, tau, cols)\n",
    "\n",
    "    indices = (\n",
    "        index_std.join(index_diff, on=\"time\")\n",
    "        .join(combined_std_df, on=\"time\", how=join_strategy)\n",
    "        .with_columns(\n",
    "            pl.sum_horizontal(\"B_std_prev\", \"B_std_next\").alias(\"B_added_std\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col(\"B_std\") / (pl.max_horizontal(\"B_std_prev\", \"B_std_next\"))).alias(\n",
    "                \"index_std\"\n",
    "            ),\n",
    "            (pl.col(\"B_combined_std\") / pl.col(\"B_added_std\")).alias(\n",
    "                \"index_fluctuation\"\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def compute_indices(\n",
    "    df: pl.DataFrame, tau: timedelta, bcols: list[str] = [\"BX\", \"BY\", \"BZ\"]\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute all index based on the given DataFrame and tau value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame.\n",
    "    tau : datetime.timedelta\n",
    "        Time interval value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple :\n",
    "        Tuple containing DataFrame results for fluctuation index,\n",
    "        standard deviation index, and 'index_num'.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> indices = compute_indices(df, tau)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This is a wrapper for `_compute_indices` with `pl.LazyFrame` input.\n",
    "    - Simply shift to calculate index_std would not work correctly if data is missing,\n",
    "        like `std_next = pl.col(\"B_std\").shift(-2)`.\n",
    "    - Drop null though may lose some IDs (using the default `join_strategy`).\n",
    "        Because we could not tell if it is a real ID or just a partial wave\n",
    "        from incomplete data without previous or/and next std.\n",
    "        Hopefully we can pick up the lost ones with smaller tau.\n",
    "    - TODO: Can be optimized further, but this is already fast enough.\n",
    "        - TEST: if `join` can be improved by shift after filling the missing values.\n",
    "        - TEST: if `list` in `polars` really fast?\n",
    "    \"\"\"\n",
    "    return _compute_indices(df.lazy(), tau, bcols).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_planet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
