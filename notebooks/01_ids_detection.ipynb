{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title:  ID identification\n",
    "subtitle: limited feature extraction / anomaly detection\n",
    "---\n",
    "\n",
    "There are couple of ways to identify the ID.\n",
    "\n",
    "-   Variance method [@liuMagneticDiscontinuitiesSolar2022] : Large variance in the magnetic field compared with neighboring intervals \n",
    "-   B-criterion [@burlagaTangentialDiscontinuitiesSolar1969] : a directional change of the magnetic ﬁeld larger than 30° during 60 s\n",
    "-   TS-criterion [@tsurutaniInterplanetaryDiscontinuitiesTemporal1979] : $|ΔB|/|B| \\geq 0.5$ within 3 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liu's variance method\n",
    "\n",
    "For each sampling instant $t$, we define three intervals: the pre-interval $[-1,-1/2]\\cdot T+t$, the middle interval $[-1/,1/2]\\cdot T+t$, and the post-interval $[1/2,1]\\cdot T+t$, in which $T$ are time lags. Let time series of the magnetic field data in these three intervals are labeled ${\\mathbf B}_-$, ${\\mathbf B}_0$, ${\\mathbf B}_+$, respectively. Compute the following indices:\n",
    "\n",
    "$$ \n",
    "I_1 = \\frac{\\sigma(B_0)}{Max(\\sigma(B_-),\\sigma(B_+))}\n",
    "$$\n",
    "$$\n",
    "I_2 = \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)} \n",
    "$$\n",
    "$$\n",
    "I_3 = \\frac{| \\Delta \\vec{B} |}{|B_{bg}|}\n",
    "$$\n",
    "\n",
    "By selecting a large and reasonable threshold for the ﬁrst two indices ($I_1>2, I_2>1$) , we could guarantee that the ﬁeld changes of the IDs identiﬁed are large enough to be distinguished from the stochastic ﬂuctuations on magnetic ﬁelds, while the third is a supplementary condition to reduce the uncertainty of recognition. While the third index (relative field jump)  is a supplementary condition to reduce the uncertainty of recognition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core/detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from datetime import timedelta\n",
    "import polars as pl\n",
    "\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "from discontinuitypy.utils.polars import pl_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from discontinuitypy.utils.basic import _expand_selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# some helper functions\n",
    "def pl_format_time(df: pl.LazyFrame | pl.DataFrame, tau: timedelta):\n",
    "    return df.with_columns(\n",
    "        tstart=pl.col(\"time\"),\n",
    "        tstop=(pl.col(\"time\") + tau),\n",
    "        time=(pl.col(\"time\") + tau / 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compute_std(\n",
    "    df: pl.DataFrame | pl.LazyFrame,\n",
    "    period: timedelta,  # period to group by\n",
    "    b_cols=[\"BX\", \"BY\", \"BZ\"],\n",
    "    every: timedelta = None,  # every to group by (default: period / 2)\n",
    "):\n",
    "    if every is None:\n",
    "        every = period / 2\n",
    "    b_std_cols = [col_name + \"_std\" for col_name in b_cols]\n",
    "\n",
    "    std_df = (\n",
    "        df.group_by_dynamic(\"time\", every=every, period=period)\n",
    "        .agg(\n",
    "            pl.count(),\n",
    "            pl.col(b_cols).std(ddof=0).map_alias(lambda col_name: col_name + \"_std\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl_norm(b_std_cols).alias(\"B_std\"),\n",
    "        )\n",
    "        .drop(b_std_cols)\n",
    "    )\n",
    "    return std_df\n",
    "\n",
    "\n",
    "def compute_combinded_std(df: pl.DataFrame | pl.LazyFrame, tau, cols) -> pl.DataFrame:\n",
    "    combined_std_cols = [col_name + \"_combined_std\" for col_name in cols]\n",
    "    offsets = [0 * tau, tau / 2]\n",
    "    combined_std_dfs = []\n",
    "    for offset in offsets:\n",
    "        truncated_df = df.select(\n",
    "            (pl.col(\"time\") - offset).dt.truncate(tau, offset=offset).alias(\"time\"),\n",
    "            pl.col(cols),\n",
    "        )\n",
    "\n",
    "        prev_df = truncated_df.select(\n",
    "            (pl.col(\"time\") + tau),\n",
    "            pl.col(cols),\n",
    "        )\n",
    "\n",
    "        next_df = truncated_df.select(\n",
    "            (pl.col(\"time\") - tau),\n",
    "            pl.col(cols),\n",
    "        )\n",
    "\n",
    "        temp_combined_std_df = (\n",
    "            pl.concat([prev_df, next_df])\n",
    "            .group_by(\"time\")\n",
    "            .agg(\n",
    "                pl.col(cols)\n",
    "                .std(ddof=0)\n",
    "                .map_alias(lambda col_name: col_name + \"_combined_std\"),\n",
    "            )\n",
    "            .with_columns(B_std_combined=pl_norm(combined_std_cols))\n",
    "            .drop(combined_std_cols)\n",
    "            .sort(\"time\")\n",
    "        )\n",
    "\n",
    "        combined_std_dfs.append(temp_combined_std_df)\n",
    "\n",
    "    combined_std_df = pl.concat(combined_std_dfs)\n",
    "    return combined_std_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def add_neighbor_std(\n",
    "    df: pl.LazyFrame, tau: timedelta, join_strategy=\"inner\"\n",
    "):  # noqa: F811\n",
    "    \"\"\"\n",
    "    Get the neighbor standard deviations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - df (pl.LazyFrame): The input DataFrame.\n",
    "    - tau : The time interval value.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Simply shift would not work correctly if data is missing, like `std_next = pl.col(\"B_std\").shift(-2)`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the standard deviation index\n",
    "    prev_std_df = df.select(\n",
    "        pl.col(\"time\") + tau,\n",
    "        B_std_prev=pl.col(\"B_std\"),\n",
    "        count_prev=pl.col(\"count\"),\n",
    "    )\n",
    "\n",
    "    next_std_df = df.select(\n",
    "        pl.col(\"time\") - tau,\n",
    "        B_std_next=pl.col(\"B_std\"),\n",
    "        count_next=pl.col(\"count\"),\n",
    "    )\n",
    "\n",
    "    return df.join(prev_std_df, on=\"time\", how=join_strategy).join(\n",
    "        next_std_df, on=\"time\", how=join_strategy\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compute_index_std(df: pl.LazyFrame):  # noqa: F811\n",
    "    \"\"\"\n",
    "    Compute the standard deviation index based on the given DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - df (pl.LazyFrame): The input DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - pl.LazyFrame: DataFrame with calculated 'index_std' column.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> index_std_df = compute_index_std_pl(df)\n",
    "    >>> index_std_df\n",
    "    \"\"\"\n",
    "\n",
    "    return df.with_columns(\n",
    "        index_std=pl.col(\"B_std\") / pl.max_horizontal(\"B_std_prev\", \"B_std_next\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of fluctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compute_index_fluctuation(df: pl.LazyFrame, base_col=\"B_std\"):\n",
    "    std_combined = pl.col(\"B_std_combined\")\n",
    "    std_added = pl.sum_horizontal(\"B_std_prev\", \"B_std_next\")\n",
    "    return df.with_columns(index_fluctuation=std_combined / std_added)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of the relative field jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def pl_dvec(columns, *more_columns):\n",
    "    all_columns = _expand_selectors(columns, *more_columns)\n",
    "    return [\n",
    "        (pl.col(column).first() - pl.col(column).last()).alias(f\"d{column}_vec\")\n",
    "        for column in all_columns\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compute_index_diff(df: pl.LazyFrame, period: timedelta, cols):\n",
    "    db_cols = [\"d\" + col + \"_vec\" for col in cols]\n",
    "\n",
    "    index_diff = (\n",
    "        df.with_columns(B=pl_norm(cols))\n",
    "        .group_by_dynamic(\"time\", every=period / 2, period=period)\n",
    "        .agg(\n",
    "            pl.col(\"B\").mean().alias(\"B_mean\"),\n",
    "            *pl_dvec(cols),\n",
    "        )\n",
    "        .with_columns(dB_vec=pl_norm(db_cols))\n",
    "        .with_columns(index_diff=pl.col(\"dB_vec\") / pl.col(\"B_mean\"))\n",
    "        .drop(db_cols)\n",
    "    )\n",
    "\n",
    "    return index_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _compute_indices(\n",
    "    df: pl.LazyFrame, tau: timedelta, cols: list[str] = [\"BX\", \"BY\", \"BZ\"]\n",
    ") -> pl.LazyFrame:\n",
    "    join_strategy = \"inner\"\n",
    "\n",
    "    std_df = compute_std(df, tau, cols)\n",
    "    stds_df = add_neighbor_std(std_df, tau)\n",
    "\n",
    "    combined_std_df = compute_combinded_std(df, tau, cols)\n",
    "    index_diff = compute_index_diff(df, tau, cols)\n",
    "\n",
    "    indices = (\n",
    "        stds_df.join(index_diff, on=\"time\")\n",
    "        .join(combined_std_df, on=\"time\", how=join_strategy)\n",
    "        .pipe(compute_index_std)\n",
    "        .pipe(compute_index_fluctuation)\n",
    "        .drop([\"B_std_prev\", \"B_std_next\", \"B_added_std\", \"B_std_combined\"])\n",
    "    )\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def compute_indices(\n",
    "    df: pl.DataFrame, tau: timedelta, bcols: list[str] = [\"BX\", \"BY\", \"BZ\"]\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute all index based on the given DataFrame and tau value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame.\n",
    "    tau : datetime.timedelta\n",
    "        Time interval value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple :\n",
    "        Tuple containing DataFrame results for fluctuation index,\n",
    "        standard deviation index, and 'index_num'.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> indices = compute_indices(df, tau)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This is a wrapper for `_compute_indices` with `pl.LazyFrame` input.\n",
    "    - Simply shift to calculate index_std would not work correctly if data is missing,\n",
    "        like `std_next = pl.col(\"B_std\").shift(-2)`.\n",
    "    - Drop null though may lose some IDs (using the default `join_strategy`).\n",
    "        Because we could not tell if it is a real ID or just a partial wave\n",
    "        from incomplete data without previous or/and next std.\n",
    "        Hopefully we can pick up the lost ones with smaller tau.\n",
    "    - TODO: Can be optimized further, but this is already fast enough.\n",
    "        - TEST: if `join` can be improved by shift after filling the missing values.\n",
    "        - TEST: if `list` in `polars` really fast?\n",
    "    \"\"\"\n",
    "    return _compute_indices(df.lazy(), tau, bcols).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# from discontinuitypy import PARAMS\n",
    "\n",
    "# INDEX_STD_THRESHOLD = PARAMS['detection']['index_std_threshold']\n",
    "# INDEX_FLUC_THRESHOLD = PARAMS['detection']['index_fluc_threshold']\n",
    "# INDEX_DIFF_THRESHOLD = PARAMS['detection']['index_diff_threshold']\n",
    "INDEX_STD_THRESHOLD = 2\n",
    "INDEX_FLUC_THRESHOLD = 1\n",
    "INDEX_DIFF_THRESHOLD = 0.1\n",
    "SPARSE_THRESHOLD = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def filter_indices(\n",
    "    df: pl.DataFrame | pl.LazyFrame,\n",
    "    index_std_threshold : float = INDEX_STD_THRESHOLD,\n",
    "    index_fluc_threshold : float =INDEX_FLUC_THRESHOLD,\n",
    "    index_diff_threshold : float =INDEX_DIFF_THRESHOLD,\n",
    "    sparse_num : int = SPARSE_THRESHOLD,\n",
    ") -> pl.DataFrame | pl.LazyFrame:\n",
    "    # filter indices to get possible IDs\n",
    "\n",
    "    return df.filter(\n",
    "        pl.col(\"index_std\") > index_std_threshold,\n",
    "        pl.col(\"index_fluctuation\") > index_fluc_threshold,\n",
    "        pl.col(\"index_diff\") > index_diff_threshold,\n",
    "        pl.col(\"index_std\").is_finite(), # for cases where neighboring groups have std=0\n",
    "        pl.col(\"count\") > sparse_num, \n",
    "        pl.col(\"count_prev\") > sparse_num, # filter out sparse intervals, which may give unreasonable results.\n",
    "        pl.col(\"count_next\") > sparse_num # filter out sparse intervals, which may give unreasonable results.\n",
    "    ).drop([\"count_prev\", \"count_next\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def detect_events(data: pl.DataFrame, tau: timedelta, ts: timedelta, bcols):\n",
    "    indices = compute_indices(data, tau, bcols)\n",
    "    sparse_num = tau / ts // 3\n",
    "    events = indices.pipe(filter_indices, sparse_num=sparse_num).pipe(\n",
    "        pl_format_time, tau\n",
    "    )\n",
    "    return events"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_planet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
