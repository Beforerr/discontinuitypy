{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: IDs from ARTHEMIS\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "output-file: artemis.html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "ARTEMIS spacecrafts will be exposed in the solar wind at 1 AU during its orbits around the Moon. So it's very interesting to look into its data.\n",
    "\n",
    "- For time inteval for THEMIS-B in solar wind, see [Link](https://omniweb.gsfc.nasa.gov/ftpbrowser/themis_b_sw.txt)\n",
    "- For time inteval for THEMIS-C in solar wind, see [Link](https://omniweb.gsfc.nasa.gov/ftpbrowser/themis_c_sw.txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to run command in shell first as `pipeline` is project-specific command\n",
    "\n",
    "```{sh}\n",
    "kedro pipeline create themis\n",
    "```\n",
    "\n",
    "To get candidates data, run `kedro run --from-inputs=jno.feature_1s --to-outputs=candidates.jno_1s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| default_exp pipelines/themis/pipeline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | code-summary: import all the packages needed for the project\n",
    "# | output: hide\n",
    "# | export\n",
    "\n",
    "from ids_finder.core import *\n",
    "from ids_finder.utils.basic import DataConfig\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "\n",
    "import polars as pl\n",
    "import pandas\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "from datetime import timedelta, datetime\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Kerdo`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from kedro.pipeline import Pipeline, node\n",
    "from kedro.pipeline.modular_pipeline import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "from ids_finder.utils.basic import load_catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = load_catalog()\n",
    "\n",
    "jno_start_date = catalog.load(\"params:jno_start_date\")\n",
    "jno_end_date = catalog.load(\"params:jno_end_date\")\n",
    "trange = [jno_start_date, jno_end_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnetic field data pipeline\n",
    "\n",
    "- For convenience, we choose magnetic field data in **GSE** coordinate system\n",
    "- The `fgs` data are in 3-4s resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import speasy as spz\n",
    "from speasy import SpeasyVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def check_dataype(ts: int):\n",
    "    ts = pandas.Timedelta(seconds=ts)\n",
    "    fgs_ts = timedelta(seconds=3)\n",
    "    fgl_ts = timedelta(seconds=0.1)\n",
    "\n",
    "    if ts > fgs_ts:\n",
    "        datatype = \"fgs\"\n",
    "    elif ts > fgl_ts:\n",
    "        datatype = \"fgl\"\n",
    "    else:\n",
    "        datatype = \"fgh\"\n",
    "    return datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def download_mag_data(\n",
    "    trange, probe: str = \"b\", datatype=\"fgs\", coord=\"gse\"\n",
    ") -> SpeasyVariable:\n",
    "    match probe:\n",
    "        case \"b\":\n",
    "            sat = \"thb\"\n",
    "\n",
    "    product = f\"cda/{sat.upper()}_L2_FGM/{sat}_{datatype}_{coord}\"\n",
    "    data = spz.get_data(product, trange, disable_proxy=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def spz2df(raw_data: SpeasyVariable):\n",
    "    return pl.from_dataframe(raw_data.to_dataframe().reset_index()).rename(\n",
    "        {\"index\": \"time\"}\n",
    "    )\n",
    "\n",
    "\n",
    "def load_mag_data(\n",
    "    start: str,\n",
    "    end: str,\n",
    "    ts: int = None,  # time resolution\n",
    "    probe: str = \"b\",\n",
    "    coord=\"gse\",\n",
    "):\n",
    "    trange = [start, end]\n",
    "\n",
    "    datatype = check_dataype(ts)\n",
    "\n",
    "    data = download_mag_data(trange, probe, datatype, coord)\n",
    "    return spz2df(data).lazy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from ids_finder.utils.basic import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def preprocess_mag_data(\n",
    "    raw_data: pl.LazyFrame,\n",
    "    datatype: str = None,\n",
    "    coord: str = \"gse\",\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the raw dataset (only minor transformations)\n",
    "\n",
    "    - Applying naming conventions for columns\n",
    "    - Dropping duplicate time\n",
    "    - Changing storing format to `parquet`\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    datatype = datatype.upper()\n",
    "    name_mapping = {\n",
    "        f\"Bx {datatype}-D\": \"B_x\",\n",
    "        f\"By {datatype}-D\": \"B_y\",\n",
    "        f\"Bz {datatype}-D\": \"B_z\",\n",
    "    }\n",
    "\n",
    "    return raw_data.sort(\"time\").unique(\"time\").rename(name_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from ids_finder.utils.basic import partition_data_by_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def process_mag_data(\n",
    "    raw_data: pl.LazyFrame,\n",
    "    ts: int = None,  # time resolution\n",
    ") -> pl.DataFrame | Dict[str, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Partitioning data, for the sake of memory\n",
    "    \"\"\"\n",
    "\n",
    "    every = timedelta(seconds=ts)\n",
    "    period = 2 * every\n",
    "\n",
    "    return raw_data.pipe(resample, every=every, period=period).pipe(\n",
    "        partition_data_by_year\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "def create_mag_data_pipeline(\n",
    "    sat_id: str,  # satellite id, used for namespace\n",
    "    ts: int = 1,  # time resolution, in seconds\n",
    "    tau: str = \"60s\",  # time window\n",
    "    **kwargs,\n",
    ") -> Pipeline:\n",
    "    \n",
    "    datatype = check_dataype(\n",
    "        ts\n",
    "    )  # get the datatype from the time resolution, which is reasonable but not always is the case\n",
    "    ts_str = f\"ts_{ts}s\"\n",
    "\n",
    "    node_load_data = node(\n",
    "        load_mag_data,\n",
    "        inputs=dict(\n",
    "            start=\"params:start_date\",\n",
    "            end=\"params:end_date\",\n",
    "            ts=\"params:mag.time_resolution\",\n",
    "        ),\n",
    "        outputs=f\"raw_mag\",\n",
    "        name=f\"load_{sat_id.upper()}_magnetic_field_data\",\n",
    "    )\n",
    "\n",
    "    node_preprocess_data = node(\n",
    "        preprocess_mag_data,\n",
    "        inputs=dict(\n",
    "            raw_data=f\"raw_mag\",\n",
    "            datatype=datatype,\n",
    "        ),\n",
    "        outputs=f\"inter_mag_{datatype}\",\n",
    "        name=f\"preprocess_{sat_id.upper()}_magnetic_field_data\",\n",
    "    )\n",
    "\n",
    "    node_process_data = node(\n",
    "        process_mag_data,\n",
    "        inputs=dict(\n",
    "            raw_data=f\"inter_mag_{datatype}\",\n",
    "            ts=\"params:mag.time_resolution\",\n",
    "        ),\n",
    "        outputs=f\"primary_mag_{ts_str}\",\n",
    "        name=f\"process_{sat_id.upper()}_magnetic_field_data\",\n",
    "    )\n",
    "\n",
    "    node_extract_features = node(\n",
    "        extract_features,\n",
    "        inputs=[f\"primary_mag_{ts_str}\", \"params:tau\", \"params:mag\"],\n",
    "        outputs=f\"feature_{ts_str}_tau_{tau}\",\n",
    "        name=f\"extract_{sat_id}_features\",\n",
    "    )\n",
    "\n",
    "    nodes = [\n",
    "        node_load_data,\n",
    "        node_preprocess_data,\n",
    "        node_process_data,\n",
    "        node_extract_features,\n",
    "    ]\n",
    "\n",
    "    pipelines = pipeline(\n",
    "        nodes,\n",
    "        namespace=sat_id,\n",
    "        parameters={\n",
    "            \"params:start_date\": \"params:jno_start_date\",\n",
    "            \"params:end_date\": \"params:jno_end_date\",\n",
    "            \"params:tau\": \"params:tau\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "ts = \"4s\"\n",
    "thb_inter_mag = catalog.load(f\"thb.inter_mag_{ts}\")\n",
    "thb_inter_mag.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ids_finder.utils.basic import check_fgm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State data pipeline\n",
    "\n",
    "We use low resolution [OMNI data](https://omniweb.gsfc.nasa.gov/ow.html) for plasma state data, see [details](https://spdf.gsfc.nasa.gov/pub/data/omni/low_res_omni/omni2.text).\n",
    "\n",
    "- Data gaps were filled with dummy numbers for the missing hours or entire\n",
    "  days to make all files of equal length.  The character '9' is used to\n",
    "  fill all fields for missing data according to their format, e.g.\n",
    "  ' 9999.9' for a field with the FORTRAN format F7.1. Note that format F7.1\n",
    "  below really means (1X,F6.1),etc.\n",
    "\n",
    "```\n",
    "The flow OMNI \"phi\" angle is opposite GSE \"phi\" angle, threrfore, Flow-vector cartesian components in GSE coordinates may be derived from the given speed and angles as\n",
    "\n",
    "Vx = - V * cos(theta) * cos(phi)\n",
    "Vy = + V * cos(theta) * sin(phi)\n",
    "Vz = + V * sin(theta)\n",
    "and vise versa: two angles may be derived from the given speed and Vx,Vy,Vz comp. as  \n",
    "          a_theta=vz/V\n",
    "          theta=(180.*asin(a_theta))/!PI\n",
    "         a_phi=Vy/(-Vx)\n",
    "        phi=(180.*atan(a_phi))/!PI\n",
    "```\n",
    "\n",
    "```\n",
    "   (*)   Quasi-GSE for the flow longitude angle means the angle increases from zero\n",
    "         to positive values as the flow changes from being aligned along the -X(GSE)\n",
    "         axis towards the +Y(GSE) axis.  The flow longitude angle is positive for \n",
    "         flow from west of the sun, towards +Y(GSE).\n",
    "         The flow latitude angle is positive for flow from south of the sun, \n",
    "         towards +Z(GSE)\n",
    "``````                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from ids_finder.utils.basic import cdf2pl, pmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def download_state_data(\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    ts: str = None,  # time resolution\n",
    "    probe: str = None,\n",
    "    coord: str = None,\n",
    "):\n",
    "    import pyspedas\n",
    "\n",
    "    trange = [start, end]\n",
    "    files = pyspedas.omni.data(trange=trange, datatype=\"hour\", downloadonly=True)\n",
    "    return files\n",
    "\n",
    "\n",
    "def load_state_data(\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    ts: str = None,  # time resolution\n",
    "    vars: dict = None,\n",
    "):\n",
    "    files = download_state_data(start, end, ts, vars)\n",
    "    df: pl.LazyFrame = pl.concat(files | pmap(cdf2pl, var_names=list(vars)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def preprocess_state_data(\n",
    "    raw_data: pl.LazyFrame,\n",
    "    vars: dict,\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the raw dataset (only minor transformations)\n",
    "\n",
    "    - Applying naming conventions for columns\n",
    "    - Extracting variables from `CDF` files, and convert them to DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    columns_name_mapping = {key: value[\"COLNAME\"] for key, value in vars.items()}\n",
    "\n",
    "    return raw_data.rename(columns_name_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we have additional data file that indicate if `THEMIS` is in solar wind or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def preprocess_sw_state_data(\n",
    "    raw_data: pandas.DataFrame,\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    - Applying naming conventions for columns\n",
    "    - Parsing and typing data (like from string to datetime for time columns)\n",
    "    \"\"\"\n",
    "\n",
    "    return pl.from_dataframe(raw_data).with_columns(\n",
    "        # Note: For `polars`, please either specify both hour and minute, or neither.\n",
    "        pl.concat_str(pl.col(\"start\"), pl.lit(\" 00\")).str.to_datetime(\n",
    "            format=\"%Y %j %H %M\"\n",
    "        ),\n",
    "        pl.concat_str(pl.col(\"end\"), pl.lit(\" 00\")).str.to_datetime(\n",
    "            format=\"%Y %j %H %M\"\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def flow2gse(df: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    - Transforming solar wind data from `Quasi-GSE` coordinate to GSE coordinate system\n",
    "    \"\"\"\n",
    "    plasma_speed = pl.col(\"plasma_speed\")\n",
    "    sw_theta = pl.col(\"sw_vel_theta\")\n",
    "    sw_phi = pl.col(\"sw_vel_phi\")\n",
    "\n",
    "    return df.with_columns(\n",
    "        sw_vel_gse_x=-plasma_speed * sw_theta.cos() * sw_phi.cos(),\n",
    "        sw_vel_gse_y=+plasma_speed * sw_theta.cos() * sw_phi.sin(),\n",
    "        sw_vel_gse_z=+plasma_speed * sw_theta.sin(),\n",
    "    ).drop([\"sw_theta\", \"sw_phi\"])\n",
    "\n",
    "\n",
    "def filter_tranges(time: pl.Series, tranges: Tuple[list, list]):\n",
    "    \"\"\"\n",
    "    - Filter data by time ranges, return the indices of the time that are in the time ranges\n",
    "    \"\"\"\n",
    "\n",
    "    starts = tranges[0]\n",
    "    ends = tranges[1]\n",
    "\n",
    "    start_indices = time.search_sorted(starts)\n",
    "    end_indices = time.search_sorted(ends)\n",
    "\n",
    "    return np.concatenate(\n",
    "        [\n",
    "            np.arange(start_index, end_index + 1)\n",
    "            for start_index, end_index in zip(start_indices, end_indices)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def add_state(l_df: pl.LazyFrame, l_state: pl.LazyFrame):\n",
    "    state = l_state.collect()\n",
    "    df = l_df.collect()\n",
    "\n",
    "    start = state.get_column(\"start\")\n",
    "    end = state.get_column(\"end\")\n",
    "\n",
    "    time = df.get_column(\"time\")\n",
    "\n",
    "    indices = filter_tranges(time, (start, end))\n",
    "\n",
    "    return (\n",
    "        df.with_row_count()\n",
    "        .with_columns(\n",
    "            state=pl.when(pl.col(\"row_nr\").is_in(indices)).then(1).otherwise(0)\n",
    "        )\n",
    "        .drop(\"row_nr\")\n",
    "    )\n",
    "\n",
    "\n",
    "def process_state_data(df: pl.LazyFrame, state: pl.LazyFrame = None) -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    - Transforming data to GSE coordinate system\n",
    "    - Combine state data with additional plasma state data\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        df.pipe(flow2gse)\n",
    "        .pipe(add_state, state)\n",
    "        .rename(\n",
    "            {\n",
    "                \"sw_vel_gse_x\": \"v_x\",\n",
    "                \"sw_vel_gse_y\": \"v_y\",\n",
    "                \"sw_vel_gse_z\": \"v_z\",\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def create_state_data_pipeline(\n",
    "    sat_id,\n",
    "    ts: str = \"1h\",  # time resolution\n",
    ") -> Pipeline:\n",
    "    node_load_data = node(\n",
    "        load_state_data,\n",
    "        inputs=dict(\n",
    "            start=\"params:start_date\",\n",
    "            end=\"params:end_date\",\n",
    "            vars=\"params:omni_vars\",\n",
    "        ),\n",
    "        outputs=f\"raw_state\",\n",
    "        name=f\"load_{sat_id.upper()}_state_data\",\n",
    "    )\n",
    "\n",
    "    node_preprocess_data = node(\n",
    "        preprocess_state_data,\n",
    "        inputs=dict(\n",
    "            raw_data=f\"raw_state\",\n",
    "            vars=\"params:omni_vars\",\n",
    "        ),\n",
    "        outputs=f\"inter_state_{ts}\",\n",
    "        name=f\"preprocess_{sat_id.upper()}_state_data\",\n",
    "    )\n",
    "\n",
    "    node_preprocess_sw_state = node(\n",
    "        preprocess_sw_state_data,\n",
    "        inputs=f\"raw_state_sw\",\n",
    "        outputs=f\"inter_state_sw\",\n",
    "        name=f\"preprocess_{sat_id.upper()}_solar_wind_state_data\",\n",
    "    )\n",
    "\n",
    "    node_process_data = node(\n",
    "        process_state_data,\n",
    "        inputs=[f\"inter_state_{ts}\", f\"inter_state_sw\"],\n",
    "        outputs=f\"primary_state_{ts}\",\n",
    "        name=f\"process_{sat_id.upper()}_state_data\",\n",
    "    )\n",
    "\n",
    "    nodes = [\n",
    "        node_load_data,\n",
    "        node_preprocess_data,\n",
    "        node_preprocess_sw_state,\n",
    "        node_process_data,\n",
    "    ]\n",
    "    pipelines = pipeline(\n",
    "        nodes,\n",
    "        namespace=sat_id,\n",
    "        parameters={\n",
    "            \"params:omni_vars\": \"params:omni_vars\",\n",
    "            \"params:start_date\": \"params:jno_start_date\",\n",
    "            \"params:end_date\": \"params:jno_end_date\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "catalog.load(\"thb.primary_state_1h\").collect().describe()\n",
    "# catalog.load('thb.feature_tau_60s').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from ids_finder.pipelines.default import create_candidate_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from ids_finder.utils.basic import filter_tranges_df\n",
    "\n",
    "def filter_sw_events(events: pl.LazyFrame, sw_state: pl.LazyFrame) -> pl.LazyFrame:\n",
    "    \n",
    "    start, end = sw_state.select(['start', 'end']).collect()\n",
    "    sw_events = filter_tranges_df(events.collect(), (start, end))\n",
    "    \n",
    "    return sw_events\n",
    "\n",
    "def create_sw_events_pipeline(\n",
    "    sat_id,\n",
    "    tau: int = 60,\n",
    "    ts_mag: int = 1,\n",
    "    \n",
    "):\n",
    "  \n",
    "    ts_mag_str = f\"ts_{ts_mag}s\"\n",
    "    tau_str = f\"tau_{tau}s\"\n",
    "    \n",
    "    node_filter_sw_events = node(\n",
    "        filter_sw_events,\n",
    "        inputs=[\n",
    "            f\"candidates.{sat_id}_{ts_mag_str}_{tau_str}\",\n",
    "            f\"{sat_id}.inter_state_sw\",\n",
    "        ],\n",
    "        outputs=f\"events.sw.{sat_id}_{ts_mag_str}_{tau_str}\"\n",
    "        \n",
    "    )\n",
    "\n",
    "    nodes = [node_filter_sw_events]\n",
    "    return pipeline(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def create_pipeline(\n",
    "    sat_id=\"thb\",\n",
    "    tau=60, # time window, in seconds\n",
    "    ts_state=\"1h\",  # time resolution of state data\n",
    "    ts_mag = 1, # time resolution of mag data, in seconds\n",
    ") -> Pipeline:\n",
    "    return (\n",
    "        create_mag_data_pipeline(sat_id, tau=tau)\n",
    "        + create_state_data_pipeline(sat_id, ts=ts_state)\n",
    "        + create_candidate_pipeline(sat_id, tau=tau, ts_mag= ts_mag, ts_state=ts_state)\n",
    "        + create_sw_events_pipeline(sat_id, tau=tau, ts_mag= ts_mag)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "# catalog.load('thb.inter_mag_4s').collect().describe()\n",
    "# catalog.load('thb.primary_state_1h').collect().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and preprocess the data\n",
    "\n",
    "As we are only interested in the data when THEMIS is in the solar wind, for simplicity we will only keep the data when `X, SSE` and `X, GSE` is positive.\n",
    "\n",
    "- State data time resolution is 1 minute...\n",
    "\n",
    "- FGS data time resolution is 4 second..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thm_state(sat):\n",
    "    sat_pos_sse_files = f\"../data/{sat}_pos_sse.parquet\"\n",
    "    sat_pos_sse = pl.scan_parquet(sat_pos_sse_files).set_sorted(\"time\")\n",
    "    sat_pos_gse_files = f\"../data/{sat}_pos_gse.parquet\"\n",
    "    sat_pos_gse = pl.scan_parquet(sat_pos_gse_files).set_sorted(\"time\")\n",
    "    sat_state = sat_pos_sse.join(sat_pos_gse, on=\"time\", how=\"inner\")\n",
    "    return sat_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "df = (\n",
    "    sat_state_sw.upsample(\"time\", every=\"1m\")\n",
    "    .group_by_dynamic(\"time\", every=\"1d\")\n",
    "    .agg(pl.col(\"X, SSE\").null_count().alias(\"null_count\"))\n",
    "    .with_columns(\n",
    "        pl.when(pl.col(\"null_count\") > 720).then(0).otherwise(1).alias(\"availablity\")\n",
    "    )\n",
    ")\n",
    "\n",
    "properties = {\n",
    "    'width': 800,\n",
    "}\n",
    "\n",
    "chart1 = alt.Chart(df).mark_point().encode(\n",
    "    x='time',\n",
    "    y='null_count'\n",
    ").properties(**properties)\n",
    "\n",
    "chart2  = alt.Chart(df).mark_point().encode(\n",
    "    x='time',\n",
    "    y='availablity'\n",
    ").properties(**properties)\n",
    "\n",
    "(chart1 & chart2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
