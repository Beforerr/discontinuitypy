{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Utils\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils/basic\n",
    "#| export\n",
    "from discontinuitypy import ROOT_DIR\n",
    "from discontinuitypy.utils.kedro import load_context\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from typing import overload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "load_catalog = partial(load_context, project_path=ROOT_DIR, catalog_only=True)\n",
    "load_params = partial(load_context, project_path=ROOT_DIR, params_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "from xarray_einstats import linalg\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from xarray import DataArray\n",
    "from typing import Union, Collection, Callable, Optional, Tuple\n",
    "from typing import Any, Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pipe import select\n",
    "from fastcore.utils import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pmap(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    map with `partial`\n",
    "    \"\"\"\n",
    "    return select(partial(func, *args, **kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pydantic import BaseModel\n",
    "from datetime import datetime, timedelta\n",
    "from pandas import Timedelta\n",
    "\n",
    "class DataConfig(BaseModel):\n",
    "    sat_id: str = None\n",
    "    start: datetime = None\n",
    "    end: datetime = None\n",
    "    ts: timedelta = None\n",
    "    coord: str = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Polars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_tranges(time: pl.Series, tranges: Tuple[list, list]):\n",
    "    \"\"\"\n",
    "    - Filter data by time ranges, return the indices of the time that are in the time ranges (left inclusive, right exclusive)\n",
    "    \"\"\"\n",
    "\n",
    "    starts = tranges[0]\n",
    "    ends = tranges[1]\n",
    "\n",
    "    start_indices = time.search_sorted(starts)\n",
    "    end_indices = time.search_sorted(ends)\n",
    "\n",
    "    return np.concatenate(\n",
    "        [\n",
    "            np.arange(start_index, end_index)\n",
    "            for start_index, end_index in zip(start_indices, end_indices)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def filter_tranges_df(df: pl.DataFrame, tranges: Tuple[list, list], time_col: str = \"time\"):\n",
    "    \"\"\"\n",
    "    - Filter data by time ranges\n",
    "    \"\"\"\n",
    "\n",
    "    time = df[time_col]\n",
    "    filtered_indices = filter_tranges(time, tranges)\n",
    "    return df[filtered_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def plot(self:pl.DataFrame, *args, **kwargs):\n",
    "    return self.to_pandas().plot(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _expand_selectors(items: Any, *more_items: Any) -> list[Any]:\n",
    "    \"\"\"\n",
    "    See `_expand_selectors` in `polars`.\n",
    "    \"\"\"\n",
    "    expanded: list[Any] = []\n",
    "    for item in (\n",
    "        *(\n",
    "            items\n",
    "            if isinstance(items, Collection) and not isinstance(items, str)\n",
    "            else [items]\n",
    "        ),\n",
    "        *more_items,\n",
    "    ):\n",
    "        expanded.append(item)\n",
    "    return expanded\n",
    "\n",
    "def pl_norm(columns, *more_columns) -> pl.Expr:\n",
    "    \"\"\"\n",
    "    Computes the square root of the sum of squares for the given columns.\n",
    "\n",
    "    Args:\n",
    "    *columns (str): Names of the columns.\n",
    "\n",
    "    Returns:\n",
    "    pl.Expr: Expression representing the square root of the sum of squares.\n",
    "    \"\"\"\n",
    "    all_columns = _expand_selectors(columns, *more_columns)\n",
    "    squares = [pl.col(column).pow(2) for column in all_columns]\n",
    "\n",
    "    return sum(squares).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition the dataset by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def partition_data_by_ts(df: pl.DataFrame, ts: timedelta) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Partition the dataset by time\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame.\n",
    "        ts: Time interval.\n",
    "\n",
    "    Returns:\n",
    "        Partitioned DataFrame.\n",
    "    \"\"\"\n",
    "    return df.with_columns(\n",
    "        key=pl.col(\"time\").dt.truncate(ts).cast(pl.Utf8)\n",
    "    ).partition_by(\"key\", include_key=False, as_dict=True)\n",
    "\n",
    "\n",
    "def partition_data_by_year(df: pl.DataFrame) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Partition the dataset by year\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Partitioned DataFrame.\n",
    "    \"\"\"\n",
    "    return df.with_columns(year=pl.col(\"time\").dt.year().cast(pl.Utf8)).partition_by(\n",
    "        \"year\", include_key=False, as_dict=True\n",
    "    )\n",
    "\n",
    "\n",
    "def partition_data_by_year_month(df: pl.DataFrame) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Partition the dataset by year\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Partitioned DataFrame.\n",
    "    \"\"\"\n",
    "    return df.with_columns(\n",
    "        year_month=pl.col(\"time\").dt.year().cast(pl.Utf8)\n",
    "        + \"_\"\n",
    "        + pl.col(\"time\").dt.month().cast(pl.Utf8).str.zfill(2),\n",
    "    ).partition_by(\"year_month\", include_key=False, as_dict=True)\n",
    "    \n",
    "def partition_data_by_time(df: pl.LazyFrame | pl.DataFrame, method) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Partition the dataset by time\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame.\n",
    "        method: The method to partition the data.\n",
    "\n",
    "    Returns:\n",
    "        Partitioned DataFrame.\n",
    "    \"\"\"\n",
    "    if isinstance(df, pl.LazyFrame):\n",
    "        df = df.collect()\n",
    "    \n",
    "    if method == \"year\":\n",
    "        return partition_data_by_year(df)\n",
    "    elif method == \"year_month\":\n",
    "        return partition_data_by_year_month(df)\n",
    "    else:\n",
    "        ts = pd.Timedelta(method)\n",
    "        return partition_data_by_ts(df, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DF_TYPE = Union[pl.DataFrame, pl.LazyFrame, pd.DataFrame]\n",
    "def concat_df(dfs: list[DF_TYPE]) -> DF_TYPE:\n",
    "    \"\"\"Concatenate a list of DataFrames into one DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    match type(dfs[0]):\n",
    "        case pl.DataFrame | pl.LazyFrame:\n",
    "            concat_func = pl.concat\n",
    "        case pandas.DataFrame:\n",
    "            concat_func = pandas.concat\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported DataFrame type: {type(dfs[0])}\")\n",
    "    \n",
    "    return concat_func(dfs)\n",
    "                     \n",
    "def concat_partitions(partitioned_input: Dict[str, Callable]):\n",
    "    \"\"\"Concatenate input partitions into one DataFrame.\n",
    "\n",
    "    Args:\n",
    "        partitioned_input: A dictionary with partition ids as keys and load functions as values.\n",
    "    \"\"\"\n",
    "    partitions_data = [\n",
    "        partition_load_func() for partition_load_func in partitioned_input.values()\n",
    "    ]  # load the actual partition data\n",
    "    \n",
    "    result = concat_df(partitions_data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_timedelta(time):\n",
    "    \"\"\"Format timedelta to `timedelta`\"\"\"\n",
    "    if isinstance(time, timedelta):\n",
    "        return time\n",
    "    elif isinstance(time, str):\n",
    "        return pd.Timedelta(time)\n",
    "    elif isinstance(time, int):\n",
    "        return pd.Timedelta(seconds=time)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type: {type(time)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@overload\n",
    "def resample(\n",
    "    df: pl.DataFrame,\n",
    "    every: timedelta,\n",
    "    period: timedelta = None,\n",
    "    offset: timedelta = None,\n",
    "    shift: timedelta = None,\n",
    "    time_column=\"time\",\n",
    ")-> pl.DataFrame:\n",
    "    ...\n",
    "\n",
    "@overload\n",
    "def resample(\n",
    "    df: pl.LazyFrame,\n",
    "    every: timedelta,\n",
    "    period: timedelta = None,\n",
    "    offset: timedelta = None,\n",
    "    shift: timedelta = None,\n",
    "    time_column=\"time\",\n",
    ")-> pl.LazyFrame:\n",
    "    ...\n",
    "\n",
    "def resample(\n",
    "    df: pl.LazyFrame | pl.DataFrame,\n",
    "    every: timedelta,\n",
    "    period: timedelta = None,\n",
    "    offset: timedelta = None,\n",
    "    shift: timedelta = None,\n",
    "    time_column=\"time\",\n",
    "):\n",
    "    \"\"\"Resample the DataFrame\"\"\"\n",
    "    if period is None:\n",
    "        period = every\n",
    "    if shift is None:\n",
    "        shift = period / 2\n",
    "    return (\n",
    "        df.sort(time_column)\n",
    "        .group_by_dynamic(time_column, every=every, period=period, offset=offset)\n",
    "        .agg(cs.numeric().mean())\n",
    "        .with_columns((pl.col(time_column) + shift))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def df2ts(\n",
    "    df: Union[pandas.DataFrame, pl.DataFrame, pl.LazyFrame],\n",
    "    cols = None,\n",
    "    time_col = \"time\",\n",
    "    attrs=None,\n",
    "    name=None,\n",
    "):\n",
    "    \"\"\"Convert DataFrame to TimeSeries\"\"\"\n",
    "    \n",
    "    if cols is None:\n",
    "        if isinstance(df, (pl.DataFrame, pl.LazyFrame)):\n",
    "            cols = df.columns\n",
    "            cols.remove(time_col)\n",
    "        else:\n",
    "            cols = df.columns.tolist()\n",
    "    \n",
    "    if isinstance(df, pl.LazyFrame):\n",
    "        df = df.collect()\n",
    "\n",
    "    # Prepare data\n",
    "    data = df[cols].to_numpy()\n",
    "\n",
    "    # Prepare coordinates\n",
    "    time = df.index if isinstance(df, pandas.DataFrame) else df[time_col]\n",
    "    if isinstance(cols, str) and len(data.shape) > 1:\n",
    "        element_len = data.shape[1]\n",
    "        v_dim = [cols + str(i) for i in range(element_len)]\n",
    "    else:\n",
    "        v_dim = cols\n",
    "        \n",
    "    # Create the DataArray\n",
    "    coords = {\"time\": time, \"v_dim\": v_dim}\n",
    "\n",
    "    return xr.DataArray(data, coords=coords, attrs=attrs, name=name)\n",
    "\n",
    "\n",
    "def calc_vec_mag(vec) -> DataArray:\n",
    "    return linalg.norm(vec, dims=\"v_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def check_fgm(vec: xr.DataArray):\n",
    "    # check if time is monotonic increasing\n",
    "    logger.info(\"Check if time is monotonic increasing\")\n",
    "    assert vec.time.to_series().is_monotonic_increasing\n",
    "    # check available time difference\n",
    "    logger.info(\n",
    "        f\"Available time delta: {vec.time.diff(dim='time').to_series().unique()}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
