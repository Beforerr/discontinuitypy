{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Utils\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp utils/basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def savefig(name, **kwargs):\n",
    "    plt.savefig(f\"../figures/{name}.png\", bbox_inches=\"tight\", **kwargs)\n",
    "    plt.savefig(f\"../figures/{name}.pdf\", bbox_inches=\"tight\", **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "from xarray_einstats import linalg\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from loguru import logger\n",
    "from multipledispatch import dispatch\n",
    "\n",
    "from xarray import DataArray\n",
    "from typing import Union, Collection, Callable, Optional, Tuple\n",
    "from typing import Any, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pipe import select\n",
    "from fastcore.utils import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pmap(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    map with `partial`\n",
    "    \"\"\"\n",
    "    return select(partial(func, *args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_code(data, name):\n",
    "    import lineapy\n",
    "    \"use lineapy to get code from data\"\n",
    "    lineapy.save(data, name)\n",
    "    code = lineapy.get(name).get_code()\n",
    "    print(code)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Kedro`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard import\n",
    "```{python}\n",
    "from kedro.pipeline import Pipeline, node\n",
    "from kedro.pipeline.modular_pipeline import pipeline\n",
    "\n",
    "def create_pipeline(**kwargs) -> Pipeline:\n",
    "    return pipeline([\n",
    "        ...\n",
    "    ])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kedro.config import OmegaConfigLoader\n",
    "from kedro.io import DataCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_catalog(conf_source: str = \"../conf\", catalog_source: str = \"catalog\"):\n",
    "    # Initialise a ConfigLoader\n",
    "    conf_loader = OmegaConfigLoader(conf_source)\n",
    "\n",
    "    # Load the data catalog configuration from catalog.yml\n",
    "    conf_catalog = conf_loader.get(catalog_source)\n",
    "\n",
    "    # Create the DataCatalog instance from the configuration\n",
    "    catalog = DataCatalog.from_config(conf_catalog)\n",
    "    \n",
    "    return catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `load_catalog` provides project-aware access to the catalog. The preceding `load_catalog` only works when notebook is run from the project root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from kedro.framework.session import KedroSession\n",
    "from kedro.framework.startup import bootstrap_project\n",
    "from kedro.ipython import _resolve_project_path\n",
    "\n",
    "def load_context(project_path: str = '.', params_only: bool = False, catalog_only: bool = False):\n",
    "    project_path = _resolve_project_path(project_path)\n",
    "    metadata = bootstrap_project(project_path)\n",
    "    # configure_project(metadata.package_name)\n",
    "\n",
    "    session = KedroSession.create(\n",
    "        metadata.package_name, project_path,\n",
    "    )\n",
    "    context = session.load_context()\n",
    "\n",
    "    if params_only:\n",
    "        return context.params\n",
    "    if catalog_only:\n",
    "        return context.catalog\n",
    "    else:\n",
    "        return context\n",
    "\n",
    "load_catalog = partial(load_context, catalog_only=True)\n",
    "load_params = partial(load_context, params_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.utils import patch\n",
    "from speasy.products import SpeasyVariable\n",
    "from humanize import naturalsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def preview(self: SpeasyVariable):\n",
    "    print(\"===========================================\")\n",
    "    print(f\"Name:         {self.name}\")\n",
    "    print(f\"Columns:      {self.columns}\")\n",
    "    print(f\"Values Unit:  {self.unit}\")\n",
    "    print(f\"Memory usage: {naturalsize(self.nbytes)}\")\n",
    "    print(f\"Axes Labels:  {self.axes_labels}\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(f\"Meta-data:    {self.meta}\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(f\"Time Axis:    {self.time[:3]}\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(f\"Values:       {self.values[:3]}\")\n",
    "    print(\"===========================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pydantic import BaseModel\n",
    "from datetime import datetime, timedelta\n",
    "from pandas import Timedelta\n",
    "\n",
    "class DataConfig(BaseModel):\n",
    "    sat_id: str = None\n",
    "    start: datetime = None\n",
    "    end: datetime = None\n",
    "    ts: timedelta = None\n",
    "    coord: str = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Polars`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_tranges(time: pl.Series, tranges: Tuple[list, list]):\n",
    "    \"\"\"\n",
    "    - Filter data by time ranges, return the indices of the time that are in the time ranges (left inclusive, right exclusive)\n",
    "    \"\"\"\n",
    "\n",
    "    starts = tranges[0]\n",
    "    ends = tranges[1]\n",
    "\n",
    "    start_indices = time.search_sorted(starts)\n",
    "    end_indices = time.search_sorted(ends)\n",
    "\n",
    "    return np.concatenate(\n",
    "        [\n",
    "            np.arange(start_index, end_index)\n",
    "            for start_index, end_index in zip(start_indices, end_indices)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def filter_tranges_df(df: pl.DataFrame, tranges: Tuple[list, list], time_col: str = \"time\"):\n",
    "    \"\"\"\n",
    "    - Filter data by time ranges\n",
    "    \"\"\"\n",
    "\n",
    "    time = df[time_col]\n",
    "    filtered_indices = filter_tranges(time, tranges)\n",
    "    return df[filtered_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pycdfpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cdf2pl(file_path: str, var_names: Union[str, list[str]]) -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Convert a CDF file to Polars Dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the CDF file.\n",
    "        var_names (Union[str, List[str]]): The name(s) of the variable(s) to retrieve from the CDF file.\n",
    "\n",
    "    Returns:\n",
    "        pl.LazyFrame: A lazy dataframe containing the requested data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure var_names is always a list\n",
    "    if isinstance(var_names, str):\n",
    "        var_names = [var_names]\n",
    "\n",
    "    cdf = pycdfpp.load(file_path)\n",
    "    epoch_time = pycdfpp.to_datetime64(cdf[\"Epoch\"])\n",
    "    \n",
    "    columns = {\"time\": epoch_time}\n",
    "    \n",
    "    for var_name in var_names:\n",
    "        var_values = cdf[var_name].values\n",
    "        var_attrs = cdf[var_name].attributes\n",
    "        \n",
    "        # Handle FILLVAL\n",
    "        if \"FILLVAL\" in var_attrs:\n",
    "            fillval = var_attrs[\"FILLVAL\"]\n",
    "            var_values[var_values == fillval] = np.nan\n",
    "\n",
    "        if var_values.shape[1] == 1:  # One-dimensional data\n",
    "            columns[var_name] = var_values[:, 0]\n",
    "        else:  # Multi-dimensional data\n",
    "            # Dynamically create column names based on the shape of the field values\n",
    "            for i in range(var_values.shape[1]):\n",
    "                columns[f\"{var_name}_{i}\"] = var_values[:, i]\n",
    "\n",
    "    df = pl.DataFrame(columns).fill_nan(None).lazy()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def plot(self:pl.DataFrame, *args, **kwargs):\n",
    "    return self.to_pandas().plot(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _expand_selectors(items: Any, *more_items: Any) -> list[Any]:\n",
    "    \"\"\"\n",
    "    See `_expand_selectors` in `polars`.\n",
    "    \"\"\"\n",
    "    expanded: list[Any] = []\n",
    "    for item in (\n",
    "        *(\n",
    "            items\n",
    "            if isinstance(items, Collection) and not isinstance(items, str)\n",
    "            else [items]\n",
    "        ),\n",
    "        *more_items,\n",
    "    ):\n",
    "        expanded.append(item)\n",
    "    return expanded\n",
    "\n",
    "def pl_norm(columns, *more_columns) -> pl.Expr:\n",
    "    \"\"\"\n",
    "    Computes the square root of the sum of squares for the given columns.\n",
    "\n",
    "    Args:\n",
    "    *columns (str): Names of the columns.\n",
    "\n",
    "    Returns:\n",
    "    pl.Expr: Expression representing the square root of the sum of squares.\n",
    "    \"\"\"\n",
    "    all_columns = _expand_selectors(columns, *more_columns)\n",
    "    squares = [pl.col(column).pow(2) for column in all_columns]\n",
    "\n",
    "    return sum(squares).sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partition the dataset by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def partition_data_by_year(df: pl.LazyFrame) -> Dict[str, pl.DataFrame]:\n",
    "    \"\"\"Partition the dataset by year\n",
    "\n",
    "    Args:\n",
    "        df: Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Partitioned DataFrame.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.with_columns(year=pl.col(\"time\").dt.year().cast(pl.Utf8))\n",
    "        .collect()\n",
    "        .partition_by(\"year\", include_key=False, as_dict=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "DF_TYPE = Union[pl.DataFrame, pl.LazyFrame, pd.DataFrame]\n",
    "def concat_df(dfs: list[DF_TYPE]) -> DF_TYPE:\n",
    "    \"\"\"Concatenate a list of DataFrames into one DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    match type(dfs[0]):\n",
    "        case pl.DataFrame | pl.LazyFrame:\n",
    "            concat_func = pl.concat\n",
    "        case pandas.DataFrame:\n",
    "            concat_func = pandas.concat\n",
    "        case _:\n",
    "            raise ValueError(f\"Unsupported DataFrame type: {type(dfs[0])}\")\n",
    "    \n",
    "    return concat_func(dfs)\n",
    "                     \n",
    "def concat_partitions(partitioned_input: Dict[str, Callable]):\n",
    "    \"\"\"Concatenate input partitions into one DataFrame.\n",
    "\n",
    "    Args:\n",
    "        partitioned_input: A dictionary with partition ids as keys and load functions as values.\n",
    "    \"\"\"\n",
    "    partitions_data = [\n",
    "        partition_load_func() for partition_load_func in partitioned_input.values()\n",
    "    ]  # load the actual partition data\n",
    "    \n",
    "    result = concat_df(partitions_data)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def format_timedelta(time):\n",
    "    \"\"\"Format timedelta to `timedelta`\"\"\"\n",
    "    if isinstance(time, timedelta):\n",
    "        return time\n",
    "    elif isinstance(time, str):\n",
    "        return pd.Timedelta(time)\n",
    "    elif isinstance(time, int):\n",
    "        return pd.Timedelta(seconds=time)\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type: {type(time)}\")\n",
    "\n",
    "def resample(\n",
    "    df: pl.DataFrame | pl.LazyFrame, \n",
    "    every: timedelta | str | int, period: str | timedelta = None,\n",
    "    time_column='time',\n",
    ") -> pl.DataFrame | pl.LazyFrame:\n",
    "    \"\"\"Resample the DataFrame\"\"\"\n",
    "    if period is None:\n",
    "        period = every\n",
    "    every = format_timedelta(every)\n",
    "    period = format_timedelta(period)\n",
    "    return (\n",
    "        df.sort(time_column)\n",
    "        .group_by_dynamic(time_column, every=every, period=period)\n",
    "        .agg(cs.numeric().mean())\n",
    "        .with_columns(\n",
    "            (pl.col(time_column) + period / 2).dt.cast_time_unit(\"ns\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read `lbl` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import PurePosixPath\n",
    "import fsspec\n",
    "\n",
    "from kedro.io import AbstractDataset\n",
    "from kedro.io.core import get_filepath_str, get_protocol_and_path\n",
    "from kedro.extras.datasets.pandas import CSVDataSet\n",
    "\n",
    "import pdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "HTTP_PROTOCOLS = (\"http\", \"https\")\n",
    "\n",
    "\n",
    "def load_lbl(filepath: str, type: str = \"table\") -> pandas.DataFrame:\n",
    "    \"\"\"Load LBL data.\n",
    "\n",
    "    Args:\n",
    "        filepath: File path to load the data from.\n",
    "        type: Type of data to load. Options are 'table' and 'index'.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the loaded data.\n",
    "    \"\"\"\n",
    "    if type == \"table\":\n",
    "        df = pdr.read(filepath).TABLE\n",
    "    elif type == \"index\":\n",
    "        df = pandas.read_csv(filepath, delimiter=\",\", quotechar='\"')\n",
    "        df.columns = df.columns.str.replace(\" \", \"\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "class LblDataset(AbstractDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        filepath: str,\n",
    "        load_type: str = \"table\",\n",
    "        metadata: Dict[str, Any] = None,\n",
    "    ):\n",
    "        # parse the path and protocol (e.g. file, http, s3, etc.)\n",
    "        protocol, path = get_protocol_and_path(filepath)\n",
    "        self._protocol = protocol\n",
    "        self._filepath = PurePosixPath(path)\n",
    "\n",
    "        self._fs = fsspec.filesystem(self._protocol)\n",
    "\n",
    "        self.load_type = load_type\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def _load(self):\n",
    "        # using get_filepath_str ensures that the protocol and path are appended correctly for different filesystems\n",
    "        load_path = get_filepath_str(self._filepath, self._protocol)\n",
    "\n",
    "        if self._protocol in HTTP_PROTOCOLS:\n",
    "            import pooch\n",
    "\n",
    "            local_fp = pooch.retrieve(load_path, known_hash=None)\n",
    "        else:\n",
    "            local_fp = load_path\n",
    "\n",
    "        return load_lbl(local_fp, self.load_type)\n",
    "\n",
    "    def _save(self):\n",
    "        pass\n",
    "\n",
    "    def _describe(self):\n",
    "        \"\"\"Returns a dict that describes the attributes of the dataset.\"\"\"\n",
    "        return dict(filepath=self._filepath, protocol=self._protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from humanize import naturalsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_memory_usage(data):\n",
    "    datatype = type(data)\n",
    "    match datatype:\n",
    "        case pl.DataFrame:\n",
    "            size = data.estimated_size()\n",
    "        case pd.DataFrame:\n",
    "            size = data.memory_usage().sum()\n",
    "        case xr.DataArray:\n",
    "            size = data.nbytes\n",
    "\n",
    "    logger.info(f\"{naturalsize(size)} ({datatype.__name__})\")\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_file(url, local_dir=\"./\", file_name=None):\n",
    "    \"\"\"\n",
    "    Download a file from a URL and save it locally.\n",
    "\n",
    "    Returns:\n",
    "    file_path (str): Path to the downloaded file.\n",
    "    \"\"\"\n",
    "    if file_name is None:\n",
    "        file_name = url.split(\"/\")[-1]\n",
    "\n",
    "    file_path = os.path.join(local_dir, file_name)\n",
    "    dir = os.path.dirname(file_path)\n",
    "    if not os.path.isdir(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        logger.debug(f\"Downloading from {url}\")\n",
    "        response = requests.get(url)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "    return file_path\n",
    "\n",
    "def check_fgm(vec: xr.DataArray):\n",
    "    # check if time is monotonic increasing\n",
    "    logger.info(\"Check if time is monotonic increasing\")\n",
    "    assert vec.time.to_series().is_monotonic_increasing\n",
    "    # check available time difference\n",
    "    logger.info(\n",
    "        f\"Available time delta: {vec.time.diff(dim='time').to_series().unique()}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def col_renamer(lbl: str):\n",
    "    if lbl.startswith(\"BX\"):\n",
    "        return \"BX\"\n",
    "    if lbl.startswith(\"BY\"):\n",
    "        return \"BY\"\n",
    "    if lbl.startswith(\"BZ\"):\n",
    "        return \"BZ\"\n",
    "    return lbl\n",
    "\n",
    "\n",
    "def df2ts(\n",
    "    df: Union[pandas.DataFrame, pl.DataFrame, pl.LazyFrame], cols, attrs=None, name=None\n",
    "):\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            raise KeyError(f\"Expected column {col} not found in the dataframe.\")\n",
    "\n",
    "    if isinstance(df, pl.LazyFrame):\n",
    "        df = df.collect()\n",
    "\n",
    "    # Prepare data\n",
    "    data = df[cols]\n",
    "\n",
    "    # Prepare coordinates\n",
    "    time = df.index if isinstance(df, pandas.DataFrame) else df[\"time\"]\n",
    "\n",
    "    # Create the DataArray\n",
    "    coords = {\"time\": time, \"v_dim\": cols}\n",
    "\n",
    "    return xr.DataArray(data, coords=coords, attrs=attrs, name=name)\n",
    "\n",
    "\n",
    "def sat_get_fgm_from_df(df: Union[pandas.DataFrame, pl.DataFrame, pl.LazyFrame]):\n",
    "    attrs = {\"coordinate_system\": \"se\", \"units\": \"nT\"}\n",
    "\n",
    "    return df2ts(df, cols=[\"BX\", \"BY\", \"BZ\"], attrs=attrs, name=\"sat_fgm\")\n",
    "\n",
    "\n",
    "def juno_get_state(df: Union[pandas.DataFrame, pl.DataFrame, pl.LazyFrame]):\n",
    "    attrs = {\"coordinate_system\": \"se\", \"units\": \"km\"}\n",
    "    return df2ts(df, cols=[\"X\", \"Y\", \"Z\"], attrs=attrs, name=\"sat_state\")\n",
    "\n",
    "\n",
    "def calc_vec_mag(vec) -> DataArray:\n",
    "    return linalg.norm(vec, dims=\"v_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dispatch(pl.DataFrame)\n",
    "def calc_time_diff(data: pl.DataFrame): \n",
    "    return data.get_column('time').diff(null_behavior=\"drop\").unique().sort()\n",
    "\n",
    "@dispatch(pl.LazyFrame)\n",
    "def calc_time_diff(\n",
    "    data: pl.LazyFrame\n",
    ") -> pl.Series: \n",
    "    return calc_time_diff(data.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flox.xarray import xarray_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Helper function to calculate combined standard deviation\n",
    "def calc_combined_std(col_name):\n",
    "    return (\n",
    "        pl.concat_list([pl.col(col_name).shift(-2), pl.col(col_name).shift(2)])\n",
    "        .list.eval(pl.element().std())\n",
    "        .flatten()\n",
    "        .alias(f\"{col_name}_combined_std\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dispatch(xr.DataArray, object)\n",
    "def compute_index_std(data: DataArray, tau):\n",
    "    \"\"\"\n",
    "    Examples\n",
    "    --------\n",
    "    >>> i1 = index_std(juno_fgm_b, tau)\n",
    "    \"\"\"\n",
    "\n",
    "    # NOTE: large tau values will speed up the computation\n",
    "\n",
    "    # Resample the data based on the provided time interval.\n",
    "    grouped_data = data.resample(time=pandas.Timedelta(tau, unit=\"s\"))\n",
    "\n",
    "    # Compute the standard deviation for all groups\n",
    "    vec_stds = linalg.norm(grouped_data.std(dim=\"time\"), dims=\"v_dim\")\n",
    "    # vec_stds = grouped_data.map(calc_vec_std) # NOTE: This is way much slower (like 30x slower)\n",
    "\n",
    "    offset = pandas.Timedelta(tau / 2, unit=\"s\")\n",
    "    vec_stds[\"time\"] = vec_stds[\"time\"] + offset\n",
    "\n",
    "    vec_stds_next = vec_stds.assign_coords(\n",
    "        {\"time\": vec_stds[\"time\"] - pandas.Timedelta(tau, unit=\"s\")}\n",
    "    )\n",
    "    vec_stds_previous = vec_stds.assign_coords(\n",
    "        {\"time\": vec_stds[\"time\"] + pandas.Timedelta(tau, unit=\"s\")}\n",
    "    )\n",
    "    return np.minimum(vec_stds / vec_stds_next, vec_stds / vec_stds_previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "def _compute_index_fluctuation_old(df: pl.DataFrame, tau) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the fluctuation index based on the given DataFrame, and tau value.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pl.DataFrame): The input DataFrame.\n",
    "    - tau (int): The time interval value.\n",
    "\n",
    "    Returns:\n",
    "    - pl.DataFrame: DataFrame with calculated 'index_fluctuation' column.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> result_df = compute_index_fluctuation(df, tau)\n",
    "    \"\"\"\n",
    "    if isinstance(tau, (int, float)):\n",
    "        tau = timedelta(seconds=tau)\n",
    "\n",
    "    # Group and compute standard deviations\n",
    "    group_df = (\n",
    "        df.group_by_dynamic(\"time\", every=tau / 2, period=tau)\n",
    "        .agg(\n",
    "            pl.col([\"BX\", \"BY\", \"BZ\"]),\n",
    "            pl.col(\"BX\").std().alias(\"BX_std\"),\n",
    "            pl.col(\"BY\").std().alias(\"BY_std\"),\n",
    "            pl.col(\"BZ\").std().alias(\"BZ_std\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl_norm(\"BX_std\", \"BY_std\", \"BZ_std\").alias(\"B_std\"),\n",
    "        )\n",
    "        .drop(\"BX_std\", \"BY_std\", \"BZ_std\")\n",
    "    )\n",
    "\n",
    "    # Compute fluctuation index\n",
    "    index_fluctuation_df = (\n",
    "        group_df.with_columns(\n",
    "            calc_combined_std(\"BX\"),\n",
    "            calc_combined_std(\"BY\"),\n",
    "            calc_combined_std(\"BZ\"),\n",
    "        )\n",
    "        .drop(\"BX\", \"BY\", \"BZ\")\n",
    "        .with_columns(\n",
    "            pl_norm(\"BX_combined_std\", \"BY_combined_std\", \"BZ_combined_std\").alias(\n",
    "                \"B_combined_std\"\n",
    "            ),\n",
    "            pl.sum_horizontal(\n",
    "                pl.col(\"B_std\").shift(-2), pl.col(\"B_std\").shift(2)\n",
    "            ).alias(\"B_added_std\"),\n",
    "        )\n",
    "        .drop(\"BX_combined_std\", \"BY_combined_std\", \"BZ_combined_std\")\n",
    "        .with_columns(\n",
    "            (pl.col(\"B_combined_std\") / pl.col(\"B_added_std\")).alias(\n",
    "                \"index_fluctuation\"\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return index_fluctuation_df\n",
    "\n",
    "    # NOTE: the following code is about 2x slower than the above code\n",
    "    # group_df.with_columns(\n",
    "    #     pl.concat_list([pl.col(\"BX_group\").shift(-2), pl.col(\"BX_group\").shift(2)]),\n",
    "    # ).explode(\"BX_group\").sort(\"time\").group_by(\"time\").agg(\n",
    "    #     pl.col(\"BX_group\").std().alias(\"BX_combined_std\"),\n",
    "    # )\n",
    "\n",
    "    # NOTE: the following code is about 2x slower than the above code\n",
    "    # pl.concat(\n",
    "    #     [\n",
    "    #         group_df.with_columns(pl.col(\"BX_group\").shift(-2)).explode(\"BX_group\"),\n",
    "    #         group_df.with_columns(pl.col(\"BX_group\").shift(2)).explode(\"BX_group\"),\n",
    "    #     ]\n",
    "    # ).sort(\"time\").group_by(\"time\").agg(\n",
    "    #     pl.col(\"BX_group\").std().alias(\"BX_combined_std\"),\n",
    "    # )\n",
    "\n",
    "\n",
    "# NOTE: the two implementation of computing the fluctuation are equivalent, but the following one is about a little bit slower.\n",
    "def _compute_index_fluctuation_xr(data: xr.DataArray, tau):\n",
    "    # Resample the data based on the provided time interval.\n",
    "    grouped_data = data.resample(time=pandas.Timedelta(tau, unit=\"s\"))\n",
    "\n",
    "    # Pre-compute std for all groups\n",
    "    vec_stds = linalg.norm(grouped_data.std(dim=\"time\"), dims=\"v_dim\")\n",
    "\n",
    "    fluctuation_values = []\n",
    "    group_keys = list(grouped_data.groups.keys())\n",
    "\n",
    "    # Iterate over the groups, skipping the first and the last one.\n",
    "    for i in range(1, len(group_keys) - 1):\n",
    "        prev_std = vec_stds[i - 1]\n",
    "        next_std = vec_stds[i + 1]\n",
    "\n",
    "        prev_group_indices = grouped_data.groups[group_keys[i - 1]]\n",
    "        next_group_indices = grouped_data.groups[group_keys[i + 1]]\n",
    "        prev_group = data[prev_group_indices]\n",
    "        next_group = data[next_group_indices]\n",
    "\n",
    "        combined_group = xr.concat([prev_group, next_group], dim=\"time\")\n",
    "        combined_std = calc_vec_std(combined_group)\n",
    "\n",
    "        fluctuation = combined_std / (prev_std + next_std)\n",
    "        fluctuation_values.append(fluctuation)\n",
    "    return DataArray(fluctuation_values, dims=[\"time\"])\n",
    "\n",
    "\n",
    "def compute_index_fluctuation(data, tau):\n",
    "    \"\"\"helper function to compute fluctuation index\n",
    "\n",
    "    Notes: the results returned are a little bit different for the two implementations (because of the implementation of `std`).\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(data, pl.DataFrame):\n",
    "        return _compute_index_fluctuation_old(data, tau)\n",
    "    if isinstance(data, xr.DataArray):\n",
    "        return compute_index_fluctuation_xr(data, tau)\n",
    "\n",
    "def compute_index_fluctuation_xr(data: xr.DataArray, tau: int) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Computes the fluctuation index for a given data array based on a specified time interval.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The xarray DataArray containing the data to be processed.\n",
    "    - tau: Time interval in seconds for resampling.\n",
    "\n",
    "    Returns:\n",
    "    - fluctuation: xarray DataArray containing the fluctuation indices.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "        ddof=0 is used for calculating the standard deviation. (ddof=1 is for sample standard deviation)\n",
    "    \"\"\"\n",
    "\n",
    "    # Resample the data based on the provided time interval.\n",
    "    grouped_data = data.resample(time=pandas.Timedelta(tau, unit=\"s\"))\n",
    "\n",
    "    # Pre-compute the standard deviation for all groups\n",
    "    vec_stds = linalg.norm(grouped_data.std(dim=\"time\"), dims=\"v_dim\")\n",
    "\n",
    "    # Assign coordinates for pre and next groups based on time offset\n",
    "    offset = pandas.Timedelta(tau, unit=\"s\")\n",
    "    pre_stds = vec_stds.assign_coords({\"time\": vec_stds[\"time\"] - offset})\n",
    "    next_stds = vec_stds.assign_coords({\"time\": vec_stds[\"time\"] + offset})\n",
    "\n",
    "    # Offset the keys of the group dictionary to get previous and next groups\n",
    "    groups_dict = grouped_data.groups\n",
    "\n",
    "    # Create DataArrays for previous and next time labels using the slices from the groups dictionaries\n",
    "    prev_labels = xr.concat(\n",
    "        [\n",
    "            xr.DataArray(\n",
    "                key + offset,\n",
    "                dims=[\"time\"],\n",
    "                coords={\"time\": data.time[slice]},\n",
    "                name=\"time\",\n",
    "            )\n",
    "            for key, slice in groups_dict.items()\n",
    "        ],\n",
    "        dim=\"time\",\n",
    "    )\n",
    "    next_labels = xr.concat(\n",
    "        [\n",
    "            xr.DataArray(\n",
    "                key - offset,\n",
    "                dims=[\"time\"],\n",
    "                coords={\"time\": data.time[slice]},\n",
    "                name=\"time\",\n",
    "            )\n",
    "            for key, slice in groups_dict.items()\n",
    "        ],\n",
    "        dim=\"time\",\n",
    "    )\n",
    "\n",
    "    # Concatenate the previous and next labels into a single DataArray\n",
    "    labels = xr.concat([prev_labels, next_labels], dim=\"y\")\n",
    "\n",
    "    # Compute the combined standard deviation for the data using the labels\n",
    "    combined_stds = linalg.norm(xarray_reduce(data, labels, func=\"std\"), dims=\"v_dim\")\n",
    "\n",
    "    # Calculate the fluctuation index\n",
    "    fluctuation = combined_stds / (pre_stds + next_stds)\n",
    "    fluctuation[\"time\"] = fluctuation[\"time\"] + pandas.Timedelta(tau / 2, unit=\"s\")\n",
    "\n",
    "    return fluctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_diff(data: DataArray, tau):\n",
    "    grouped_data = data.resample(time=pandas.Timedelta(tau, unit='s'))\n",
    "\n",
    "    dvecs = grouped_data.first()-grouped_data.last()\n",
    "    vec_mean_mags = grouped_data.map(calc_vec_mean_mag)\n",
    "    vec_diffs = linalg.norm(dvecs, dims='v_dim') / vec_mean_mags\n",
    "    \n",
    "    # vec_diffs = grouped_data.map(calc_vec_relative_diff) # NOTE: this is slower than the above implementation.\n",
    "    # INFO: Do your spatial and temporal indexing (e.g. .sel() or .isel()) early in the pipeline, especially before calling resample() or groupby(). Grouping and resampling triggers some computation on all the blocks, which in theory should commute with indexing, but this optimization hasnâ€™t been implemented in Dask yet. (See Dask issue #746).\n",
    "    \n",
    "    offset = pandas.Timedelta(tau/2, unit='s')\n",
    "    vec_diffs['time'] = vec_diffs['time'] + offset\n",
    "    return vec_diffs\n",
    "\n",
    "\n",
    "# %%\n",
    "def _compute_indices_old(df: pl.DataFrame, tau):\n",
    "    \"\"\"\n",
    "    Compute all index based on the given DataFrame and tau value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - df (pl.DataFrame): The input DataFrame.\n",
    "    - tau (int): The time interval value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - tuple: Tuple containing DataFrame results for fluctuation index, standard deviation index, and 'index_num'.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> indices = compute_indices(df, tau)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Simply shift to calculate index_std would not work correctly if data is missing, like `std_next = pl.col(\"B_std\").shift(-2)`.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(tau, (int, float)):\n",
    "        tau = timedelta(seconds=tau)\n",
    "\n",
    "    b_cols = [\"BX\", \"BY\", \"BZ\"]\n",
    "    db_cols = [\"d\" + col_name + \"_vec\" for col_name in b_cols]\n",
    "\n",
    "    std_next = pl.col(\"B_std\").shift(-2)\n",
    "    std_current = pl.col(\"B_std\")\n",
    "    std_previous = pl.col(\"B_std\").shift(2)\n",
    "    pl_std_index = pl.min_horizontal(\n",
    "        [std_current / std_previous, std_current / std_next]\n",
    "    )\n",
    "\n",
    "    # Compute fluctuation index\n",
    "    group_df = (\n",
    "        df.with_columns(pl_norm(\"BX\", \"BY\", \"BZ\").alias(\"B\"))\n",
    "        .group_by_dynamic(\"time\", every=tau / 2, period=tau)\n",
    "        .agg(\n",
    "            pl.count(),\n",
    "            pl.col(b_cols),\n",
    "            pl.col(\"BX\").std().alias(\"BX_std\"),\n",
    "            pl.col(\"BY\").std().alias(\"BY_std\"),\n",
    "            pl.col(\"BZ\").std().alias(\"BZ_std\"),\n",
    "            pl.col(\"B\").mean().alias(\"B_mean\"),\n",
    "            *pl_dvec(b_cols),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl_norm(\"BX_std\", \"BY_std\", \"BZ_std\").alias(\"B_std\"),\n",
    "            pl_norm(db_cols).alias(\"dB_vec\"),\n",
    "        )\n",
    "        .drop(\"BX_std\", \"BY_std\", \"BZ_std\")\n",
    "    )\n",
    "\n",
    "    indices = (\n",
    "        group_df.with_columns(\n",
    "            calc_combined_std(\"BX\"),\n",
    "            calc_combined_std(\"BY\"),\n",
    "            calc_combined_std(\"BZ\"),\n",
    "        )\n",
    "        .drop(\"BX\", \"BY\", \"BZ\")\n",
    "        .with_columns(\n",
    "            pl_norm(\"BX_combined_std\", \"BY_combined_std\", \"BZ_combined_std\").alias(\n",
    "                \"B_combined_std\"\n",
    "            ),\n",
    "            pl.sum_horizontal(\n",
    "                pl.col(\"B_std\").shift(-2), pl.col(\"B_std\").shift(2)\n",
    "            ).alias(\"B_added_std\"),\n",
    "        )\n",
    "        .drop(\"BX_combined_std\", \"BY_combined_std\", \"BZ_combined_std\")\n",
    "        .with_columns(\n",
    "            pl_std_index.alias(\"index_std\"),\n",
    "            (pl.col(\"B_combined_std\") / pl.col(\"B_added_std\")).alias(\n",
    "                \"index_fluctuation\"\n",
    "            ),\n",
    "            (pl.col(\"dB_vec\") / pl.col(\"B_mean\")).alias(\"index_diff\"),\n",
    "        )\n",
    "    )\n",
    "    return indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
