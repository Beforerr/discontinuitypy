{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Finding magnetic discontinuities\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06-Oct-23 21:31:05: UserWarning: The pandas version installed (1.5.3) does not match the supported pandas version in Modin (2.1.X). This may cause undesired side effects!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "#| code-summary: \"Import all the packages needed for the project\"\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "from ids_finder.utils.basic import *\n",
    "import polars as pl\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "try:\n",
    "    import modin.pandas as pd\n",
    "    import modin.pandas as mpd\n",
    "    from modin.config import ProgressBar\n",
    "    ProgressBar.enable()\n",
    "except ImportError:\n",
    "    import pandas as pd\n",
    "import pandas\n",
    "    \n",
    "import numpy as np\n",
    "from xarray_einstats import linalg\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "import pdpipe as pdp\n",
    "from multipledispatch import dispatch\n",
    "\n",
    "from typing import Any, Collection, Callable\n",
    "\n",
    "from xarray.core.dataarray import DataArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Stages\n",
    "\n",
    "- [ ] Smoothing\n",
    "- [ ] Interpolating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID identification (limited feature extraction / anomaly detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first index is $$ \\frac{\\sigma(B)}{Max(\\sigma(B_-),\\sigma(B_+))} $$\n",
    "The second index is $$ \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)} $$\n",
    "The ﬁrst two conditions guarantee that the ﬁeld changes of the IDs identiﬁed are large enough to be distinguished from the stochastic ﬂuctuations on magnetic ﬁelds, while the third is a supplementary condition toreduce the uncertainty of recognition.\n",
    "\n",
    "third index (relative field jump) is $$ \\frac{| \\Delta \\vec{B} |}{|B_{bg}|} $$ a supplementary condition to reduce the uncertainty of recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ids_finder.utils.basic import _expand_selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "def pl_format_time(tau):\n",
    "    return [\n",
    "        pl.col(\"time\").alias(\"tstart\"),\n",
    "        (pl.col(\"time\") + tau).dt.cast_time_unit(\"ns\").alias(\"tstop\"),\n",
    "        (pl.col(\"time\") + tau / 2).dt.cast_time_unit(\"ns\"),\n",
    "    ]\n",
    "\n",
    "def pl_dvec(columns, *more_columns):\n",
    "    all_columns = _expand_selectors(columns, *more_columns)\n",
    "    return [\n",
    "        (pl.col(column).first() - pl.col(column).last()).alias(f\"d{column}_vec\")\n",
    "        for column in all_columns\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_std(\n",
    "    df: pl.DataFrame, \n",
    "    tau) -> pl.DataFrame:\n",
    "    b_cols = [\"BX\", \"BY\", \"BZ\"]\n",
    "    b_std_cols = [col_name + \"_std\" for col_name in b_cols]\n",
    "\n",
    "    std_df = (\n",
    "        df.group_by_dynamic(\"time\", every=tau / 2, period=tau)\n",
    "        .agg(\n",
    "            pl.count(),\n",
    "            pl.col(b_cols).std(ddof=0).map_alias(lambda col_name: col_name + \"_std\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl_norm(b_std_cols).alias(\"B_std\"),\n",
    "        )\n",
    "        .drop(b_std_cols)\n",
    "    )\n",
    "    return std_df\n",
    "\n",
    "\n",
    "def compute_combinded_std(df: pl.DataFrame, tau) -> pl.DataFrame:\n",
    "    b_cols = [\"BX\", \"BY\", \"BZ\"]\n",
    "    b_combined_std_cols = [col_name + \"_combined_std\" for col_name in b_cols]\n",
    "    offsets = [0 * tau, tau / 2]\n",
    "    combined_std_dfs = []\n",
    "    for offset in offsets:\n",
    "        truncated_df = df.select(\n",
    "            (pl.col(\"time\") - offset).dt.truncate(tau, offset=offset).alias(\"time\"),\n",
    "            pl.col(b_cols),\n",
    "        )\n",
    "\n",
    "        prev_df = truncated_df.select(\n",
    "            (pl.col(\"time\") + tau).dt.cast_time_unit(\"ns\"),\n",
    "            pl.col(b_cols),\n",
    "        )\n",
    "\n",
    "        next_df = truncated_df.select(\n",
    "            (pl.col(\"time\") - tau).dt.cast_time_unit(\"ns\"),\n",
    "            pl.col(b_cols),\n",
    "        )\n",
    "\n",
    "        temp_combined_std_df = (\n",
    "            pl.concat([prev_df, next_df])\n",
    "            .group_by(\"time\")\n",
    "            .agg(\n",
    "                pl.col(b_cols)\n",
    "                .std(ddof=0)\n",
    "                .map_alias(lambda col_name: col_name + \"_combined_std\"),\n",
    "            )\n",
    "            .with_columns(pl_norm(b_combined_std_cols).alias(\"B_combined_std\"))\n",
    "            .drop(b_combined_std_cols)\n",
    "            .sort(\"time\")\n",
    "        )\n",
    "\n",
    "        combined_std_dfs.append(temp_combined_std_df)\n",
    "\n",
    "    combined_std_df = pl.concat(combined_std_dfs)\n",
    "    return combined_std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dispatch(pl.LazyFrame, object)\n",
    "def compute_index_std(df: pl.LazyFrame, tau, join_strategy=\"inner\"):  # noqa: F811\n",
    "    \"\"\"\n",
    "    Compute the standard deviation index based on the given DataFrame and tau value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - df (pl.LazyFrame): The input DataFrame.\n",
    "    - tau (int): The time interval value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - pl.LazyFrame: DataFrame with calculated 'index_std' column.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> index_std_df = compute_index_std_pl(df, tau)\n",
    "    >>> index_std_df\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Simply shift to calculate index_std would not work correctly if data is missing, like `std_next = pl.col(\"B_std\").shift(-2)`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(tau, (int, float)):\n",
    "        tau = timedelta(seconds=tau)\n",
    "\n",
    "    if \"B_std\" in df.columns:\n",
    "        std_df = df\n",
    "    else:\n",
    "        # Compute standard deviations\n",
    "        std_df = compute_std(df, tau)\n",
    "\n",
    "    # Calculate the standard deviation index\n",
    "    prev_std_df = std_df.select(\n",
    "        (pl.col(\"time\") + tau).dt.cast_time_unit(\"ns\"),\n",
    "        pl.col(\"B_std\").alias(\"B_std_prev\"),\n",
    "        pl.col(\"count\").alias(\"count_prev\"),\n",
    "    )\n",
    "\n",
    "    next_std_df = std_df.select(\n",
    "        (pl.col(\"time\") - tau).dt.cast_time_unit(\"ns\"),\n",
    "        pl.col(\"B_std\").alias(\"B_std_next\"),\n",
    "        pl.col(\"count\").alias(\"count_next\")\n",
    "    )\n",
    "\n",
    "    index_std_df = (\n",
    "        std_df.join(prev_std_df, on=\"time\", how=join_strategy)\n",
    "        .join(next_std_df, on=\"time\", how=join_strategy)\n",
    "        .with_columns(\n",
    "            (pl.col(\"B_std\") / (pl.max_horizontal(\"B_std_prev\", \"B_std_next\"))).alias(\n",
    "                \"index_std\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return index_std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_index_diff(df, tau):\n",
    "    b_cols = [\"BX\", \"BY\", \"BZ\"]\n",
    "    db_cols = [\"d\" + col_name + \"_vec\" for col_name in b_cols]\n",
    "\n",
    "    index_diff = (\n",
    "        df.with_columns(pl_norm(b_cols).alias(\"B\"))\n",
    "        .group_by_dynamic(\"time\", every=tau / 2, period=tau)\n",
    "        .agg(\n",
    "            pl.col(\"B\").mean().alias(\"B_mean\"),\n",
    "            *pl_dvec(b_cols),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl_norm(db_cols).alias(\"dB_vec\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col(\"dB_vec\") / pl.col(\"B_mean\")).alias(\"index_diff\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return index_diff\n",
    "\n",
    "\n",
    "@dispatch(pl.LazyFrame, timedelta)\n",
    "def compute_indices(\n",
    "    df: pl.LazyFrame, \n",
    "    tau: timedelta,\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Compute all index based on the given DataFrame and tau value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame.\n",
    "    tau : datetime.timedelta\n",
    "        Time interval value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple : \n",
    "        Tuple containing DataFrame results for fluctuation index, \n",
    "        standard deviation index, and 'index_num'.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> indices = compute_indices(df, tau)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Simply shift to calculate index_std would not work correctly if data is missing, \n",
    "        like `std_next = pl.col(\"B_std\").shift(-2)`.\n",
    "    - Drop null though may lose some IDs (using the default `join_strategy`). \n",
    "        Because we could not tell if it is a real ID or just a partial wave \n",
    "        from incomplete data without previous or/and next std. \n",
    "        Hopefully we can pick up the lost ones with smaller tau.\n",
    "    - TODO: Can be optimized further, but this is already fast enough.\n",
    "        - TEST: if `join` can be improved by shift after filling the missing values.\n",
    "        - TEST: if `list` in `polars` really fast?\n",
    "    \"\"\"\n",
    "    join_strategy = \"inner\"\n",
    "    \n",
    "    std_df = compute_std(df, tau)\n",
    "    combined_std_df = compute_combinded_std(df, tau)\n",
    "\n",
    "    index_std = compute_index_std(std_df, tau)\n",
    "    index_diff = compute_index_diff(df, tau)\n",
    "\n",
    "    indices = (\n",
    "        index_std.join(index_diff, on=\"time\")\n",
    "        .join(combined_std_df, on=\"time\", how=join_strategy)\n",
    "        .with_columns(\n",
    "            pl.sum_horizontal(\"B_std_prev\", \"B_std_next\").alias(\"B_added_std\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col(\"B_std\") / (pl.max_horizontal(\"B_std_prev\", \"B_std_next\"))).alias(\n",
    "                \"index_std\"\n",
    "            ),\n",
    "            (pl.col(\"B_combined_std\") / pl.col(\"B_added_std\")).alias(\n",
    "                \"index_fluctuation\"\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "@dispatch(pl.DataFrame, timedelta)\n",
    "def compute_indices(    # noqa: F811\n",
    "    df: pl.DataFrame, \n",
    "    tau: timedelta,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    wrapper for `compute_indices` with `pl.LazyFrame` input.\n",
    "    \"\"\"\n",
    "    return compute_indices(df.lazy(), tau).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of the standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of fluctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of the relative field jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dispatch(object, xr.DataArray)\n",
    "def get_candidate_data(candidate, data, coord:str=None, neighbor:int=0) -> xr.DataArray:\n",
    "    duration = candidate['tstop'] - candidate['tstart']\n",
    "    offset = neighbor*duration\n",
    "    temp_tstart = candidate['tstart'] - offset\n",
    "    temp_tstop = candidate['tstop'] + offset\n",
    "    \n",
    "    return data.sel(time=slice(temp_tstart,  temp_tstop))\n",
    "\n",
    "@dispatch(object, pl.DataFrame)\n",
    "def get_candidate_data(candidate, data, coord:str=None, neighbor:int=0) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Notes\n",
    "    -----\n",
    "    much slower than `get_candidate_data_xr`\n",
    "    \"\"\"\n",
    "    duration = candidate['tstart'] - candidate['tstop']\n",
    "    offset = neighbor*duration\n",
    "    temp_tstart = candidate['tstart'] - offset\n",
    "    temp_tstop = candidate['tstop'] + offset\n",
    "    \n",
    "    temp_data = data.filter(\n",
    "        pl.col(\"time\").is_between(temp_tstart, temp_tstop)\n",
    "    )\n",
    "    \n",
    "    return df2ts(temp_data, [\"BX\", \"BY\", \"BZ\"], attrs={\"coordinate_system\": coord, \"units\": \"nT\"})\n",
    "\n",
    "def get_candidates(candidates: pd.DataFrame, candidate_type=None, num:int=4):\n",
    "    \n",
    "    if candidate_type is not None:\n",
    "        _candidates = candidates[candidates['type'] == candidate_type]\n",
    "    else:\n",
    "        _candidates = candidates\n",
    "    \n",
    "    # Sample a specific number of candidates if num is provided and it's less than the total number\n",
    "    if num < len(_candidates):\n",
    "        logger.info(f\"Sampling {num} {candidate_type} candidates out of {len(_candidates)}\")\n",
    "        return _candidates.sample(num)\n",
    "    else:\n",
    "        return _candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID parameters (full feature extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of duration\n",
    "- Define $d^* = \\max( | dB / dt | ) $, and then define time interval where $| dB/dt |$ decreases to $d^*/4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "THRESHOLD_RATIO  = 1/4\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def calc_duration(vec: xr.DataArray, threshold_ratio=THRESHOLD_RATIO) -> pandas.Series:\n",
    "    # NOTE: gradient calculated at the edge is not reliable.\n",
    "    vec_diff = vec.differentiate(\"time\", datetime_unit=\"s\").isel(time=slice(1,-1))\n",
    "    vec_diff_mag = linalg.norm(vec_diff, dims='v_dim')\n",
    "\n",
    "    # Determine d_star based on trend\n",
    "    if vec_diff_mag.isnull().all():\n",
    "        raise ValueError(\"The differentiated vector magnitude contains only NaN values. Cannot compute duration.\")\n",
    "    \n",
    "    d_star_index = vec_diff_mag.argmax(dim=\"time\")\n",
    "    d_star = vec_diff_mag[d_star_index]\n",
    "    d_time = vec_diff_mag.time[d_star_index]\n",
    "    \n",
    "    threshold = d_star * threshold_ratio\n",
    "\n",
    "    start_time, end_time = find_start_end_times(vec_diff_mag, d_time, threshold)\n",
    "\n",
    "    dict = {\n",
    "        'd_star': d_star.item(),\n",
    "        'd_time': d_time.values,\n",
    "        'threshold': threshold.item(),\n",
    "        'd_tstart': start_time,\n",
    "        'd_tstop': end_time,\n",
    "    }\n",
    "\n",
    "    return pandas.Series(dict)\n",
    "\n",
    "def calc_d_duration(vec: xr.DataArray, d_time, threshold) -> pd.Series:\n",
    "    vec_diff = vec.differentiate(\"time\", datetime_unit=\"s\")\n",
    "    vec_diff_mag = linalg.norm(vec_diff, dims='v_dim')\n",
    "\n",
    "    start_time, end_time = find_start_end_times(vec_diff_mag, d_time, threshold)\n",
    "\n",
    "    return pandas.Series({\n",
    "        'd_tstart': start_time,\n",
    "        'd_tstop': end_time,\n",
    "    })\n",
    " \n",
    "def find_start_end_times(vec_diff_mag: xr.DataArray, d_time, threshold) -> Tuple[pd.Timestamp, pd.Timestamp]:\n",
    "    # Determine start time\n",
    "    pre_vec_mag = vec_diff_mag.sel(time=slice(None, d_time))\n",
    "    start_time = get_time_from_condition(pre_vec_mag, threshold, \"last_below\")\n",
    "\n",
    "    # Determine stop time\n",
    "    post_vec_mag = vec_diff_mag.sel(time=slice(d_time, None))\n",
    "    end_time = get_time_from_condition(post_vec_mag, threshold, \"first_below\")\n",
    "\n",
    "    return start_time, end_time\n",
    "\n",
    "\n",
    "def get_time_from_condition(vec: xr.DataArray, threshold, condition_type) -> pd.Timestamp:\n",
    "    if condition_type == \"first_below\":\n",
    "        condition = vec < threshold\n",
    "        index_choice = 0\n",
    "    elif condition_type == \"last_below\":\n",
    "        condition = vec < threshold\n",
    "        index_choice = -1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown condition_type: {condition_type}\")\n",
    "\n",
    "    where_result = np.where(condition)[0]\n",
    "\n",
    "    if len(where_result) > 0:\n",
    "        return vec.time[where_result[index_choice]].values\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_candidate_duration(candidate: pd.Series, data) -> pd.Series:\n",
    "    try:\n",
    "        candidate_data = get_candidate_data(candidate, data)\n",
    "        return calc_duration(candidate_data)\n",
    "    except Exception as e:\n",
    "        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\") # can not be serialized\n",
    "        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "def calc_candidate_d_duration(candidate, data) -> pd.Series:\n",
    "    try:\n",
    "        if pd.isnull(candidate['d_tstart']) or pd.isnull(candidate['d_tstop']):\n",
    "            candidate_data = get_candidate_data(candidate, data, neighbor=1)\n",
    "            d_time = candidate['d_time']\n",
    "            threshold = candidate['threshold']\n",
    "            return calc_d_duration(candidate_data, d_time, threshold)\n",
    "        else:\n",
    "            return pandas.Series({\n",
    "                'd_tstart': candidate['d_tstart'],\n",
    "                'd_tstop': candidate['d_tstop'],\n",
    "            })\n",
    "    except Exception as e:\n",
    "        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calibrate_candidate_duration(\n",
    "    candidate: pd.Series, data:xr.DataArray, data_resolution, ratio = 3/4\n",
    "):\n",
    "    \"\"\"\n",
    "    Calibrates the candidate duration. \n",
    "    - If only one of 'd_tstart' or 'd_tstop' is provided, calculates the missing one based on the provided one and 'd_time'.\n",
    "    - Then if this is not enough points between 'd_tstart' and 'd_tstop', returns None for both.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - candidate (pd.Series): The input candidate with potential missing 'd_tstart' or 'd_tstop'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - pd.Series: The calibrated candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_notnull = pd.notnull(candidate['d_tstart'])\n",
    "    stop_notnull = pd.notnull(candidate['d_tstop']) \n",
    "    \n",
    "    match start_notnull, stop_notnull:\n",
    "        case (True, True):\n",
    "            d_tstart = candidate['d_tstart']\n",
    "            d_tstop = candidate['d_tstop']\n",
    "        case (True, False):\n",
    "            d_tstart = candidate['d_tstart']\n",
    "            d_tstop = candidate['d_time'] -  candidate['d_tstart'] + candidate['d_time']\n",
    "        case (False, True):\n",
    "            d_tstart = candidate['d_time'] -  candidate['d_tstop'] + candidate['d_time']\n",
    "            d_tstop = candidate['d_tstop']\n",
    "        case (False, False):\n",
    "            return pandas.Series({\n",
    "                'd_tstart': None,\n",
    "                'd_tstop': None,\n",
    "            })\n",
    "    \n",
    "    duration = d_tstop - d_tstart\n",
    "    num_of_points_between = data.time.sel(time=slice(d_tstart, d_tstop)).count().item()\n",
    "    \n",
    "    if num_of_points_between <= (duration/data_resolution) * ratio:\n",
    "        d_tstart = None\n",
    "        d_tstop = None\n",
    "    \n",
    "    return pandas.Series({\n",
    "        'd_tstart': d_tstart,\n",
    "        'd_tstop': d_tstop,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID classification\n",
    "\n",
    "In this method, TDs and RDs satisfy $ \\frac{ |B_N| }{ |B_{bg}| } < 0.2$ and $ | \\frac{ \\Delta |B| }{ |B_{bg}| } | > 0.4$ B BN bg ∣∣ ∣∣ , < D 0.2 B B bg ∣∣ ∣ ∣ , respectively. Moreover, IDs with < 0.4 B BN bg ∣∣ ∣∣ , < D 0.2 B B bg ∣∣ ∣ ∣ could be either TDs or RDs, and so are termed EDs. Similarly, NDs are defined as > 0.4 B BN bg ∣∣ ∣∣ , > D 0.2 B B bg ∣∣ ∣ ∣ because they can be neither TDs nor RDs. It is worth noting that EDs and NDs here are not physical concepts like RDs and TDs. RDs or TDs correspond to specific types of structures in the MHD framework, while EDs and NDs are introduced just to better quantify the statistical results.\n",
    "\n",
    "\n",
    "Criteria Used to Classify Discontinuities on the Basis of Magnetic Data Type\n",
    "\n",
    "| Type   |  $\\|B_n/B\\|$      | $\\| \\Delta B / B \\|$  |\n",
    "|----------|-------------|------|\n",
    "| Rotational (RD) | large | small |\n",
    "| Tangential (TD) | small |  large |\n",
    "| Either (ED) | small | small |\n",
    "| Neither (ND) | large | large |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### minimum variance analysis (MVA)\n",
    "\n",
    "To ensure the accuracy of MVA, only when the ratio of the middle to the minimum eigenvalue (labeled QMVA for simplicity) is larger than 3 are the results used for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- `Vl_x`, `Vl_y`, `Vl_z`: Maximum variance direction eigenvector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "BnOverB_RD_lower_threshold = 0.4\n",
    "dBOverB_RD_upper_threshold = 0.2\n",
    "\n",
    "BnOverB_TD_upper_threshold = 0.2\n",
    "dBOverB_TD_lower_threshold = dBOverB_RD_upper_threshold\n",
    "\n",
    "BnOverB_ED_upper_threshold = BnOverB_RD_lower_threshold\n",
    "dBOverB_ED_upper_threshold = dBOverB_TD_lower_threshold\n",
    "\n",
    "BnOverB_ND_lower_threshold = BnOverB_TD_upper_threshold\n",
    "dBOverB_ND_lower_threshold = dBOverB_RD_upper_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def minvar(data):\n",
    "    \"\"\"\n",
    "    see `pyspedas.cotrans.minvar`\n",
    "    This program computes the principal variance directions and variances of a\n",
    "    vector quantity as well as the associated eigenvalues.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    data:\n",
    "        Vxyz, an (npoints, ndim) array of data(ie Nx3)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vrot:\n",
    "        an array of (npoints, ndim) containing the rotated data in the new coordinate system, ijk.\n",
    "        Vi(maximum direction)=vrot[0,:]\n",
    "        Vj(intermediate direction)=vrot[1,:]\n",
    "        Vk(minimum variance direction)=Vrot[2,:]\n",
    "    v:\n",
    "        an (ndim,ndim) array containing the principal axes vectors\n",
    "        Maximum variance direction eigenvector, Vi=v[*,0]\n",
    "        Intermediate variance direction, Vj=v[*,1] (descending order)\n",
    "    w:\n",
    "        the eigenvalues of the computation\n",
    "    \"\"\"\n",
    "\n",
    "    #  Min var starts here\n",
    "    # data must be Nx3\n",
    "    vecavg = np.nanmean(np.nan_to_num(data, nan=0.0), axis=0)\n",
    "\n",
    "    mvamat = np.zeros((3, 3))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            mvamat[i, j] = np.nanmean(np.nan_to_num(data[:, i] * data[:, j], nan=0.0)) - vecavg[i] * vecavg[j]\n",
    "\n",
    "    # Calculate eigenvalues and eigenvectors\n",
    "    w, v = np.linalg.eigh(mvamat, UPLO='U')\n",
    "\n",
    "    # Sorting to ensure descending order\n",
    "    w = np.abs(w)\n",
    "    idx = np.flip(np.argsort(w))\n",
    "\n",
    "    # IDL compatability\n",
    "    if True:\n",
    "        if np.sum(w) == 0.0:\n",
    "            idx = [0, 2, 1]\n",
    "\n",
    "    w = w[idx]\n",
    "    v = v[:, idx]\n",
    "\n",
    "    # Rotate intermediate var direction if system is not Right Handed\n",
    "    YcrossZdotX = v[0, 0] * (v[1, 1] * v[2, 2] - v[2, 1] * v[1, 2])\n",
    "    if YcrossZdotX < 0:\n",
    "        v[:, 1] = -v[:, 1]\n",
    "        # v[:, 2] = -v[:, 2] # Should not it is being flipped at Z-axis?\n",
    "\n",
    "    # Ensure minvar direction is along +Z (for FAC system)\n",
    "    if v[2, 2] < 0:\n",
    "        v[:, 2] = -v[:, 2]\n",
    "        v[:, 1] = -v[:, 1]\n",
    "\n",
    "    vrot = np.array([np.dot(row, v) for row in data])\n",
    "\n",
    "    return vrot, v, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_classification_index(\n",
    "    data: xr.DataArray\n",
    ") -> pandas.Series:\n",
    "\n",
    "    vrot, v, w = minvar(data.to_numpy()) # NOTE: using `.to_numpy()` will significantly speed up the computation.\n",
    "    Vl = v[:,0] # Maximum variance direction eigenvector\n",
    "\n",
    "    B_rot = xr.DataArray(vrot, dims=['time', 'v_dim'], coords={'time': data.time})\n",
    "    B = calc_vec_mag(B_rot)\n",
    "    \n",
    "    # Compute dB for each component\n",
    "    dB_values = [B_rot.isel(v_dim = i, time=0) - B_rot.isel(v_dim = i, time=-1) for i in range(3)]\n",
    "    \n",
    "    # Compute mean values\n",
    "    B_mean = B.mean(dim=\"time\")\n",
    "    B_n_mean = B_rot.isel(v_dim=2).mean(dim=\"time\")\n",
    "    BnOverB = B_n_mean / B_mean # BnOverB = np.abs(B_n / B).mean(dim=\"time\")\n",
    "\n",
    "    dB = B.isel(time=-1) - B.isel(time=0)\n",
    "    dBOverB = np.abs(dB / B_mean)\n",
    "    dBOverB_max = (B.max(dim=\"time\") - B.min(dim=\"time\")) / B_mean\n",
    "    \n",
    "    results = {\n",
    "        'Vl_x': Vl[0],\n",
    "        'Vl_y': Vl[1],\n",
    "        'Vl_z': Vl[2],\n",
    "        'eig0': w[0],\n",
    "        'eig1': w[1],\n",
    "        'eig2': w[2],\n",
    "        'Q_mva': w[1]/w[2],\n",
    "        'B': B_mean.item(),\n",
    "        'B_n': B_n_mean.item(),\n",
    "        'dB': dB.item(),\n",
    "        'BnOverB': BnOverB.item(), \n",
    "        'dBOverB': dBOverB.item(),\n",
    "        'dBOverB_max': dBOverB_max.item(),\n",
    "        'dB_l': dB_values[0].item(),\n",
    "        'dB_m': dB_values[1].item(),\n",
    "        'dB_n': dB_values[2].item(),\n",
    "        }\n",
    "    return pandas.Series(results)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def classify_id(BnOverB, dBOverB):\n",
    "    BnOverB = np.abs(np.asarray(BnOverB))\n",
    "    dBOverB = np.asarray(dBOverB)\n",
    "\n",
    "    s1 = (BnOverB > BnOverB_RD_lower_threshold)\n",
    "    s2 = (dBOverB > dBOverB_RD_upper_threshold)\n",
    "    s3 = (BnOverB > BnOverB_TD_upper_threshold)\n",
    "    s4 = s2 # note: s4 = (dBOverB > dBOverB_TD_lower_threshold)\n",
    "    \n",
    "    RD = s1 & ~s2\n",
    "    TD = ~s3 & s4\n",
    "    ED = ~s1 & ~s4\n",
    "    ND = s3 & s2\n",
    "\n",
    "    # Create an empty result array with the same shape\n",
    "    result = np.empty_like(BnOverB, dtype=object)\n",
    "\n",
    "    result[RD] = \"RD\"\n",
    "    result[TD] = \"TD\"\n",
    "    result[ED] = \"ED\"\n",
    "    result[ND] = \"ND\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field rotation angles\n",
    "The PDF of the field rotation angles across the solar-wind IDs is well fitted by the exponential function exp(−θ/)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_rotation_angle(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - v1: The first vector.\n",
    "    - v2: The second vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    if v1.shape != v2.shape:\n",
    "        raise ValueError(\"Vectors must have the same shape.\")\n",
    "\n",
    "    # convert xr.Dataarray to numpy arrays\n",
    "    if isinstance(v1, DataArray):\n",
    "        v1 = v1.to_numpy()\n",
    "    if isinstance(v2, DataArray):\n",
    "        v2 = v2.to_numpy()\n",
    "    \n",
    "    # Normalize the vectors\n",
    "    v1_u = v1 / np.linalg.norm(v1, axis=-1, keepdims=True)\n",
    "    v2_u = v2 / np.linalg.norm(v2, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Calculate the cosine of the angle for each time step\n",
    "    cosine_angle = np.sum(v1_u * v2_u, axis=-1)\n",
    "    \n",
    "    # Clip the values to handle potential floating point errors\n",
    "    cosine_angle = np.clip(cosine_angle, -1, 1)\n",
    "    \n",
    "    angle = np.arccos(cosine_angle)\n",
    "    \n",
    "    # Convert the angles from radians to degrees\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def calc_candidate_rotation_angle(candidates, data:  xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle(s) at two different time steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    tstart = candidates['d_tstart']\n",
    "    tstop = candidates['d_tstop']\n",
    "    \n",
    "    # Convert Series to numpy arrays if necessary\n",
    "    if isinstance(tstart, pd.Series):\n",
    "        tstart = tstart.to_numpy()\n",
    "        tstop = tstop.to_numpy()\n",
    "        # no need to Handle NaT values (as `calibrate_candidate_duration` will handle this)\n",
    "    \n",
    "    # Get the vectors at the two time steps\n",
    "    vecs_before = data.sel(time=tstart, method=\"nearest\")\n",
    "    vecs_after = data.sel(time=tstop, method=\"nearest\")\n",
    "    \n",
    "    # Compute the rotation angle(s)\n",
    "    rotation_angles = calc_rotation_angle(vecs_before, vecs_after)\n",
    "    return rotation_angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign satellite locations to the discontinuities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_candidate_location(candidate, location_data: DataArray):\n",
    "    return location_data.sel(time = candidate['d_time'], method=\"nearest\").to_series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_ID_filter_condition(\n",
    "    index_std_threshold = 2,\n",
    "    index_fluc_threshold = 1,\n",
    "    index_diff_threshold = 0.1,\n",
    "    sparse_num = 15\n",
    "):\n",
    "    return (\n",
    "        (pl.col(\"index_std\") > index_std_threshold)\n",
    "        & (pl.col(\"index_fluctuation\") > index_fluc_threshold)\n",
    "        & (pl.col(\"index_diff\") > index_diff_threshold)\n",
    "        & (\n",
    "            pl.col(\"index_std\").is_finite()\n",
    "        )  # for cases where neighboring groups have std=0\n",
    "        & (\n",
    "            pl.col(\"count\") > sparse_num\n",
    "        )  # filter out sparse intervals, which may give unreasonable results.\n",
    "        & (\n",
    "            pl.col(\"count_prev\") > sparse_num\n",
    "        ) \n",
    "        & (\n",
    "            pl.col(\"count_next\") > sparse_num\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pdpipe.util import out_of_place_col_insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patch `pdp.ApplyToRows` to work with `modin` and `xorbits` DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _transform(self: pdp.ApplyToRows, X, verbose):\n",
    "    new_cols = X.apply(self._func, axis=1)\n",
    "    if isinstance(new_cols, (pd.Series, pandas.Series)):\n",
    "        loc = len(X.columns)\n",
    "        if self._follow_column:\n",
    "            loc = X.columns.get_loc(self._follow_column) + 1\n",
    "        return out_of_place_col_insert(\n",
    "            X=X, series=new_cols, loc=loc, column_name=self._colname\n",
    "        )\n",
    "    if isinstance(new_cols, (mpd.DataFrame, pandas.DataFrame)):\n",
    "        sorted_cols = sorted(list(new_cols.columns))\n",
    "        new_cols = new_cols[sorted_cols]\n",
    "        if self._follow_column:\n",
    "            inter_X = X\n",
    "            loc = X.columns.get_loc(self._follow_column) + 1\n",
    "            for colname in new_cols.columns:\n",
    "                inter_X = out_of_place_col_insert(\n",
    "                    X=inter_X,\n",
    "                    series=new_cols[colname],\n",
    "                    loc=loc,\n",
    "                    column_name=colname,\n",
    "                )\n",
    "                loc += 1\n",
    "            return inter_X\n",
    "        assign_map = {\n",
    "            colname: new_cols[colname] for colname in new_cols.columns\n",
    "        }\n",
    "        return X.assign(**assign_map)\n",
    "    raise TypeError(  # pragma: no cover\n",
    "        \"Unexpected type generated by applying a function to a DataFrame.\"\n",
    "        \" Only Series and DataFrame are allowed.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_candidate_classification_index(candidate, data):\n",
    "    return calc_classification_index(\n",
    "        data.sel(time=slice(candidate[\"d_tstart\"], candidate[\"d_tstop\"]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_to_dataframe(\n",
    "    data: pl.DataFrame, # orignal Dataframe\n",
    ")->pd.DataFrame:\n",
    "    \"convert data into a pandas/modin DataFrame\"\n",
    "    if isinstance(data, pl.LazyFrame):\n",
    "        data = data.collect().to_pandas(use_pyarrow_extension_array=True)\n",
    "    if isinstance(data, pl.DataFrame):\n",
    "        data = data.to_pandas(use_pyarrow_extension_array=True)\n",
    "    if not isinstance(data, pd.DataFrame):  # `modin` supports\n",
    "        data = pd.DataFrame(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipelines` Class for processing IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class IDsPipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    # fmt: off\n",
    "    def calc_duration(self, sat_fgm: xr.DataArray):\n",
    "        return pdp.PdPipeline([\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: calc_candidate_duration(candidate, sat_fgm),\n",
    "                func_desc=\"calculating duration parameters\"\n",
    "            ),\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: calc_candidate_d_duration(candidate, sat_fgm),\n",
    "                func_desc=\"calculating duration parameters if needed\"\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def calibrate_duration(self, sat_fgm, data_resolution):\n",
    "        return \\\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: calibrate_candidate_duration(candidate, sat_fgm, data_resolution),\n",
    "                func_desc=\"calibrating duration parameters if needed\"\n",
    "            )\n",
    "\n",
    "    def classify_id(self, sat_fgm):\n",
    "        return pdp.PdPipeline([\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: calc_candidate_classification_index(candidate, sat_fgm),\n",
    "                func_desc='calculating index \"q_mva\", \"BnOverB\" and \"dBOverB\"'\n",
    "            ),\n",
    "            pdp.ColByFrameFunc(\n",
    "                \"type\",\n",
    "                lambda df: classify_id(df[\"BnOverB\"], df[\"dBOverB\"]),\n",
    "                func_desc=\"classifying the type of the ID\"\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def calc_rotation_angle(self, sat_fgm):\n",
    "        return \\\n",
    "            pdp.ColByFrameFunc(\n",
    "                \"rotation_angle\",\n",
    "                lambda df: calc_candidate_rotation_angle(df, sat_fgm),\n",
    "                func_desc=\"calculating rotation angle\",\n",
    "            ) \n",
    "\n",
    "    def assign_coordinates(self, sat_state: xr.DataArray):\n",
    "        \"NOTE: not optimized, quite slow\"\n",
    "        return \\\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: get_candidate_location(candidate, sat_state),\n",
    "                func_desc=\"assigning coordinates\",\n",
    "            )\n",
    "    # fmt: on\n",
    "    # ... you can add more methods as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes that the candidates only require a small portion of the data so we can compress the data to speed up the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compress_data_by_cands(\n",
    "    data: pl.DataFrame, candidates: pl.DataFrame, tau: timedelta\n",
    "):\n",
    "    \"\"\"Compress the data for parallel processing\"\"\"\n",
    "    ttstarts = candidates[\"tstart\"] - tau\n",
    "    ttstops = candidates[\"tstop\"] + tau\n",
    "\n",
    "    ttstarts_index = data[\"time\"].search_sorted(ttstarts)\n",
    "    ttstops_index = data[\"time\"].search_sorted(ttstops)\n",
    "\n",
    "    indices = np.concatenate(\n",
    "        [\n",
    "            np.arange(ttstart_index, ttstop_index + 1)\n",
    "            for ttstart_index, ttstop_index in zip(ttstarts_index, ttstops_index)\n",
    "        ]\n",
    "    )  # faster than `pl.arange`\n",
    "    indices_unique = (\n",
    "        pl.Series(indices).unique().sort()\n",
    "    )  # faster than `np.unique(index)`\n",
    "    return data[indices_unique]\n",
    "\n",
    "\n",
    "# data.filter(\n",
    "#     pl.any_horizontal(\n",
    "#         pl.col('time').is_between(*ttrange) for ttrange in ttranges\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "def sort_df(df: pl.DataFrame, col='time'):\n",
    "    if df.get_column(col).is_sorted():\n",
    "        return df.set_sorted(col)\n",
    "    else:\n",
    "        return df.sort(col)\n",
    "\n",
    "def process_candidates(\n",
    "    candidates_pl: pl.DataFrame,  # potential candidates DataFrame\n",
    "    sat_fgm: xr.DataArray,  # satellite FGM data\n",
    "    data_resolution: timedelta,  # time resolution of the data\n",
    "    sat_state: pl.DataFrame = None  # satellite state data\n",
    ") -> pl.DataFrame:  # processed candidates DataFrame\n",
    "    \n",
    "    test_eq(sat_fgm.shape[1],3)\n",
    "    candidates = convert_to_dataframe(candidates_pl)\n",
    "    \n",
    "    id_pipelines = IDsPipeline()\n",
    "    candidates = id_pipelines.calc_duration(sat_fgm).apply(candidates)\n",
    "\n",
    "    # calibrate duration\n",
    "    temp_candidates = candidates.loc[\n",
    "        lambda df: df[\"d_tstart\"].isnull() | df[\"d_tstop\"].isnull()\n",
    "    ]  # temp_candidates = candidates.query('d_tstart.isnull() | d_tstop.isnull()') # not implemented in `modin`\n",
    "\n",
    "    if not temp_candidates.empty:\n",
    "        temp_candidates_updated = id_pipelines.calibrate_duration(\n",
    "            sat_fgm, data_resolution\n",
    "        ).apply(temp_candidates)\n",
    "        candidates.update(temp_candidates_updated)\n",
    "\n",
    "    ids = (\n",
    "        id_pipelines.classify_id(sat_fgm)\n",
    "        + id_pipelines.calc_rotation_angle(sat_fgm)\n",
    "    ).apply(\n",
    "        candidates.dropna()  # Remove candidates with NaN values)\n",
    "    )\n",
    "\n",
    "    if isinstance(ids, mpd.DataFrame):\n",
    "        ids = ids._to_pandas()\n",
    "    if isinstance(ids, pandas.DataFrame):\n",
    "        ids_pl = pl.DataFrame(ids)\n",
    "\n",
    "    ids_pl = sort_df(ids_pl, col=\"d_time\")\n",
    "    sat_state = sort_df(sat_state, col=\"time\")\n",
    "    \n",
    "    ids_pl = ids_pl.join_asof(\n",
    "        sat_state, left_on=\"d_time\", right_on=\"time\", strategy=\"nearest\"\n",
    "    ).drop(\"time_right\")\n",
    "\n",
    "    return ids_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def ids_finder(data: pl.DataFrame, tau, params):\n",
    "    tau = timedelta(seconds=tau)\n",
    "    data_resolution = timedelta(seconds=params[\"data_resolution\"])\n",
    "    bcols = params[\"bcols\"]\n",
    "    data = data.sort(\"time\")\n",
    "\n",
    "    # get candidates\n",
    "    indices = compute_indices(data, tau)\n",
    "    sparse_num = tau / data_resolution // 3\n",
    "    filter_condition = get_ID_filter_condition(sparse_num=sparse_num)\n",
    "    candidates_pl = indices.filter(filter_condition).with_columns(pl_format_time(tau))\n",
    "    candidates = convert_to_dataframe(candidates_pl)\n",
    "\n",
    "    data_c = compress_data_by_cands(data, candidates_pl, tau)\n",
    "    sat_fgm = df2ts(data_c, bcols, attrs={\"units\": \"nT\"})\n",
    "    ids = process_candidates(candidates, sat_fgm, data_resolution)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def extract_features(\n",
    "    partitioned_input: Dict[str, Callable[[], Any]], tau, params\n",
    ") -> pl.DataFrame:\n",
    "    ids = pl.concat(\n",
    "        [\n",
    "            ids_finder(\n",
    "                partition_load()\n",
    "                if isinstance(partition_load, Callable)\n",
    "                else partition_load,\n",
    "                tau,\n",
    "                params,\n",
    "            )\n",
    "            for partition_load in partitioned_input.values()\n",
    "        ]\n",
    "    )  # load the actual partition data\n",
    "\n",
    "    return ids.unique([\"d_time\", \"d_tstart\", \"d_tstop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Generally `mapply` and `modin` are the fastest. `xorbits` is expected to be the fastest but it is not and it is the slowest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "sat = 'jno'\n",
    "coord = 'se'\n",
    "bcols = [\"BX\", \"BY\", \"BZ\"]\n",
    "tau = timedelta(seconds=60)\n",
    "data_resolution = timedelta(seconds=1)\n",
    "\n",
    "if True:\n",
    "    year = 2012\n",
    "    files = f'../data/{sat}_data_{year}.parquet'\n",
    "    output = f'../data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n",
    "\n",
    "    data = pl.scan_parquet(files).set_sorted('time').collect()\n",
    "\n",
    "    indices = compute_indices(data, tau)\n",
    "    # filter condition\n",
    "    sparse_num = tau / data_resolution // 3\n",
    "    filter_condition = get_ID_filter_condition(sparse_num = sparse_num)\n",
    "\n",
    "    candidates_pl = indices.filter(filter_condition).with_columns(pl_format_time(tau)).sort('time')\n",
    "    \n",
    "    data_c = compress_data_by_cands(data, candidates_pl, tau)\n",
    "    sat_fgm = df2ts(data_c, bcols, attrs={\"coordinate_system\": coord, \"units\": \"nT\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "candidates_pd = candidates_pl.to_pandas()\n",
    "candidates_modin = mpd.DataFrame(candidates_pd)\n",
    "# candidates_x = xpd.DataFrame(candidates_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Test different libraries to parallelize the computation\n",
    "#| notest\n",
    "if True:\n",
    "    pdp_test = pdp.ApplyToRows(\n",
    "        lambda candidate: calc_candidate_duration(candidate, sat_fgm),  # fast a little bit\n",
    "        # lambda candidate: calc_duration(get_candidate_data_xr(candidate, sat_fgm)),\n",
    "        # lambda candidate: calc_duration(sat_fgm.sel(time=slice(candidate['tstart'], candidate['tstop']))),\n",
    "        func_desc=\"calculating duration parameters\",\n",
    "    )\n",
    "    \n",
    "    # process_candidates(candidates_modin, sat_fgm, sat_state, data_resolution)\n",
    "    \n",
    "    # ---\n",
    "    # successful cases\n",
    "    # ---\n",
    "    # candidates_pd.mapply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works, 4.2 secs\n",
    "    # candidates_pd.mapply(calc_candidate_duration, axis=1, data=sat_fgm) # this works, but a little bit slower, 6.7 secs\n",
    "    \n",
    "    # candidates_pd.apply(calc_candidate_duration, axis=1, data=sat_fgm) # Standard case: 24+s secs\n",
    "    # candidates_pd.swifter.apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 80 secs\n",
    "    # candidates_pd.swifter.set_dask_scheduler(scheduler=\"threads\").apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 60 secs\n",
    "    # candidates_modin.apply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works with ray, 6 secs # NOTE: can not work with dask\n",
    "    # candidates_x.apply(calc_candidate_duration, axis=1, data=sat_fgm) # 30 seconds\n",
    "    # pdp_test(candidates_modin) # this works, 8 secs\n",
    "    \n",
    "    # ---\n",
    "    # failed cases\n",
    "    # ---\n",
    "    # candidates_modin.apply(calc_candidate_duration, axis=1, data=sat_fgm) # AttributeError: 'DataFrame' object has no attribute 'sel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_c.to_pandas().set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsflex.features import MultipleFeatureDescriptors, FeatureCollection\n",
    "\n",
    "from tsflex.features.integrations import catch22_wrapper\n",
    "from pycatch22 import catch22_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06-Oct-23 21:37:55: RuntimeWarning: There are gaps in the sequence of the BX-series!\n",
      "\n",
      "06-Oct-23 21:37:55: RuntimeWarning: There are gaps in the sequence of the BZ-series!\n",
      "\n",
      "06-Oct-23 21:37:55: RuntimeWarning: There are gaps in the sequence of the BY-series!\n",
      "\n",
      "06-Oct-23 21:38:31: Finished function [[wrapped]__catch22_all] on [('BX',)] with window-stride [0 days 00:01:00, ('0 days 00:00:30',)] in [35.801105260849 seconds]!\n",
      "06-Oct-23 21:38:31: Finished function [[wrapped]__catch22_all] on [('BY',)] with window-stride [0 days 00:01:00, ('0 days 00:00:30',)] in [35.89824819564819 seconds]!\n",
      "06-Oct-23 21:38:31: Finished function [[wrapped]__catch22_all] on [('BZ',)] with window-stride [0 days 00:01:00, ('0 days 00:00:30',)] in [36.15569996833801 seconds]!\n"
     ]
    }
   ],
   "source": [
    "tau_pd = pd.Timedelta(tau)\n",
    "\n",
    "catch22_feats = MultipleFeatureDescriptors(\n",
    "    functions=catch22_wrapper(catch22_all),\n",
    "    series_names=bcols,  # list of signal names\n",
    "    windows = tau_pd, strides=tau_pd/2,\n",
    ")\n",
    "\n",
    "fc = FeatureCollection(catch22_feats)\n",
    "features = fc.calculate(data, return_df=True)  # calculate the features on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pl = pl.DataFrame(features.reset_index()).sort('time')\n",
    "df = candidates_pl.join_asof(features_pl, on='time').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title=\"JUNO Candidates Report\")\n",
    "profile.to_file(\"jno.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(task_dict, number=1):\n",
    "    results = {}\n",
    "    for name, (data, task) in task_dict.items():\n",
    "        try:\n",
    "            time_taken = timeit.timeit(\n",
    "                lambda: task(data),\n",
    "                number=number\n",
    "            )\n",
    "            results[name] = time_taken / number\n",
    "        except Exception as e:\n",
    "            results[name] = str(e)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "func = lambda candidate: calc_candidate_duration(candidate, sat_fgm)\n",
    "task_dict = {\n",
    "    'pandas': (candidates_pd, lambda _: _.apply(func, axis=1)),\n",
    "    'pandas-mapply': (candidates_pd, lambda _: _.mapply(func, axis=1)),\n",
    "    'modin': (candidates_modin, lambda _: _.apply(func, axis=1)),\n",
    "    # 'xorbits': (candidates_x, lambda _: _.apply(func, axis=1)),\n",
    "}\n",
    "\n",
    "results = benchmark(task_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs\n",
    "\n",
    "1. Feature engineering\n",
    "2. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vec_mean_mag(vec: xr.DataArray):\n",
    "    return linalg.norm(vec, dims=\"v_dim\").mean(dim=\"time\")\n",
    "\n",
    "\n",
    "def calc_vec_std(vec: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the standard deviation of a vector.\n",
    "    \"\"\"\n",
    "    return linalg.norm(vec.std(dim=\"time\"), dims=\"v_dim\")\n",
    "\n",
    "\n",
    "def calc_vec_relative_diff(vec: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the relative difference between the last and first elements of a vector.\n",
    "    \"\"\"\n",
    "    dvec = vec.isel(time=-1) - vec.isel(time=0)\n",
    "    return linalg.norm(dvec, dims=\"v_dim\") / linalg.norm(vec, dims=\"v_dim\").mean(\n",
    "        dim=\"time\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `process_candidates`\n",
    "Assign coordinates using `Dataframe.apply` is not optimized, quite slow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_candidates(\n",
    "    candidates: pd.DataFrame, # potential candidates DataFrame\n",
    "    sat_fgm: xr.DataArray, # satellite FGM data\n",
    "    sat_state: xr.DataArray, # satellite state data\n",
    "    data_resolution: timedelta, # time resolution of the data\n",
    ") -> pd.DataFrame: # processed candidates DataFrame\n",
    "    id_pipelines = IDsPipeline()\n",
    "\n",
    "    candidates = id_pipelines.calc_duration(sat_fgm).apply(candidates)\n",
    "\n",
    "    # calibrate duration\n",
    "    temp_candidates = candidates.loc[\n",
    "        lambda df: df[\"d_tstart\"].isnull() | df[\"d_tstop\"].isnull()\n",
    "    ]  # temp_candidates = candidates.query('d_tstart.isnull() | d_tstop.isnull()') # not implemented in `modin`\n",
    "\n",
    "    if not temp_candidates.empty:\n",
    "        candidates.update(\n",
    "            id_pipelines.calibrate_duration(sat_fgm, data_resolution).apply(\n",
    "                temp_candidates\n",
    "            )\n",
    "        )\n",
    "\n",
    "    ids = (\n",
    "        id_pipelines.classify_id(sat_fgm)\n",
    "        + id_pipelines.calc_rotation_angle(sat_fgm)\n",
    "        + id_pipelines.assign_coordinates(sat_state)\n",
    "    ).apply(\n",
    "        candidates.dropna()  # Remove candidates with NaN values)\n",
    "    )\n",
    "\n",
    "    return ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
