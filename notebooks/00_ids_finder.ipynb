{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Finding magnetic discontinuities\n",
    "order: 0\n",
    "---\n",
    "\n",
    "It can be divided into two parts:\n",
    "\n",
    "1. Finding the discontinuities, see [this notebook](./01_ids_detection.ipynb)\n",
    "    - Corresponding to limited feature extraction / anomaly detection\n",
    "\n",
    "    - Output should contain the following:\n",
    "        - \"tstart\" and \"tstop\" of the event\n",
    "\n",
    "2. Calculating the properties of the discontinuities, see [this notebook](./02_ids_properties.ipynb)\n",
    "    - One can use higher time resolution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# | code-summary: \"Import all the packages needed for the project\"\n",
    "import polars as pl\n",
    "from discontinuitypy.detection.variance import detect_variance\n",
    "from discontinuitypy.core.propeties import process_events\n",
    "from space_analysis.ds.ts.io import df2ts\n",
    "from loguru import logger\n",
    "\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes that the candidates only require a small portion of the data so we can compress the data to speed up the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "from beforerr.polars import filter_df_by_ranges\n",
    "\n",
    "\n",
    "def compress_data_by_events(data: pl.DataFrame, events: pl.DataFrame):\n",
    "    \"\"\"Compress the data for parallel processing\"\"\"\n",
    "    starts = events[\"tstart\"]\n",
    "    ends = events[\"tstop\"]\n",
    "    return filter_df_by_ranges(data, starts, ends)\n",
    "\n",
    "\n",
    "def get_bcols(df: pl.LazyFrame):\n",
    "    \"\"\"Get the magnetic field components\"\"\"\n",
    "    bcols = df.collect_schema().names()\n",
    "    bcols.remove(\"time\")\n",
    "    len(bcols) == 3 or logger.error(\"Expect 3 field components\")\n",
    "    return bcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def ids_finder(\n",
    "    detection_df: pl.LazyFrame,  # data used for anomaly dectection (typically low cadence data)\n",
    "    bcols=None,\n",
    "    detect_func: Callable[..., pl.LazyFrame] = detect_variance,\n",
    "    detect_kwargs: dict = {},\n",
    "    extract_df: pl.LazyFrame = None,  # data used for feature extraction (typically high cadence data),\n",
    "    **kwargs,\n",
    "):\n",
    "    bcols = bcols or get_bcols(detection_df)\n",
    "    detection_df = detection_df.select(bcols + [\"time\"]).sort(\"time\")\n",
    "    extract_df = (extract_df or detection_df).sort(\"time\")\n",
    "\n",
    "    events = detect_func(detection_df, bcols=bcols, **detect_kwargs)\n",
    "\n",
    "    data_c = compress_data_by_events(extract_df.collect(), events)\n",
    "    sat_fgm = df2ts(data_c, bcols)\n",
    "    ids = process_events(events, sat_fgm, **kwargs)\n",
    "    return ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
