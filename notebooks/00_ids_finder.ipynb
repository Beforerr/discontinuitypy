{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Finding magnetic discontinuities\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide \n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| code-summary: \"Import all the packages needed for the project\"\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "from ids_finder.utils.basic import *\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "try:\n",
    "    import modin.pandas as pd\n",
    "    import modin.pandas as mpd\n",
    "    from modin.config import ProgressBar\n",
    "    ProgressBar.enable()\n",
    "except ImportError:\n",
    "    import pandas as pd\n",
    "import pandas\n",
    "    \n",
    "import numpy as np\n",
    "from xarray_einstats import linalg\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "import pdpipe as pdp\n",
    "from multipledispatch import dispatch\n",
    "\n",
    "from typing import Any, Collection, Callable\n",
    "\n",
    "from xarray.core.dataarray import DataArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Stages\n",
    "\n",
    "- [ ] Smoothing\n",
    "- [ ] Interpolating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID identification (limited feature extraction / anomaly detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first index is $$ \\frac{\\sigma(B)}{Max(\\sigma(B_-),\\sigma(B_+))} $$\n",
    "The second index is $$ \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)} $$\n",
    "The ﬁrst two conditions guarantee that the ﬁeld changes of the IDs identiﬁed are large enough to be distinguished from the stochastic ﬂuctuations on magnetic ﬁelds, while the third is a supplementary condition toreduce the uncertainty of recognition.\n",
    "\n",
    "third index (relative field jump) is $$ \\frac{| \\Delta \\vec{B} |}{|B_{bg}|} $$ a supplementary condition to reduce the uncertainty of recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ids_finder.utils.basic import _expand_selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# some helper functions\n",
    "def pl_format_time(df: pl.LazyFrame, tau: timedelta):\n",
    "    return df.with_columns(\n",
    "        tstart=pl.col(\"time\"),\n",
    "        tstop=(pl.col(\"time\") + tau).dt.cast_time_unit(\"ns\"),\n",
    "        time=(pl.col(\"time\") + tau / 2).dt.cast_time_unit(\"ns\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def pl_dvec(columns, *more_columns):\n",
    "    all_columns = _expand_selectors(columns, *more_columns)\n",
    "    return [\n",
    "        (pl.col(column).first() - pl.col(column).last()).alias(f\"d{column}_vec\")\n",
    "        for column in all_columns\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compute_std(\n",
    "    df: pl.DataFrame | pl.LazyFrame, tau, b_cols=[\"BX\", \"BY\", \"BZ\"]\n",
    ") -> pl.DataFrame:\n",
    "    b_std_cols = [col_name + \"_std\" for col_name in b_cols]\n",
    "\n",
    "    std_df = (\n",
    "        df.group_by_dynamic(\"time\", every=tau / 2, period=tau)\n",
    "        .agg(\n",
    "            pl.count(),\n",
    "            pl.col(b_cols).std(ddof=0).map_alias(lambda col_name: col_name + \"_std\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl_norm(b_std_cols).alias(\"B_std\"),\n",
    "        )\n",
    "        .drop(b_std_cols)\n",
    "    )\n",
    "    return std_df\n",
    "\n",
    "\n",
    "def compute_combinded_std(\n",
    "    df: pl.DataFrame | pl.LazyFrame, tau, cols\n",
    ") -> pl.DataFrame:\n",
    "    combined_std_cols = [col_name + \"_combined_std\" for col_name in cols]\n",
    "    offsets = [0 * tau, tau / 2]\n",
    "    combined_std_dfs = []\n",
    "    for offset in offsets:\n",
    "        truncated_df = df.select(\n",
    "            (pl.col(\"time\") - offset).dt.truncate(tau, offset=offset).alias(\"time\"),\n",
    "            pl.col(cols),\n",
    "        )\n",
    "\n",
    "        prev_df = truncated_df.select(\n",
    "            (pl.col(\"time\") + tau).dt.cast_time_unit(\"ns\"),\n",
    "            pl.col(cols),\n",
    "        )\n",
    "\n",
    "        next_df = truncated_df.select(\n",
    "            (pl.col(\"time\") - tau).dt.cast_time_unit(\"ns\"),\n",
    "            pl.col(cols),\n",
    "        )\n",
    "\n",
    "        temp_combined_std_df = (\n",
    "            pl.concat([prev_df, next_df])\n",
    "            .group_by(\"time\")\n",
    "            .agg(\n",
    "                pl.col(cols)\n",
    "                .std(ddof=0)\n",
    "                .map_alias(lambda col_name: col_name + \"_combined_std\"),\n",
    "            )\n",
    "            .with_columns(pl_norm(combined_std_cols).alias(\"B_combined_std\"))\n",
    "            .drop(combined_std_cols)\n",
    "            .sort(\"time\")\n",
    "        )\n",
    "\n",
    "        combined_std_dfs.append(temp_combined_std_df)\n",
    "\n",
    "    combined_std_df = pl.concat(combined_std_dfs)\n",
    "    return combined_std_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dispatch(pl.LazyFrame, object)\n",
    "def compute_index_std(df: pl.LazyFrame, tau, join_strategy=\"inner\"):  # noqa: F811\n",
    "    \"\"\"\n",
    "    Compute the standard deviation index based on the given DataFrame and tau value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - df (pl.LazyFrame): The input DataFrame.\n",
    "    - tau (int): The time interval value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - pl.LazyFrame: DataFrame with calculated 'index_std' column.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> index_std_df = compute_index_std_pl(df, tau)\n",
    "    >>> index_std_df\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Simply shift to calculate index_std would not work correctly if data is missing, like `std_next = pl.col(\"B_std\").shift(-2)`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(tau, (int, float)):\n",
    "        tau = timedelta(seconds=tau)\n",
    "\n",
    "    if \"B_std\" in df.columns:\n",
    "        std_df = df\n",
    "    else:\n",
    "        # Compute standard deviations\n",
    "        std_df = compute_std(df, tau)\n",
    "\n",
    "    # Calculate the standard deviation index\n",
    "    prev_std_df = std_df.select(\n",
    "        (pl.col(\"time\") + tau).dt.cast_time_unit(\"ns\"),\n",
    "        pl.col(\"B_std\").alias(\"B_std_prev\"),\n",
    "        pl.col(\"count\").alias(\"count_prev\"),\n",
    "    )\n",
    "\n",
    "    next_std_df = std_df.select(\n",
    "        (pl.col(\"time\") - tau).dt.cast_time_unit(\"ns\"),\n",
    "        pl.col(\"B_std\").alias(\"B_std_next\"),\n",
    "        pl.col(\"count\").alias(\"count_next\")\n",
    "    )\n",
    "\n",
    "    index_std_df = (\n",
    "        std_df.join(prev_std_df, on=\"time\", how=join_strategy)\n",
    "        .join(next_std_df, on=\"time\", how=join_strategy)\n",
    "        .with_columns(\n",
    "            (pl.col(\"B_std\") / (pl.max_horizontal(\"B_std_prev\", \"B_std_next\"))).alias(\n",
    "                \"index_std\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return index_std_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of fluctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of the relative field jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def compute_index_diff(\n",
    "    df: pl.DataFrame, \n",
    "    tau: timedelta,\n",
    "    cols\n",
    "    ):\n",
    "    db_cols = [\"d\" + col + \"_vec\" for col in cols]\n",
    "\n",
    "    index_diff = (\n",
    "        df.with_columns(pl_norm(cols).alias(\"B\"))\n",
    "        .group_by_dynamic(\"time\", every=tau / 2, period=tau)\n",
    "        .agg(\n",
    "            pl.col(\"B\").mean().alias(\"B_mean\"),\n",
    "            *pl_dvec(cols),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl_norm(db_cols).alias(\"dB_vec\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col(\"dB_vec\") / pl.col(\"B_mean\")).alias(\"index_diff\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return index_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def _compute_indices(\n",
    "    df: pl.LazyFrame, tau: timedelta, cols: list[str] = [\"BX\", \"BY\", \"BZ\"]\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Compute all index based on the given DataFrame and tau value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame.\n",
    "    tau : datetime.timedelta\n",
    "        Time interval value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple :\n",
    "        Tuple containing DataFrame results for fluctuation index,\n",
    "        standard deviation index, and 'index_num'.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> indices = compute_indices(df, tau)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Simply shift to calculate index_std would not work correctly if data is missing,\n",
    "        like `std_next = pl.col(\"B_std\").shift(-2)`.\n",
    "    - Drop null though may lose some IDs (using the default `join_strategy`).\n",
    "        Because we could not tell if it is a real ID or just a partial wave\n",
    "        from incomplete data without previous or/and next std.\n",
    "        Hopefully we can pick up the lost ones with smaller tau.\n",
    "    - TODO: Can be optimized further, but this is already fast enough.\n",
    "        - TEST: if `join` can be improved by shift after filling the missing values.\n",
    "        - TEST: if `list` in `polars` really fast?\n",
    "    \"\"\"\n",
    "    join_strategy = \"inner\"\n",
    "\n",
    "    std_df = compute_std(df, tau, cols)\n",
    "    combined_std_df = compute_combinded_std(df, tau, cols)\n",
    "\n",
    "    index_std = compute_index_std(std_df, tau)\n",
    "    index_diff = compute_index_diff(df, tau, cols)\n",
    "\n",
    "    indices = (\n",
    "        index_std.join(index_diff, on=\"time\")\n",
    "        .join(combined_std_df, on=\"time\", how=join_strategy)\n",
    "        .with_columns(\n",
    "            pl.sum_horizontal(\"B_std_prev\", \"B_std_next\").alias(\"B_added_std\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            (pl.col(\"B_std\") / (pl.max_horizontal(\"B_std_prev\", \"B_std_next\"))).alias(\n",
    "                \"index_std\"\n",
    "            ),\n",
    "            (pl.col(\"B_combined_std\") / pl.col(\"B_added_std\")).alias(\n",
    "                \"index_fluctuation\"\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "def compute_indices(\n",
    "    df: pl.DataFrame, tau: timedelta, bcols: list[str] = [\"BX\", \"BY\", \"BZ\"]\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    wrapper for `compute_indices` with `pl.LazyFrame` input.\n",
    "    \"\"\"\n",
    "    return _compute_indices(df.lazy(), tau, bcols).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID parameters (full feature extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dispatch(object, xr.DataArray)\n",
    "def get_candidate_data(\n",
    "    candidate, data, coord: str = None, neighbor: int = 0\n",
    ") -> xr.DataArray:\n",
    "    duration = candidate[\"tstop\"] - candidate[\"tstart\"]\n",
    "    offset = neighbor * duration\n",
    "    temp_tstart = candidate[\"tstart\"] - offset\n",
    "    temp_tstop = candidate[\"tstop\"] + offset\n",
    "\n",
    "    return data.sel(time=slice(temp_tstart, temp_tstop))\n",
    "\n",
    "\n",
    "@dispatch(object, pl.DataFrame)\n",
    "def get_candidate_data(\n",
    "    candidate, data, coord: str = None, neighbor: int = 0, bcols=[\"BX\", \"BY\", \"BZ\"]\n",
    ") -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Notes\n",
    "    -----\n",
    "    much slower than `get_candidate_data_xr`\n",
    "    \"\"\"\n",
    "    duration = candidate[\"tstart\"] - candidate[\"tstop\"]\n",
    "    offset = neighbor * duration\n",
    "    temp_tstart = candidate[\"tstart\"] - offset\n",
    "    temp_tstop = candidate[\"tstop\"] + offset\n",
    "\n",
    "    temp_data = data.filter(pl.col(\"time\").is_between(temp_tstart, temp_tstop))\n",
    "\n",
    "    return df2ts(temp_data, bcols, attrs={\"coordinate_system\": coord, \"units\": \"nT\"})\n",
    "\n",
    "\n",
    "def get_candidates(candidates: pd.DataFrame, candidate_type=None, num: int = 4):\n",
    "    if candidate_type is not None:\n",
    "        _candidates = candidates[candidates[\"type\"] == candidate_type]\n",
    "    else:\n",
    "        _candidates = candidates\n",
    "\n",
    "    # Sample a specific number of candidates if num is provided and it's less than the total number\n",
    "    if num < len(_candidates):\n",
    "        logger.info(\n",
    "            f\"Sampling {num} {candidate_type} candidates out of {len(_candidates)}\"\n",
    "        )\n",
    "        return _candidates.sample(num)\n",
    "    else:\n",
    "        return _candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of duration\n",
    "- Define $d^* = \\max( | dB / dt | ) $, and then define time interval where $| dB/dt |$ decreases to $d^*/4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "THRESHOLD_RATIO  = 1/4\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def calc_duration(vec: xr.DataArray, threshold_ratio=THRESHOLD_RATIO) -> pandas.Series:\n",
    "    # NOTE: gradient calculated at the edge is not reliable.\n",
    "    vec_diff = vec.differentiate(\"time\", datetime_unit=\"s\").isel(time=slice(1,-1))\n",
    "    vec_diff_mag = linalg.norm(vec_diff, dims='v_dim')\n",
    "\n",
    "    # Determine d_star based on trend\n",
    "    if vec_diff_mag.isnull().all():\n",
    "        raise ValueError(\"The differentiated vector magnitude contains only NaN values. Cannot compute duration.\")\n",
    "    \n",
    "    d_star_index = vec_diff_mag.argmax(dim=\"time\")\n",
    "    d_star = vec_diff_mag[d_star_index]\n",
    "    d_time = vec_diff_mag.time[d_star_index]\n",
    "    \n",
    "    threshold = d_star * threshold_ratio\n",
    "\n",
    "    start_time, end_time = find_start_end_times(vec_diff_mag, d_time, threshold)\n",
    "\n",
    "    dict = {\n",
    "        'd_star': d_star.item(),\n",
    "        'd_time': d_time.values,\n",
    "        'threshold': threshold.item(),\n",
    "        'd_tstart': start_time,\n",
    "        'd_tstop': end_time,\n",
    "    }\n",
    "\n",
    "    return pandas.Series(dict)\n",
    "\n",
    "def calc_d_duration(vec: xr.DataArray, d_time, threshold) -> pd.Series:\n",
    "    vec_diff = vec.differentiate(\"time\", datetime_unit=\"s\")\n",
    "    vec_diff_mag = linalg.norm(vec_diff, dims='v_dim')\n",
    "\n",
    "    start_time, end_time = find_start_end_times(vec_diff_mag, d_time, threshold)\n",
    "\n",
    "    return pandas.Series({\n",
    "        'd_tstart': start_time,\n",
    "        'd_tstop': end_time,\n",
    "    })\n",
    " \n",
    "def find_start_end_times(vec_diff_mag: xr.DataArray, d_time, threshold) -> Tuple[pd.Timestamp, pd.Timestamp]:\n",
    "    # Determine start time\n",
    "    pre_vec_mag = vec_diff_mag.sel(time=slice(None, d_time))\n",
    "    start_time = get_time_from_condition(pre_vec_mag, threshold, \"last_below\")\n",
    "\n",
    "    # Determine stop time\n",
    "    post_vec_mag = vec_diff_mag.sel(time=slice(d_time, None))\n",
    "    end_time = get_time_from_condition(post_vec_mag, threshold, \"first_below\")\n",
    "\n",
    "    return start_time, end_time\n",
    "\n",
    "\n",
    "def get_time_from_condition(vec: xr.DataArray, threshold, condition_type) -> pd.Timestamp:\n",
    "    if condition_type == \"first_below\":\n",
    "        condition = vec < threshold\n",
    "        index_choice = 0\n",
    "    elif condition_type == \"last_below\":\n",
    "        condition = vec < threshold\n",
    "        index_choice = -1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown condition_type: {condition_type}\")\n",
    "\n",
    "    where_result = np.where(condition)[0]\n",
    "\n",
    "    if len(where_result) > 0:\n",
    "        return vec.time[where_result[index_choice]].values\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_candidate_duration(candidate: pd.Series, data) -> pd.Series:\n",
    "    try:\n",
    "        candidate_data = get_candidate_data(candidate, data)\n",
    "        return calc_duration(candidate_data)\n",
    "    except Exception as e:\n",
    "        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\") # can not be serialized\n",
    "        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "def calc_candidate_d_duration(candidate, data) -> pd.Series:\n",
    "    try:\n",
    "        if pd.isnull(candidate['d_tstart']) or pd.isnull(candidate['d_tstop']):\n",
    "            candidate_data = get_candidate_data(candidate, data, neighbor=1)\n",
    "            d_time = candidate['d_time']\n",
    "            threshold = candidate['threshold']\n",
    "            return calc_d_duration(candidate_data, d_time, threshold)\n",
    "        else:\n",
    "            return pandas.Series({\n",
    "                'd_tstart': candidate['d_tstart'],\n",
    "                'd_tstop': candidate['d_tstop'],\n",
    "            })\n",
    "    except Exception as e:\n",
    "        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrates candidate duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calibrate_candidate_duration(\n",
    "    candidate: pd.Series, data:xr.DataArray, data_resolution, ratio = 3/4\n",
    "):\n",
    "    \"\"\"\n",
    "    Calibrates the candidate duration. \n",
    "    - If only one of 'd_tstart' or 'd_tstop' is provided, calculates the missing one based on the provided one and 'd_time'.\n",
    "    - Then if this is not enough points between 'd_tstart' and 'd_tstop', returns None for both.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - candidate (pd.Series): The input candidate with potential missing 'd_tstart' or 'd_tstop'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - pd.Series: The calibrated candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_notnull = pd.notnull(candidate['d_tstart'])\n",
    "    stop_notnull = pd.notnull(candidate['d_tstop']) \n",
    "    \n",
    "    match start_notnull, stop_notnull:\n",
    "        case (True, True):\n",
    "            d_tstart = candidate['d_tstart']\n",
    "            d_tstop = candidate['d_tstop']\n",
    "        case (True, False):\n",
    "            d_tstart = candidate['d_tstart']\n",
    "            d_tstop = candidate['d_time'] -  candidate['d_tstart'] + candidate['d_time']\n",
    "        case (False, True):\n",
    "            d_tstart = candidate['d_time'] -  candidate['d_tstop'] + candidate['d_time']\n",
    "            d_tstop = candidate['d_tstop']\n",
    "        case (False, False):\n",
    "            return pandas.Series({\n",
    "                'd_tstart': None,\n",
    "                'd_tstop': None,\n",
    "            })\n",
    "    \n",
    "    duration = d_tstop - d_tstart\n",
    "    num_of_points_between = data.time.sel(time=slice(d_tstart, d_tstop)).count().item()\n",
    "    \n",
    "    if num_of_points_between <= (duration/data_resolution) * ratio:\n",
    "        d_tstart = None\n",
    "        d_tstop = None\n",
    "    \n",
    "    return pandas.Series({\n",
    "        'd_tstart': d_tstart,\n",
    "        'd_tstop': d_tstop,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minimum variance analysis (MVA) features\n",
    "\n",
    "To ensure the accuracy of MVA, only when the ratio of the middle to the minimum eigenvalue (labeled QMVA for simplicity) is larger than 3 are the results used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def minvar(data):\n",
    "    \"\"\"\n",
    "    see `pyspedas.cotrans.minvar`\n",
    "    This program computes the principal variance directions and variances of a\n",
    "    vector quantity as well as the associated eigenvalues.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    data:\n",
    "        Vxyz, an (npoints, ndim) array of data(ie Nx3)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vrot:\n",
    "        an array of (npoints, ndim) containing the rotated data in the new coordinate system, ijk.\n",
    "        Vi(maximum direction)=vrot[0,:]\n",
    "        Vj(intermediate direction)=vrot[1,:]\n",
    "        Vk(minimum variance direction)=Vrot[2,:]\n",
    "    v:\n",
    "        an (ndim,ndim) array containing the principal axes vectors\n",
    "        Maximum variance direction eigenvector, Vi=v[*,0]\n",
    "        Intermediate variance direction, Vj=v[*,1] (descending order)\n",
    "    w:\n",
    "        the eigenvalues of the computation\n",
    "    \"\"\"\n",
    "\n",
    "    #  Min var starts here\n",
    "    # data must be Nx3\n",
    "    vecavg = np.nanmean(np.nan_to_num(data, nan=0.0), axis=0)\n",
    "\n",
    "    mvamat = np.zeros((3, 3))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            mvamat[i, j] = np.nanmean(np.nan_to_num(data[:, i] * data[:, j], nan=0.0)) - vecavg[i] * vecavg[j]\n",
    "\n",
    "    # Calculate eigenvalues and eigenvectors\n",
    "    w, v = np.linalg.eigh(mvamat, UPLO='U')\n",
    "\n",
    "    # Sorting to ensure descending order\n",
    "    w = np.abs(w)\n",
    "    idx = np.flip(np.argsort(w))\n",
    "\n",
    "    # IDL compatability\n",
    "    if True:\n",
    "        if np.sum(w) == 0.0:\n",
    "            idx = [0, 2, 1]\n",
    "\n",
    "    w = w[idx]\n",
    "    v = v[:, idx]\n",
    "\n",
    "    # Rotate intermediate var direction if system is not Right Handed\n",
    "    YcrossZdotX = v[0, 0] * (v[1, 1] * v[2, 2] - v[2, 1] * v[1, 2])\n",
    "    if YcrossZdotX < 0:\n",
    "        v[:, 1] = -v[:, 1]\n",
    "        # v[:, 2] = -v[:, 2] # Should not it is being flipped at Z-axis?\n",
    "\n",
    "    # Ensure minvar direction is along +Z (for FAC system)\n",
    "    if v[2, 2] < 0:\n",
    "        v[:, 2] = -v[:, 2]\n",
    "        v[:, 1] = -v[:, 1]\n",
    "\n",
    "    vrot = np.array([np.dot(row, v) for row in data])\n",
    "\n",
    "    return vrot, v, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mva_features(data: np.ndarray) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Compute MVA features based on the given data array.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): Input data\n",
    "\n",
    "    Returns:\n",
    "    - List: Computed features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute variance properties\n",
    "    vrot, v, w = minvar(data)\n",
    "\n",
    "    # Maximum variance direction eigenvector\n",
    "    Vl = v[:, 0]\n",
    "\n",
    "    vec_mag = np.linalg.norm(vrot, axis=1)\n",
    "    \n",
    "    # Compute changes in each component of B_rot\n",
    "    dvec = [vrot[0, i] - vrot[-1, i] for i in range(3)]\n",
    "    \n",
    "    # Compute mean values\n",
    "    vec_mag_mean = np.mean(vec_mag)\n",
    "    vec_n_mean = np.mean(vrot[:, 2])\n",
    "    VnOverVmag = vec_n_mean / vec_mag_mean\n",
    "\n",
    "    # Compute relative changes in magnitude\n",
    "    dvec_mag = vec_mag[-1] - vec_mag[0]\n",
    "    dBOverB = np.abs(dvec_mag / vec_mag_mean)\n",
    "    dBOverB_max = (np.max(vec_mag) - np.min(vec_mag)) / vec_mag_mean\n",
    "    \n",
    "    \n",
    "    results = [\n",
    "        Vl[0], Vl[1], Vl[2],\n",
    "        w[0], w[1], w[2],\n",
    "        w[1] / w[2],\n",
    "        vec_mag_mean,\n",
    "        vec_n_mean,\n",
    "        dvec_mag,\n",
    "        VnOverVmag, \n",
    "        dBOverB,\n",
    "        dBOverB_max,\n",
    "        dvec[0], dvec[1], dvec[2]\n",
    "    ]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "from fastcore.test import *\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)  # for reproducibility\n",
    "data = np.random.rand(100, 3)  # 100 time points, 3-dimensional data\n",
    "# Call the mva_features function\n",
    "features = mva_features(data)\n",
    "_features = [0.3631060892452051, 0.8978455426527485, -0.24905290500542857, 0.09753158579102299, 0.086943767300213, 0.07393142040422575, 1.1760056390752571, 0.9609421690770317, 0.6152039820297959, -0.5922397773398479, 0.6402091632847049, 0.61631157045453, 1.2956351134759623, 0.19091785005728523, 0.5182488424049534, 0.4957624347593598]\n",
    "test_eq(features, _features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field rotation angles\n",
    "The PDF of the field rotation angles across the solar-wind IDs is well fitted by the exponential function exp(−θ/)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_rotation_angle(v1: np.ndarray, v2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - v1: The first vector.\n",
    "    - v2: The second vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    if v1.shape != v2.shape:\n",
    "        raise ValueError(\"Vectors must have the same shape.\")\n",
    "    \n",
    "    # Normalize the vectors\n",
    "    v1_u = v1 / np.linalg.norm(v1, axis=-1, keepdims=True)\n",
    "    v2_u = v2 / np.linalg.norm(v2, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Calculate the cosine of the angle for each time step\n",
    "    cosine_angle = np.sum(v1_u * v2_u, axis=-1)\n",
    "    \n",
    "    # Clip the values to handle potential floating point errors\n",
    "    cosine_angle = np.clip(cosine_angle, -1, 1)\n",
    "    \n",
    "    angle = np.arccos(cosine_angle)\n",
    "    \n",
    "    # Convert the angles from radians to degrees\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def calc_candidate_rotation_angle(candidates, data:  xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle(s) at two different time steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    tstart = candidates['d_tstart']\n",
    "    tstop = candidates['d_tstop']\n",
    "    \n",
    "    # Convert Series to numpy arrays if necessary\n",
    "    if isinstance(tstart, pd.Series):\n",
    "        tstart = tstart.to_numpy()\n",
    "        tstop = tstop.to_numpy()\n",
    "        # no need to Handle NaT values (as `calibrate_candidate_duration` will handle this)\n",
    "    \n",
    "    # Get the vectors at the two time steps\n",
    "    vecs_before = data.sel(time=tstart, method=\"nearest\").to_numpy()\n",
    "    vecs_after = data.sel(time=tstop, method=\"nearest\").to_numpy()\n",
    "    \n",
    "    # Compute the rotation angle(s)\n",
    "    rotation_angles = calc_rotation_angle(vecs_before, vecs_after)\n",
    "    return rotation_angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def filter_indices(\n",
    "    df: pl.DataFrame | pl.LazyFrame,\n",
    "    index_std_threshold=2,\n",
    "    index_fluc_threshold=1,\n",
    "    index_diff_threshold=0.1,\n",
    "    sparse_num=15,\n",
    ") -> pl.DataFrame | pl.LazyFrame:\n",
    "    # filter indices to get possible IDs\n",
    "\n",
    "    return df.filter(\n",
    "        pl.col(\"index_std\") > index_std_threshold,\n",
    "        pl.col(\"index_fluctuation\") > index_fluc_threshold,\n",
    "        pl.col(\"index_diff\") > index_diff_threshold,\n",
    "        pl.col(\"index_std\").is_finite(), # for cases where neighboring groups have std=0\n",
    "        pl.col(\"count\") > sparse_num, \n",
    "        pl.col(\"count_prev\") > sparse_num, # filter out sparse intervals, which may give unreasonable results.\n",
    "        pl.col(\"count_next\") > sparse_num, # filter out sparse intervals, which may give unreasonable results.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pdpipe.util import out_of_place_col_insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patch `pdp.ApplyToRows` to work with `modin` and `xorbits` DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _transform(self: pdp.ApplyToRows, X, verbose):\n",
    "    new_cols = X.apply(self._func, axis=1)\n",
    "    if isinstance(new_cols, (pd.Series, pandas.Series)):\n",
    "        loc = len(X.columns)\n",
    "        if self._follow_column:\n",
    "            loc = X.columns.get_loc(self._follow_column) + 1\n",
    "        return out_of_place_col_insert(\n",
    "            X=X, series=new_cols, loc=loc, column_name=self._colname\n",
    "        )\n",
    "    if isinstance(new_cols, (mpd.DataFrame, pandas.DataFrame)):\n",
    "        sorted_cols = sorted(list(new_cols.columns))\n",
    "        new_cols = new_cols[sorted_cols]\n",
    "        if self._follow_column:\n",
    "            inter_X = X\n",
    "            loc = X.columns.get_loc(self._follow_column) + 1\n",
    "            for colname in new_cols.columns:\n",
    "                inter_X = out_of_place_col_insert(\n",
    "                    X=inter_X,\n",
    "                    series=new_cols[colname],\n",
    "                    loc=loc,\n",
    "                    column_name=colname,\n",
    "                )\n",
    "                loc += 1\n",
    "            return inter_X\n",
    "        assign_map = {\n",
    "            colname: new_cols[colname] for colname in new_cols.columns\n",
    "        }\n",
    "        return X.assign(**assign_map)\n",
    "    raise TypeError(  # pragma: no cover\n",
    "        \"Unexpected type generated by applying a function to a DataFrame.\"\n",
    "        \" Only Series and DataFrame are allowed.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_candidate_mva_features(candidate, data: xr.DataArray):\n",
    "\n",
    "    output_names = [\"Vl_x\", \"Vl_y\", \"Vl_z\", \"eig0\", \"eig1\", \"eig2\", 'Q_mva', 'b_mag', 'b_n', 'db_mag', 'bn_over_b', 'db_over_b', 'db_over_b_max', 'db_l', 'db_m', 'db_n']\n",
    "    results = mva_features(\n",
    "        data.sel(time=slice(candidate[\"d_tstart\"], candidate[\"d_tstop\"])).to_numpy()\n",
    "    )\n",
    "    \n",
    "    return pandas.Series(results, output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_to_dataframe(\n",
    "    data: pl.DataFrame | pl.LazyFrame # orignal Dataframe\n",
    ")->pd.DataFrame:\n",
    "    \"convert data into a pandas/modin DataFrame\"\n",
    "    if isinstance(data, pl.LazyFrame):\n",
    "        data = data.collect().to_pandas(use_pyarrow_extension_array=True)\n",
    "    if isinstance(data, pl.DataFrame):\n",
    "        data = data.to_pandas(use_pyarrow_extension_array=True)\n",
    "    if not isinstance(data, pd.DataFrame):  # `modin` supports\n",
    "        data = pd.DataFrame(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipelines` Class for processing IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class IDsPipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    # fmt: off\n",
    "    def calc_duration(self, sat_fgm: xr.DataArray):\n",
    "        return pdp.PdPipeline([\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: calc_candidate_duration(candidate, sat_fgm),\n",
    "                func_desc=\"calculating duration parameters\"\n",
    "            ),\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: calc_candidate_d_duration(candidate, sat_fgm),\n",
    "                func_desc=\"calculating duration parameters if needed\"\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def calibrate_duration(self, sat_fgm, data_resolution):\n",
    "        return \\\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: calibrate_candidate_duration(candidate, sat_fgm, data_resolution),\n",
    "                func_desc=\"calibrating duration parameters if needed\"\n",
    "            )\n",
    "\n",
    "    def calc_mva_features(self, sat_fgm):\n",
    "        return pdp.PdPipeline([\n",
    "            pdp.ApplyToRows(\n",
    "                lambda candidate: calc_candidate_mva_features(candidate, sat_fgm),\n",
    "                func_desc='calculating index \"q_mva\", \"BnOverB\" and \"dBOverB\"'\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def calc_rotation_angle(self, sat_fgm):\n",
    "        return \\\n",
    "            pdp.ColByFrameFunc(\n",
    "                \"rotation_angle\",\n",
    "                lambda df: calc_candidate_rotation_angle(df, sat_fgm),\n",
    "                func_desc=\"calculating rotation angle\",\n",
    "            ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes that the candidates only require a small portion of the data so we can compress the data to speed up the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compress_data_by_cands(\n",
    "    data: pl.DataFrame, candidates: pl.DataFrame, tau: timedelta\n",
    "):\n",
    "    \"\"\"Compress the data for parallel processing\"\"\"\n",
    "    ttstarts = candidates[\"tstart\"] - tau\n",
    "    ttstops = candidates[\"tstop\"] + tau\n",
    "\n",
    "    ttstarts_index = data[\"time\"].search_sorted(ttstarts)\n",
    "    ttstops_index = data[\"time\"].search_sorted(ttstops)\n",
    "\n",
    "    indices = np.concatenate(\n",
    "        [\n",
    "            np.arange(ttstart_index, ttstop_index + 1)\n",
    "            for ttstart_index, ttstop_index in zip(ttstarts_index, ttstops_index)\n",
    "        ]\n",
    "    )  # faster than `pl.arange`\n",
    "    indices_unique = (\n",
    "        pl.Series(indices).unique().sort()\n",
    "    )  # faster than `np.unique(index)`\n",
    "    return data[indices_unique]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def sort_df(df: pl.DataFrame, col=\"time\"):\n",
    "    if df.get_column(col).is_sorted():\n",
    "        return df.set_sorted(col)\n",
    "    else:\n",
    "        return df.sort(col)\n",
    "\n",
    "\n",
    "def process_candidates(\n",
    "    candidates_pl: pl.DataFrame,  # potential candidates DataFrame\n",
    "    sat_fgm: xr.DataArray,  # satellite FGM data\n",
    "    data_resolution: timedelta,  # time resolution of the data\n",
    ") -> pl.DataFrame:  # processed candidates DataFrame\n",
    "    test_eq(sat_fgm.shape[1], 3)\n",
    "    candidates = convert_to_dataframe(candidates_pl)\n",
    "\n",
    "    id_pipelines = IDsPipeline()\n",
    "    candidates = id_pipelines.calc_duration(sat_fgm).apply(candidates)\n",
    "\n",
    "    # calibrate duration\n",
    "    temp_candidates = candidates.loc[\n",
    "        lambda df: df[\"d_tstart\"].isnull() | df[\"d_tstop\"].isnull()\n",
    "    ]  # temp_candidates = candidates.query('d_tstart.isnull() | d_tstop.isnull()') # not implemented in `modin`\n",
    "\n",
    "    if not temp_candidates.empty:\n",
    "        temp_candidates_updated = id_pipelines.calibrate_duration(\n",
    "            sat_fgm, data_resolution\n",
    "        ).apply(temp_candidates)\n",
    "        candidates.update(temp_candidates_updated)\n",
    "\n",
    "    ids = (\n",
    "        id_pipelines.calc_mva_features(sat_fgm)\n",
    "        + id_pipelines.calc_rotation_angle(sat_fgm)\n",
    "    ).apply(\n",
    "        candidates.dropna()  # Remove candidates with NaN values)\n",
    "    )\n",
    "\n",
    "    if isinstance(ids, mpd.DataFrame):\n",
    "        ids = ids._to_pandas()\n",
    "    if isinstance(ids, pandas.DataFrame):\n",
    "        ids_pl = pl.DataFrame(ids)\n",
    "\n",
    "    return ids_pl.pipe(sort_df, col=\"d_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def ids_finder(data: pl.DataFrame, tau: float, params: dict):\n",
    "    tau = timedelta(seconds=tau)\n",
    "    data_resolution = timedelta(seconds=params[\"data_resolution\"])\n",
    "    bcols = params[\"bcols\"]\n",
    "    data = data.sort(\"time\")\n",
    "\n",
    "    # get candidates\n",
    "    indices = compute_indices(data, tau, bcols)\n",
    "    sparse_num = tau / data_resolution // 3\n",
    "    candidates_pl = indices.pipe(filter_indices, sparse_num=sparse_num).pipe(\n",
    "        pl_format_time, tau\n",
    "    ).pipe(convert_to_dataframe)\n",
    "    candidates = (candidates_pl)\n",
    "\n",
    "    data_c = compress_data_by_cands(data, candidates_pl, tau)\n",
    "    sat_fgm = df2ts(data_c, bcols)\n",
    "    ids = process_candidates(candidates, sat_fgm, data_resolution)\n",
    "    return ids\n",
    "\n",
    "def extract_features(\n",
    "    partitioned_input: Dict[str, Callable], tau: float, params\n",
    ") -> pl.DataFrame:\n",
    "    ids = pl.concat(\n",
    "        [\n",
    "            ids_finder(partition_load(), tau, params)\n",
    "            for partition_load in partitioned_input.values()\n",
    "        ]\n",
    "    )\n",
    "    return ids.unique([\"d_time\", \"d_tstart\", \"d_tstop\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Generally `mapply` and `modin` are the fastest. `xorbits` is expected to be the fastest but it is not and it is the slowest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "sat = 'jno'\n",
    "coord = 'se'\n",
    "cols = [\"BX\", \"BY\", \"BZ\"]\n",
    "tau = timedelta(seconds=60)\n",
    "data_resolution = timedelta(seconds=1)\n",
    "\n",
    "if True:\n",
    "    year = 2012\n",
    "    files = f'../data/{sat}_data_{year}.parquet'\n",
    "    output = f'../data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n",
    "\n",
    "    data = pl.scan_parquet(files).set_sorted('time').collect()\n",
    "\n",
    "    indices = compute_indices(data, tau)\n",
    "    # filter condition\n",
    "    sparse_num = tau / data_resolution // 3\n",
    "    filter_condition = filter_indices(sparse_num = sparse_num)\n",
    "\n",
    "    candidates_pl = indices.filter(filter_condition).with_columns(pl_format_time(tau)).sort('time')\n",
    "    \n",
    "    data_c = compress_data_by_cands(data, candidates_pl, tau)\n",
    "    sat_fgm = df2ts(data_c, cols, attrs={\"coordinate_system\": coord, \"units\": \"nT\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "candidates_pd = candidates_pl.to_pandas()\n",
    "candidates_modin = mpd.DataFrame(candidates_pd)\n",
    "# candidates_x = xpd.DataFrame(candidates_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Test different libraries to parallelize the computation\n",
    "#| notest\n",
    "if True:\n",
    "    pdp_test = pdp.ApplyToRows(\n",
    "        lambda candidate: calc_candidate_duration(candidate, sat_fgm),  # fast a little bit\n",
    "        # lambda candidate: calc_duration(get_candidate_data_xr(candidate, sat_fgm)),\n",
    "        # lambda candidate: calc_duration(sat_fgm.sel(time=slice(candidate['tstart'], candidate['tstop']))),\n",
    "        func_desc=\"calculating duration parameters\",\n",
    "    )\n",
    "    \n",
    "    # process_candidates(candidates_modin, sat_fgm, sat_state, data_resolution)\n",
    "    \n",
    "    # ---\n",
    "    # successful cases\n",
    "    # ---\n",
    "    # candidates_pd.mapply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works, 4.2 secs\n",
    "    # candidates_pd.mapply(calc_candidate_duration, axis=1, data=sat_fgm) # this works, but a little bit slower, 6.7 secs\n",
    "    \n",
    "    # candidates_pd.apply(calc_candidate_duration, axis=1, data=sat_fgm) # Standard case: 24+s secs\n",
    "    # candidates_pd.swifter.apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 80 secs\n",
    "    # candidates_pd.swifter.set_dask_scheduler(scheduler=\"threads\").apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 60 secs\n",
    "    # candidates_modin.apply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works with ray, 6 secs # NOTE: can not work with dask\n",
    "    # candidates_x.apply(calc_candidate_duration, axis=1, data=sat_fgm) # 30 seconds\n",
    "    # pdp_test(candidates_modin) # this works, 8 secs\n",
    "    \n",
    "    # ---\n",
    "    # failed cases\n",
    "    # ---\n",
    "    # candidates_modin.apply(calc_candidate_duration, axis=1, data=sat_fgm) # AttributeError: 'DataFrame' object has no attribute 'sel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tsflex.features import MultipleFeatureDescriptors, FeatureCollection\n",
    "\n",
    "# from tsflex.features.integrations import catch22_wrapper\n",
    "# from pycatch22 import catch22_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau_pd = pd.Timedelta(tau)\n",
    "\n",
    "# catch22_feats = MultipleFeatureDescriptors(\n",
    "#     functions=catch22_wrapper(catch22_all),\n",
    "#     series_names=bcols,  # list of signal names\n",
    "#     windows = tau_pd, strides=tau_pd/2,\n",
    "# )\n",
    "\n",
    "# fc = FeatureCollection(catch22_feats)\n",
    "# features = fc.calculate(data, return_df=True)  # calculate the features on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_pl = pl.DataFrame(features.reset_index()).sort('time')\n",
    "# df = candidates_pl.join_asof(features_pl, on='time').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = ProfileReport(df, title=\"JUNO Candidates Report\")\n",
    "# profile.to_file(\"jno.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(task_dict, number=1):\n",
    "    results = {}\n",
    "    for name, (data, task) in task_dict.items():\n",
    "        try:\n",
    "            time_taken = timeit.timeit(\n",
    "                lambda: task(data),\n",
    "                number=number\n",
    "            )\n",
    "            results[name] = time_taken / number\n",
    "        except Exception as e:\n",
    "            results[name] = str(e)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "func = lambda candidate: calc_candidate_duration(candidate, sat_fgm)\n",
    "task_dict = {\n",
    "    'pandas': (candidates_pd, lambda _: _.apply(func, axis=1)),\n",
    "    'pandas-mapply': (candidates_pd, lambda _: _.mapply(func, axis=1)),\n",
    "    'modin': (candidates_modin, lambda _: _.apply(func, axis=1)),\n",
    "    # 'xorbits': (candidates_x, lambda _: _.apply(func, axis=1)),\n",
    "}\n",
    "\n",
    "results = benchmark(task_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs\n",
    "\n",
    "1. Feature engineering\n",
    "2. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vec_mean_mag(vec: xr.DataArray):\n",
    "    return linalg.norm(vec, dims=\"v_dim\").mean(dim=\"time\")\n",
    "\n",
    "\n",
    "def calc_vec_std(vec: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the standard deviation of a vector.\n",
    "    \"\"\"\n",
    "    return linalg.norm(vec.std(dim=\"time\"), dims=\"v_dim\")\n",
    "\n",
    "\n",
    "def calc_vec_relative_diff(vec: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the relative difference between the last and first elements of a vector.\n",
    "    \"\"\"\n",
    "    dvec = vec.isel(time=-1) - vec.isel(time=0)\n",
    "    return linalg.norm(dvec, dims=\"v_dim\") / linalg.norm(vec, dims=\"v_dim\").mean(\n",
    "        dim=\"time\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `process_candidates`\n",
    "Assign coordinates using `Dataframe.apply` is not optimized, quite slow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_candidates(\n",
    "    candidates: pd.DataFrame, # potential candidates DataFrame\n",
    "    sat_fgm: xr.DataArray, # satellite FGM data\n",
    "    sat_state: xr.DataArray, # satellite state data\n",
    "    data_resolution: timedelta, # time resolution of the data\n",
    ") -> pd.DataFrame: # processed candidates DataFrame\n",
    "    id_pipelines = IDsPipeline()\n",
    "\n",
    "    candidates = id_pipelines.calc_duration(sat_fgm).apply(candidates)\n",
    "\n",
    "    # calibrate duration\n",
    "    temp_candidates = candidates.loc[\n",
    "        lambda df: df[\"d_tstart\"].isnull() | df[\"d_tstop\"].isnull()\n",
    "    ]  # temp_candidates = candidates.query('d_tstart.isnull() | d_tstop.isnull()') # not implemented in `modin`\n",
    "\n",
    "    if not temp_candidates.empty:\n",
    "        candidates.update(\n",
    "            id_pipelines.calibrate_duration(sat_fgm, data_resolution).apply(\n",
    "                temp_candidates\n",
    "            )\n",
    "        )\n",
    "\n",
    "    ids = (\n",
    "        id_pipelines.calc_mva_features(sat_fgm)\n",
    "        + id_pipelines.calc_rotation_angle(sat_fgm)\n",
    "        + id_pipelines.assign_coordinates(sat_state)\n",
    "    ).apply(\n",
    "        candidates.dropna()  # Remove candidates with NaN values)\n",
    "    )\n",
    "\n",
    "    return ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
