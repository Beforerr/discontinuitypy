{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Finding magnetic discontinuities\n",
    "order: 0\n",
    "---\n",
    "\n",
    "It can be divided into two parts:\n",
    "\n",
    "1. Finding the discontinuities, see [this notebook](./01_ids_detection.ipynb)\n",
    "    - Corresponding to limited feature extraction / anomaly detection\n",
    "\n",
    "    - Output should contain the following:\n",
    "        - \"tstart\" and \"tstop\" of the event\n",
    "\n",
    "2. Calculating the properties of the discontinuities, see [this notebook](./02_ids_properties.ipynb)\n",
    "    - One can use higher time resolution data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp core/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# | code-summary: \"Import all the packages needed for the project\"\n",
    "import polars as pl\n",
    "from discontinuitypy.detection.variance import detect_variance\n",
    "from discontinuitypy.core.propeties import process_events\n",
    "from space_analysis.ds.ts.io import df2ts\n",
    "from loguru import logger\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes that the candidates only require a small portion of the data so we can compress the data to speed up the processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from beforerr.polars import filter_df_by_ranges\n",
    "\n",
    "\n",
    "def compress_data_by_events(data: pl.DataFrame, events: pl.DataFrame):\n",
    "    \"\"\"Compress the data for parallel processing\"\"\"\n",
    "    starts = events[\"tstart\"]\n",
    "    ends = events[\"tstop\"]\n",
    "    return filter_df_by_ranges(data, starts, ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def ids_finder(\n",
    "    detection_df: pl.LazyFrame,  # data used for anomaly dectection (typically low cadence data)\n",
    "    bcols=None,\n",
    "    detect_func: Callable[..., pl.LazyFrame] = detect_variance,\n",
    "    detect_kwargs: dict = {},\n",
    "    extract_df: pl.LazyFrame = None,  # data used for feature extraction (typically high cadence data),\n",
    "    **kwargs,\n",
    "):\n",
    "    if bcols is None:\n",
    "        bcols = detection_df.collect_schema().names()\n",
    "        bcols.remove(\"time\")\n",
    "    if len(bcols) != 3:\n",
    "        logger.error(\"Expect 3 field components\")\n",
    "\n",
    "    detection_df = detection_df.select(bcols + [\"time\"])\n",
    "    extract_df = extract_df or detection_df\n",
    "\n",
    "    detection_df = detection_df.sort(\"time\")\n",
    "    extract_df = extract_df.sort(\"time\")\n",
    "\n",
    "    events = detect_func(detection_df, bcols=bcols, **detect_kwargs)\n",
    "\n",
    "    data_c = compress_data_by_events(extract_df.collect(), events)\n",
    "    sat_fgm = df2ts(data_c, bcols)\n",
    "    ids = process_events(events, sat_fgm, **kwargs)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wrapper function for partitioned input used in `Kedro`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def extract_features(\n",
    "    partitioned_input: dict[str, Callable[..., pl.LazyFrame]],\n",
    "    tau: float,  # in seconds, yaml input\n",
    "    ts: float,  # in seconds, yaml input\n",
    "    **kwargs,\n",
    ") -> pl.DataFrame:\n",
    "    \"wrapper function for partitioned input\"\n",
    "\n",
    "    _tau = timedelta(seconds=tau)\n",
    "    _ts = timedelta(seconds=ts)\n",
    "\n",
    "    ids = pl.concat(\n",
    "        [\n",
    "            ids_finder(partition_load(), _tau, _ts, **kwargs)\n",
    "            for partition_load in partitioned_input.values()\n",
    "        ]\n",
    "    )\n",
    "    return ids.unique([\"d_time\", \"t.d_start\", \"t.d_end\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventions\n",
    "\n",
    "As we are dealing with multiple spacecraft, we need to be careful about naming conventions. Here are the conventions we use in this project.\n",
    "\n",
    "-   `sat_id`: name of the spacecraft. We also use abbreviation, for example\n",
    "    -   `sta` for `STEREO-A`\n",
    "    -   `thb` for `ARTEMIS-B`\n",
    "-   `sat_state`: state data of the spacecraft\n",
    "-   `b_vl`: maximum variance vector of the magnetic field, (major eigenvector)\n",
    "\n",
    "Data Level\n",
    "\n",
    "-   l0: unprocessed\n",
    "\n",
    "-   l1: cleaned data, fill null value, add useful columns\n",
    "\n",
    "-   l2: time-averaged data\n",
    "\n",
    "### Columns naming conventions\n",
    "\n",
    "-   `radial_distance`: radial distance of the spacecraft, in units of $AU$\n",
    "\n",
    "-   `plasma_speed`: solar wind plasma speed, in units of $km/s$\n",
    "\n",
    "-   `sw_elevation`: solar wind elevation angle, in units of $\\degree$\n",
    "\n",
    "-   `sw_azimuth`: solar wind azimuth angle, in units of $\\degree$\n",
    "\n",
    "-   `v_{x,y,z}` or `sw_vel_{X,Y,Z}`: solar wind plasma speed in the *ANY* coordinate system, in units of $km/s$\n",
    "\n",
    "    -   `sw_vel_{r,t,n}`: solar wind plasma speed in the RTN coordinate system, in units of $km/s$\n",
    "    -   `sw_vel_gse_{x,y,z}`: solar wind plasma speed in the GSE coordinate system, in units of $km/s$\n",
    "    -   `sw_vel_lmn_{x,y,z}`: solar wind plasma speed in the LMN coordinate system, in units of $km/s$\n",
    "        -   `v_l` or `sw_vel_l`: abbreviation for `sw_vel_lmn_1`\n",
    "        -   `v_mn` or `sw_vel_mn` (deprecated)\n",
    "\n",
    "-   `plasma_density`: plasma density, in units of $1/cm^{3}$\n",
    "\n",
    "-   `plasma_temperature`: plasma temperature, in units of $K$\n",
    "\n",
    "-   `B_{x,y,z}`: magnetic field in *ANY* coordinate system\n",
    "\n",
    "    -   `b_rtn_{x,y,z}` or `b_{r,t,n}`: magnetic field in the RTN coordinate system\n",
    "    -   `b_gse_{x,y,z}`: magnetic field in the GSE coordinate system\n",
    "\n",
    "-   `B_mag`: magnetic field magnitude\n",
    "\n",
    "-   `Vl_{x,y,z}` or `b_vecL_{X,Y,Z}`: maxium variance vector of the magnetic field in *ANY* coordinate system\n",
    "\n",
    "    -   `b_vecL_{r,t,n}`: maxium variance vector of the magnetic field in the RTN coordinate system\n",
    "\n",
    "-   `model_b_{r,t,n}`: modelled magnetic field in the RTN coordinate system\n",
    "\n",
    "-   `state` : *1* for *solar wind*, *0* for *non-solar wind*\n",
    "\n",
    "-   `L_mn{_norm}`: thickness of the current sheet in MN direction, in units of $km$\n",
    "\n",
    "-   `j0{_norm}`: current density, in units of $nA/m^2$\n",
    "\n",
    "Notes: we recommend use unique names for each variable, for example, `plasma_speed` instead of `speed`. Because it is easier to search and replace the variable names in the code whenever necessary.\n",
    "\n",
    "For the unit, by default we use\n",
    "\n",
    "-   length : $km$\n",
    "-   time : $s$\n",
    "-   magnetic field : $nT$\n",
    "-   current : $nA/m^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tsflex.features import MultipleFeatureDescriptors, FeatureCollection\n",
    "\n",
    "# from tsflex.features.integrations import catch22_wrapper\n",
    "# from pycatch22 import catch22_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau_pd = pd.Timedelta(tau)\n",
    "\n",
    "# catch22_feats = MultipleFeatureDescriptors(\n",
    "#     functions=catch22_wrapper(catch22_all),\n",
    "#     series_names=bcols,  # list of signal names\n",
    "#     windows = tau_pd, strides=tau_pd/2,\n",
    "# )\n",
    "\n",
    "# fc = FeatureCollection(catch22_feats)\n",
    "# features = fc.calculate(data, return_df=True)  # calculate the features on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_pl = pl.DataFrame(features.reset_index()).sort('time')\n",
    "# df = candidates_pl.join_asof(features_pl, on='time').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = ProfileReport(df, title=\"JUNO Candidates Report\")\n",
    "# profile.to_file(\"jno.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOs\n",
    "\n",
    "1. Feature engineering\n",
    "2. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete codes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
