{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Data Pipeline\n",
    "subtitle: Base layer pipeline\n",
    "description: Pipeline for a specific data type from a specific source\n",
    "---\n",
    "\n",
    "Roughly speaking every data source corresponds to an instrument in the mission.\n",
    "\n",
    "Generally, it includes the following steps:\n",
    "\n",
    "- Downloading data\n",
    "- Loading data\n",
    "- Preprocessing data\n",
    "- Processing data\n",
    "- Extracting features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "# | export\n",
    "import polars as pl\n",
    "\n",
    "from kedro.pipeline import Pipeline, node\n",
    "from kedro.pipeline.modular_pipeline import pipeline\n",
    "from ids_finder.utils.basic import load_params\n",
    "from typing import Callable, Optional, Any, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp pipelines/default/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    datatype=None,\n",
    "    ts=None,  # time resolution\n",
    "    probe: str = None,\n",
    "    coord: str = None,\n",
    "):\n",
    "    \"\"\"Downloading data\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    datatype=None,\n",
    "    ts=None,  # time resolution\n",
    "    probe: str = None,\n",
    "    coord: str = None,\n",
    "    vars: dict = None,\n",
    "):\n",
    "    \"\"\"Load data into a proper data structure, like dataframe.\n",
    "\n",
    "    - Downloading data\n",
    "    - Converting data structure\n",
    "    - Parsing original data (dealing with delimiters, missing values, etc.)\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    raw_data: Any | pl.DataFrame = None,\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    ts=None,  # time resolution\n",
    "    coord: str = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the raw dataset (only minor transformations)\n",
    "\n",
    "    - Applying naming conventions for columns\n",
    "    - Parsing and typing data (like from string to datetime for time columns)\n",
    "    - Structuring the data (like pivoting, unpivoting, etc.)\n",
    "    - Changing storing format (like from `csv` to `parquet`)\n",
    "    - Dropping null columns\n",
    "    - Dropping duplicate time\n",
    "    - Resampling data to a given time resolution (better to do in the next stage)\n",
    "    - ... other 'transformations' commonly performed at this stage.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data\n",
    "\n",
    "Some common preprocessing steps are:\n",
    "\n",
    "- Partition data by year, see `ids_finder.utils.basic.partition_data_by_year`\n",
    "\n",
    "Note: we process the data every year to minimize the memory usage and to avoid the failure of the processing (so need to process all the data again if only fails sometimes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(\n",
    "    raw_data: Any | pl.DataFrame,\n",
    "    ts: str = None,  # time resolution\n",
    "    coord: str = None,\n",
    ") -> pl.DataFrame | Dict[str, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Corresponding to primary data layer, where source data models are transformed into domain data models\n",
    "\n",
    "    - Transforming coordinate system if needed\n",
    "    - Discarding unnecessary columns\n",
    "    - Smoothing data\n",
    "    - Resampling data to a given time resolution\n",
    "    - Partitioning data, for the sake of memory\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def extract_features():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "DEFAULT_LOAD_INPUTS = dict(\n",
    "    start=\"params:start_date\",\n",
    "    end=\"params:end_date\",\n",
    "    datatype=\"params:datatype\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "def create_pipeline_template(\n",
    "    sat_id: str,  # satellite id, used for namespace\n",
    "    source: str,  # source data, like \"mag\" or \"plasma\", used for namespace\n",
    "    load_data_fn: Callable,\n",
    "    preprocess_data_fn: Callable,\n",
    "    process_data_fn: Callable,\n",
    "    load_inputs: dict = DEFAULT_LOAD_INPUTS,\n",
    "    params: Optional[dict] = None,\n",
    "    **kwargs,\n",
    ") -> Pipeline:\n",
    "    if params is None:\n",
    "        params = load_params()\n",
    "\n",
    "    namespace = f\"{sat_id}.{source}\"\n",
    "\n",
    "    ts = params[sat_id][source][\"time_resolution\"]\n",
    "    datatype = params[sat_id][source][\"datatype\"]\n",
    "\n",
    "    ts_str = f\"ts_{ts}s\"\n",
    "\n",
    "    node_load_data = node(\n",
    "        load_data_fn,\n",
    "        inputs=load_inputs,\n",
    "        outputs=\"raw_data\",\n",
    "        name=\"load_data\",\n",
    "    )\n",
    "\n",
    "    node_preprocess_data = node(\n",
    "        preprocess_data_fn,\n",
    "        inputs=\"raw_data\",\n",
    "        outputs=f\"inter_data_{datatype}\",\n",
    "        name=\"preprocess_data\",\n",
    "    )\n",
    "\n",
    "    node_process_data = node(\n",
    "        process_data_fn,\n",
    "        inputs=dict(\n",
    "            raw_data=f\"inter_data_{datatype}\",\n",
    "            ts=\"params:time_resolution\",\n",
    "        ),\n",
    "        outputs=f\"primary_data_{ts_str}\",\n",
    "        name=\"process_data\",\n",
    "    )\n",
    "\n",
    "    nodes = [\n",
    "        node_load_data,\n",
    "        node_preprocess_data,\n",
    "        node_process_data,\n",
    "    ]\n",
    "\n",
    "    pipelines = pipeline(\n",
    "        nodes,\n",
    "        namespace=namespace,\n",
    "        parameters={\n",
    "            \"params:start_date\": \"params:jno_start_date\",\n",
    "            \"params:end_date\": \"params:jno_end_date\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_planet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
