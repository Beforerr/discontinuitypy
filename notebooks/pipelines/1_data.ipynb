{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Data Pipeline\n",
    "subtitle: Base layer pipeline\n",
    "description: Pipeline for a specific data type from a specific source\n",
    "---\n",
    "\n",
    "Roughly speaking every data source corresponds to an instrument in the mission.\n",
    "\n",
    "Generally, it includes the following steps:\n",
    "\n",
    "- Downloading data\n",
    "- Loading data\n",
    "- Preprocessing data\n",
    "- Processing data\n",
    "- Extracting features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "import polars as pl\n",
    "\n",
    "from kedro.pipeline import Pipeline, node\n",
    "from kedro.pipeline.modular_pipeline import pipeline\n",
    "\n",
    "from typing import Any, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pipelines/default/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_data(\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    datatype = None,\n",
    "    ts = None,  # time resolution\n",
    "    probe: str = None,\n",
    "    coord: str = None,\n",
    "):\n",
    "    \"\"\"Downloading data\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def load_data(\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    datatype = None,\n",
    "    ts = None,  # time resolution\n",
    "    probe: str = None,\n",
    "    coord: str = None,\n",
    "    vars: dict = None,\n",
    "):\n",
    "    \"\"\"Load data into a proper data structure, like dataframe.\n",
    "\n",
    "    - Downloading data\n",
    "    - Converting data structure\n",
    "    - Parsing original data (dealing with delimiters, missing values, etc.)\n",
    "    \"\"\"\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(\n",
    "    raw_data: Any | pl.DataFrame = None,\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    ts = None,  # time resolution\n",
    "    coord: str = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the raw dataset (only minor transformations)\n",
    "\n",
    "    - Applying naming conventions for columns\n",
    "    - Parsing and typing data (like from string to datetime for time columns)\n",
    "    - Structuring the data (like pivoting, unpivoting, etc.)\n",
    "    - Changing storing format (like from `csv` to `parquet`)\n",
    "    - Dropping null columns \n",
    "    - Dropping duplicate time\n",
    "    - Resampling data to a given time resolution (better to do in the next stage)\n",
    "    - ... other 'transformations' commonly performed at this stage.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data\n",
    "\n",
    "Some common preprocessing steps are:\n",
    "\n",
    "- Partition data by year, see `ids_finder.utils.basic.partition_data_by_year`\n",
    "\n",
    "Note: we process the data every year to minimize the memory usage and to avoid the failure of the processing (so need to process all the data again if only fails sometimes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(\n",
    "    raw_data: Any | pl.DataFrame,\n",
    "    ts: str = None,  # time resolution\n",
    "    coord: str = None,\n",
    ") -> pl.DataFrame | Dict[str, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Corresponding to primary data layer, where source data models are transformed into domain data models\n",
    "\n",
    "    - Transforming coordinate system if needed\n",
    "    - Discarding unnecessary columns\n",
    "    - Smoothing data\n",
    "    - Resampling data to a given time resolution\n",
    "    - Partitioning data, for the sake of memory\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def extract_features():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataype(ts: int):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mfunction\u001b[0m\u001b[39m load_data at \u001b[0m\u001b[1;36m0x1040920e0\u001b[0m\u001b[1m>\u001b[0m"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    sat_id = None,\n",
    "    source = None,\n",
    "    load_data_fn=load_data,\n",
    "    check_dataype_fn=check_dataype,\n",
    "    preprocess_data_fn=preprocess_data,\n",
    "    process_data_fn=process_data,\n",
    "    extract_features_fn=extract_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "from typing import Callable\n",
    "from ids_finder.utils.basic import load_params\n",
    "\n",
    "def create_pipeline_template(\n",
    "    sat_id: str,  # satellite id, used for namespace\n",
    "    source: str,  # source data, like \"mag\" or \"plasma\"\n",
    "    check_dataype_fn: Callable,\n",
    "    load_data_fn: Callable,\n",
    "    preprocess_data_fn: Callable,\n",
    "    process_data_fn: Callable,\n",
    "    **kwargs,\n",
    ") -> Pipeline:\n",
    "\n",
    "    params = load_params()\n",
    "    \n",
    "    namespace = f\"{sat_id}.{source}\"\n",
    "\n",
    "    ts = params[sat_id][source][\"time_resolution\"]\n",
    "    ts_str = f\"ts_{ts}s\"\n",
    "\n",
    "    datatype = check_dataype_fn(ts)\n",
    "\n",
    "    node_load_data = node(\n",
    "        load_data_fn,\n",
    "        inputs=dict(\n",
    "            start=\"params:start_date\",\n",
    "            end=\"params:end_date\",\n",
    "            datatype=datatype,\n",
    "        ),\n",
    "        outputs=\"raw_data\",\n",
    "        name=\"load_data\",\n",
    "    )\n",
    "\n",
    "    node_preprocess_data = node(\n",
    "        preprocess_data_fn,\n",
    "        inputs=dict(\n",
    "            raw_data=\"raw_data\",\n",
    "        ),\n",
    "        outputs=f\"inter_data_{datatype}\",\n",
    "        name=\"preprocess_data\",\n",
    "    )\n",
    "\n",
    "    node_process_data = node(\n",
    "        process_data_fn,\n",
    "        inputs=dict(\n",
    "            raw_data=f\"inter_data_{datatype}\",\n",
    "            ts=\"params:time_resolution\",\n",
    "        ),\n",
    "        outputs=f\"primary_data_{ts_str}\",\n",
    "        name=\"process_data\",\n",
    "    )\n",
    "    \n",
    "    nodes = [\n",
    "        node_load_data,\n",
    "        node_preprocess_data,\n",
    "        node_process_data,\n",
    "    ]\n",
    "\n",
    "    pipelines = pipeline(\n",
    "        nodes,\n",
    "        namespace=namespace,\n",
    "        parameters={\n",
    "            \"params:start_date\": \"params:jno_start_date\",\n",
    "            \"params:end_date\": \"params:jno_end_date\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_planet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
