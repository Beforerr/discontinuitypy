{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Data Pipeline\n",
    "subtitle: Base layer pipeline\n",
    "description: Data pipeline processes data for a specific type from a specific source, roughly speaking every data source corresponds to an instrument in the mission.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "\n",
    "import polars as pl\n",
    "from typing import Any, Dict, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kerdo\n",
    "from kedro.pipeline import Pipeline, node\n",
    "from kedro.pipeline.modular_pipeline import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnetic field data pipeline\n",
    "\n",
    "The product of this pipeline is a data set of interesting magnetic field events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mag_data(\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    ts: str = None,  # time resolution\n",
    "    probe: str = None,\n",
    "    coord: str = None,\n",
    "):\n",
    "    \"\"\"Downloading data\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def load_mag_data(\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    ts: str = None,  # time resolution\n",
    "    probe: str = None,\n",
    "    coord: str = None,\n",
    "):\n",
    "    \"\"\"Load data into a proper data structure, like dataframe.\n",
    "\n",
    "    - Downloading data\n",
    "    - Converting data structure\n",
    "    \"\"\"\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mag_data(\n",
    "    raw_data: Any | pl.DataFrame = None,\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    ts: str = None,  # time resolution\n",
    "    coord: str = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the raw dataset (only minor transformations)\n",
    "\n",
    "    - Applying naming conventions for columns\n",
    "    - Parsing and typing data (like from string to datetime for time columns)\n",
    "    - Structuring the data (like pivoting, unpivoting, etc.)\n",
    "    - Changing storing format (like from `csv` to `parquet`)\n",
    "    - Dropping null columns \n",
    "    - Dropping duplicate time\n",
    "    - Resampling data to a given time resolution (better to do in the next stage)\n",
    "    - ... other 'transformations' commonly performed at this stage.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data\n",
    "\n",
    "Some common preprocessing steps are:\n",
    "\n",
    "- Partition data by year, see `ids_finder.utils.basic.partition_data_by_year`\n",
    "\n",
    "Note: we process the data every year to minimize the memory usage and to avoid the failure of the processing (so need to process all the data again if only fails sometimes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mag_data(\n",
    "    raw_data: Any | pl.DataFrame,\n",
    "    ts: str = None,  # time resolution\n",
    "    coord: str = None,\n",
    ") -> pl.DataFrame | Dict[str, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Corresponding to primary data layer, where source data models are transformed into domain data models\n",
    "\n",
    "    - Transforming coordinate system if needed\n",
    "    - Smoothing data\n",
    "    - Resampling data to a given time resolution\n",
    "    - Partitioning data, for the sake of memory\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def extract_features():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(\n",
    "    sat_id: str,  # satellite id, used for namespace\n",
    "    ts: int = 1,  # time resolution,\n",
    "    tau: str = '60s',  # time window\n",
    "    **kwargs,\n",
    ") -> Pipeline:\n",
    "    \n",
    "    ts_str = f\"ts_{ts}s\"\n",
    "    \n",
    "    node_load_data = node(\n",
    "        load_mag_data,\n",
    "        inputs=dict(\n",
    "            start=\"params:start_date\",\n",
    "            end=\"params:end_date\",\n",
    "        ),\n",
    "        outputs=f\"raw_mag\",\n",
    "        name=f\"load_{sat_id.upper()}_magnetic_field_data\",\n",
    "    )\n",
    "\n",
    "    node_preprocess_data = node(\n",
    "        preprocess_mag_data,\n",
    "        inputs=dict(\n",
    "            raw_data=f\"raw_mag\",\n",
    "            start=\"params:start_date\",\n",
    "            end=\"params:end_date\",\n",
    "        ),\n",
    "        outputs=f\"inter_mag_{ts_str}\",\n",
    "        name=f\"preprocess_{sat_id.upper()}_magnetic_field_data\",\n",
    "    )\n",
    "\n",
    "    node_process_data = node(\n",
    "        process_mag_data,\n",
    "        inputs=f\"inter_mag_{ts_str}\",\n",
    "        outputs=f\"primary_mag_{ts_str}\",\n",
    "        name=f\"process_{sat_id.upper()}_magnetic_field_data\",\n",
    "    )\n",
    "\n",
    "    node_extract_features = node(\n",
    "        extract_features,\n",
    "        inputs=[f\"primary_mag_{ts_str}\", \"params:tau\", \"params:extract_params\"],\n",
    "        outputs=f\"feature_tau_{tau}\",\n",
    "        name=f\"extract_{sat_id}_features\",\n",
    "    )\n",
    "\n",
    "    nodes = [\n",
    "        node_load_data,\n",
    "        node_preprocess_data,\n",
    "        node_process_data,\n",
    "        node_extract_features,\n",
    "    ]\n",
    "\n",
    "    pipelines = pipeline(\n",
    "        nodes,\n",
    "        namespace=sat_id,\n",
    "        parameters={\n",
    "            \"params:start_date\": \"params:jno_start_date\",\n",
    "            \"params:end_date\": \"params:jno_end_date\",\n",
    "            \"params:tau\": tau,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConfig:\n",
    "    def __init__(self, sat_id, download_func, preprocess_func, process_func):\n",
    "        self.sat_id = sat_id\n",
    "        self.download_func = download_func\n",
    "        self.preprocess_func = preprocess_func\n",
    "        self.process_func = process_func\n",
    "\n",
    "class PipelineGenerator:\n",
    "    def __init__(self, config: DatasetConfig, ts='1s', tau='60s'):\n",
    "        self.config = config\n",
    "        self.ts = ts\n",
    "        self.tau = tau\n",
    "\n",
    "    def _node(self, func, inputs, outputs, name):\n",
    "        return node(func, inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "    def generate_pipeline(self):\n",
    "        node_download = self._node(\n",
    "            self.config.download_func,\n",
    "            inputs=dict(start=\"params:start_date\", end=\"params:end_date\"),\n",
    "            outputs=f\"raw_data_{self.ts}\",\n",
    "            name=f\"download_{self.config.sat_id.upper()}_data\"\n",
    "        )\n",
    "\n",
    "        node_preprocess = self._node(\n",
    "            self.config.preprocess_func,\n",
    "            inputs=dict(raw_data=f\"raw_data_{self.ts}\", start=\"params:start_date\", end=\"params:end_date\"),\n",
    "            outputs=f\"inter_data_{self.ts}\",\n",
    "            name=f\"preprocess_{self.config.sat_id.upper()}_data\"\n",
    "        )\n",
    "\n",
    "        node_process = self._node(\n",
    "            self.config.process_func,\n",
    "            inputs=f\"inter_data_{self.ts}\",\n",
    "            outputs=f\"primary_data_rtn_{self.ts}\",\n",
    "            name=f\"process_{self.config.sat_id.upper()}_data\"\n",
    "        )\n",
    "\n",
    "        node_extract = self._node(\n",
    "            extract_features,\n",
    "            inputs=[f\"primary_data_rtn_{self.ts}\", \"params:tau\", \"params:extract_params\"],\n",
    "            outputs=f\"feature_tau_{self.tau}\",\n",
    "            name=f\"extract_{self.config.sat_id}_features\"\n",
    "        )\n",
    "\n",
    "        return pipeline(\n",
    "            [node_download, node_preprocess, node_process, node_extract],\n",
    "            namespace=self.config.sat_id,\n",
    "            parameters={\"params:start_date\": \"params:jno_start_date\", \"params:end_date\": \"params:jno_end_date\", \"params:tau\": self.tau}\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_planet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
