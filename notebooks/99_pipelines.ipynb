{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Pipelines\n",
    "subtitle: The data flow abstraction\n",
    "---\n",
    "\n",
    "We are using `Kedro` to build a data pipeline. A pipeline is a collection of nodes that are connected to each other. Each node is a function that takes inputs and produces outputs. The inputs and outputs are data sets of different layer/level.\n",
    "\n",
    "This notebook mainly demonstrate the concept and common building blocks of a pipeline, see each mission notebook for implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp pipelines/default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import polars as pl\n",
    "from typing import Any, Dict, List, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kerdo\n",
    "from kedro.pipeline import Pipeline, node\n",
    "from kedro.pipeline.modular_pipeline import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnetic field data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mag_data(\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    ts: str = None,  # time resolution\n",
    "    probe: str = None,\n",
    "    coord: str = None,\n",
    "):\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data\n",
    "\n",
    "Some common preprocessing steps are:\n",
    "\n",
    "- Partition data by year, see `ids_finder.utils.basic.partition_data_by_year`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mag_data(\n",
    "    raw_data: Any = None,\n",
    "    start: str = None,\n",
    "    end: str = None,\n",
    "    ts: str = None,  # time resolution\n",
    "    coord: str = None,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess the raw dataset (only minor transformations)\n",
    "\n",
    "    - Applying naming conventions for columns\n",
    "    - Parsing and typing data (like from string to datetime for time columns)\n",
    "    - Structuring the data (like pivoting, unpivoting, etc.)\n",
    "    - Changing storing format (like from `csv` to `parquet`)\n",
    "    - Dropping null columns\n",
    "    - Resampling data to a given time resolution\n",
    "    - ... other 'transformations' commonly performed at this stage.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data\n",
    "\n",
    "Note: we process the data every year to minimize the memory usage and to avoid the failure of the processing (so need to process all the data again if only fails sometimes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "\n",
    "def process_mag_data(\n",
    "    raw_data: Any | pl.DataFrame,\n",
    "    ts: str = None,  # time resolution\n",
    "    coord: str = None,\n",
    ") -> pl.DataFrame | Dict[str, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Corresponding to primary data layer, where source data models are transformed into domain data models\n",
    "\n",
    "    - Transforming data to RTN (Radial-Tangential-Normal) coordinate system\n",
    "    - Smoothing data\n",
    "    - Resampling data to a given time resolution\n",
    "    - Partitioning data, for the sake of memory\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def extract_features():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exports\n",
    "def create_mag_data_pipeline(\n",
    "    sat_id: str,  # satellite id, used for namespace\n",
    "    ts: str = '1s',  # time resolution,\n",
    "    tau: str = '60s',  # time window\n",
    "    **kwargs,\n",
    ") -> Pipeline:\n",
    "    \n",
    "    node_download_mag_data = node(\n",
    "        download_mag_data,\n",
    "        inputs=dict(\n",
    "            start=\"params:start_date\",\n",
    "            end=\"params:end_date\",\n",
    "        ),\n",
    "        outputs=f\"raw_mag\",\n",
    "        name=f\"download_{sat_id.upper()}_magnetic_field_data\",\n",
    "    )\n",
    "\n",
    "    node_preprocess_mag_data = node(\n",
    "        preprocess_mag_data,\n",
    "        inputs=dict(\n",
    "            raw_data=f\"raw_mag\",\n",
    "            start=\"params:start_date\",\n",
    "            end=\"params:end_date\",\n",
    "        ),\n",
    "        outputs=f\"inter_mag_{ts}\",\n",
    "        name=f\"preprocess_{sat_id.upper()}_magnetic_field_data\",\n",
    "    )\n",
    "\n",
    "    node_process_mag_data = node(\n",
    "        process_mag_data,\n",
    "        inputs=f\"inter_mag_{ts}\",\n",
    "        outputs=f\"primary_mag_rtn_{ts}\",\n",
    "        name=f\"process_{sat_id.upper()}_magnetic_field_data\",\n",
    "    )\n",
    "\n",
    "    node_extract_features = node(\n",
    "        extract_features,\n",
    "        inputs=[f\"primary_mag_rtn_{ts}\", \"params:tau\", \"params:extract_params\"],\n",
    "        outputs=f\"feature_tau_{tau}\",\n",
    "        name=f\"extract_{sat_id}_features\",\n",
    "    )\n",
    "\n",
    "    nodes = [\n",
    "        node_download_mag_data,\n",
    "        node_preprocess_mag_data,\n",
    "        node_process_mag_data,\n",
    "        node_extract_features,\n",
    "    ]\n",
    "\n",
    "    pipelines = pipeline(\n",
    "        nodes,\n",
    "        namespace=sat_id,\n",
    "        parameters={\n",
    "            \"params:start_date\": \"params:jno_start_date\",\n",
    "            \"params:end_date\": \"params:jno_end_date\",\n",
    "            \"params:tau\": tau,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConfig:\n",
    "    def __init__(self, sat_id, download_func, preprocess_func, process_func):\n",
    "        self.sat_id = sat_id\n",
    "        self.download_func = download_func\n",
    "        self.preprocess_func = preprocess_func\n",
    "        self.process_func = process_func\n",
    "\n",
    "class PipelineGenerator:\n",
    "    def __init__(self, config: DatasetConfig, ts='1s', tau='60s'):\n",
    "        self.config = config\n",
    "        self.ts = ts\n",
    "        self.tau = tau\n",
    "\n",
    "    def _node(self, func, inputs, outputs, name):\n",
    "        return node(func, inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "    def generate_pipeline(self):\n",
    "        node_download = self._node(\n",
    "            self.config.download_func,\n",
    "            inputs=dict(start=\"params:start_date\", end=\"params:end_date\"),\n",
    "            outputs=f\"raw_data_{self.ts}\",\n",
    "            name=f\"download_{self.config.sat_id.upper()}_data\"\n",
    "        )\n",
    "\n",
    "        node_preprocess = self._node(\n",
    "            self.config.preprocess_func,\n",
    "            inputs=dict(raw_data=f\"raw_data_{self.ts}\", start=\"params:start_date\", end=\"params:end_date\"),\n",
    "            outputs=f\"inter_data_{self.ts}\",\n",
    "            name=f\"preprocess_{self.config.sat_id.upper()}_data\"\n",
    "        )\n",
    "\n",
    "        node_process = self._node(\n",
    "            self.config.process_func,\n",
    "            inputs=f\"inter_data_{self.ts}\",\n",
    "            outputs=f\"primary_data_rtn_{self.ts}\",\n",
    "            name=f\"process_{self.config.sat_id.upper()}_data\"\n",
    "        )\n",
    "\n",
    "        node_extract = self._node(\n",
    "            extract_features,\n",
    "            inputs=[f\"primary_data_rtn_{self.ts}\", \"params:tau\", \"params:extract_params\"],\n",
    "            outputs=f\"feature_tau_{self.tau}\",\n",
    "            name=f\"extract_{self.config.sat_id}_features\"\n",
    "        )\n",
    "\n",
    "        return pipeline(\n",
    "            [node_download, node_preprocess, node_process, node_extract],\n",
    "            namespace=self.config.sat_id,\n",
    "            parameters={\"params:start_date\": \"params:jno_start_date\", \"params:end_date\": \"params:jno_end_date\", \"params:tau\": self.tau}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_data(tstart=None, tend=None, raw_data=None, columns=None, **kwargs):\n",
    "    \"\"\"Get the state data with proper column names and types in RTN coordinates.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def processs_state_data(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_data_pipeline(sat_id, **kwargs) -> Pipeline:\n",
    "    node_get_state_data = node(\n",
    "        get_state_data,\n",
    "        inputs={\n",
    "            \"tstart\": \"params:start_date\",\n",
    "            \"tend\": \"params:end_date\",\n",
    "            \"raw_data\": None,\n",
    "            \"columns\": None,\n",
    "        },\n",
    "        outputs=\"inter_state_rtn_1h\",\n",
    "        name=f\"get_{sat_id.upper()}_state_data\",\n",
    "    )\n",
    "\n",
    "    node_processs_state_data = node(\n",
    "        processs_state_data,\n",
    "        inputs=\"inter_state_rtn_1h\",\n",
    "        outputs=\"primary_state_rtn_1h\",\n",
    "        name=f\"process_{sat_id.upper()}_state_data\",\n",
    "    )\n",
    "\n",
    "    nodes = [node_get_state_data, node_processs_state_data]\n",
    "    pipelines = pipeline(\n",
    "        nodes,\n",
    "        namespace=sat_id,\n",
    "        parameters={\n",
    "            \"params:start_date\": \"params:jno_start_date\",\n",
    "            \"params:end_date\": \"params:jno_end_date\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    return pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(df: pl.DataFrame, state: pl.DataFrame) -> pl.DataFrame:\n",
    "    pass\n",
    "\n",
    "def create_candidate_pipeline(sat_id, **kwargs) -> Pipeline:\n",
    "    time_resolution = \"1s\"\n",
    "\n",
    "    node_combine_features = node(\n",
    "        combine_features,\n",
    "        inputs=[\n",
    "            f\"{sat_id}.feature_rtn_{time_resolution}\",\n",
    "            f\"{sat_id}.primary_state_rtn_1h\",\n",
    "        ],\n",
    "        outputs=f\"candidates.{sat_id}_{time_resolution}\",\n",
    "    )\n",
    "\n",
    "    nodes = [node_combine_features]\n",
    "    return pipeline(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def combine_candidates(dict):\n",
    "    pass\n",
    "\n",
    "# node_thm_extract_features = node(\n",
    "#     extract_features,\n",
    "#     inputs=[\"primary_thm_rtn_1s\", \"params:tau\", \"params:thm_1s_params\"],\n",
    "#     outputs=\"candidates_thm_rtn_1s\",\n",
    "#     name=\"extract_ARTEMIS_features\",\n",
    "# )\n",
    "\n",
    "# node_combine_candidates = node(\n",
    "#     combine_candidates,\n",
    "#     inputs=dict(\n",
    "#         sta_candidates=\"candidates_sta_rtn_1s\",\n",
    "#         jno_candidates=\"candidates_jno_ss_se_1s\",\n",
    "#         thm_candidates=\"candidates_thm_rtn_1s\",\n",
    "#     ),\n",
    "#     outputs=\"candidates_all_1s\",\n",
    "#     name=\"combine_candidates\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(**kwargs) -> Pipeline:\n",
    "    sat_id = \"sta\"\n",
    "    return (\n",
    "        create_mag_data_pipeline(sat_id)\n",
    "        + create_state_data_pipeline()\n",
    "        + create_candidate_pipeline(sat_id)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
