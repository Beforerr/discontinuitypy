{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: ID properties\n",
    "subtitle: Full feature extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/19/23 00:30:32] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook    <a href=\"file:///Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/warnings.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">warnings.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/warnings.py#109\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         mode. Use `tqdm.tqdm` instead to force console mode <span style=\"font-weight: bold\">(</span>e.g. in jupyter   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         console<span style=\"font-weight: bold\">)</span>                                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/19/23 00:30:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook    \u001b]8;id=931555;file:///Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/warnings.py\u001b\\\u001b[2mwarnings.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=634934;file:///Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/warnings.py#109\u001b\\\u001b[2m109\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         mode. Use `tqdm.tqdm` instead to force console mode \u001b[1m(\u001b[0me.g. in jupyter   \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         console\u001b[1m)\u001b[0m                                                               \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                                \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| default_exp core/propeties\n",
    "#| export\n",
    "#| code-summary: \"Import all the packages needed for the project\"\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "import polars as pl\n",
    "import xarray as xr\n",
    "\n",
    "try:\n",
    "    import modin.pandas as pd\n",
    "    import modin.pandas as mpd\n",
    "    from modin.config import ProgressBar\n",
    "    ProgressBar.enable()\n",
    "except ImportError:\n",
    "    import pandas as pd\n",
    "import pandas\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "import pdpipe as pdp\n",
    "from pdpipe.util import out_of_place_col_insert\n",
    "from multipledispatch import dispatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dispatch(object, xr.DataArray)\n",
    "def get_candidate_data(\n",
    "    candidate, data, neighbor: int = 0\n",
    ") -> xr.DataArray:\n",
    "    duration = candidate[\"tstop\"] - candidate[\"tstart\"]\n",
    "    offset = neighbor * duration\n",
    "    temp_tstart = candidate[\"tstart\"] - offset\n",
    "    temp_tstop = candidate[\"tstop\"] + offset\n",
    "\n",
    "    return data.sel(time=slice(temp_tstart, temp_tstop))\n",
    "\n",
    "def get_candidates(candidates: pd.DataFrame, candidate_type=None, num: int = 4):\n",
    "    if candidate_type is not None:\n",
    "        _candidates = candidates[candidates[\"type\"] == candidate_type]\n",
    "    else:\n",
    "        _candidates = candidates\n",
    "\n",
    "    # Sample a specific number of candidates if num is provided and it's less than the total number\n",
    "    if num < len(_candidates):\n",
    "        logger.info(\n",
    "            f\"Sampling {num} {candidate_type} candidates out of {len(_candidates)}\"\n",
    "        )\n",
    "        return _candidates.sample(num)\n",
    "    else:\n",
    "        return _candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from discontinuitypy.propeties.duration import calc_duration\n",
    "\n",
    "\n",
    "def calc_candidate_duration(candidate: pd.Series, data, method=\"distance\"):\n",
    "    try:\n",
    "        candidate_data = get_candidate_data(candidate, data)\n",
    "        result = calc_duration(candidate_data)\n",
    "    except Exception as e:\n",
    "        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\") # can not be serialized\n",
    "        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "    if method == \"distance\":\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"d_tstart\": result[0],\n",
    "                \"d_tstop\": result[1],\n",
    "            }\n",
    "        )\n",
    "    elif method == \"derivative\":\n",
    "        return pd.Series(\n",
    "            {\n",
    "                \"d_tstart\": result[0],\n",
    "                \"d_tstop\": result[1],\n",
    "                \"d_time\": result[2],\n",
    "                \"d_star\": result[3],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum variance analysis (MVA) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from discontinuitypy.propeties.mva import calc_candidate_mva_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field rotation angles\n",
    "The PDF of the field rotation angles across the solar-wind IDs is well fitted by the exponential function exp(−θ/)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_data_at_times(data: xr.DataArray, times) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Select data at specified times.\n",
    "    \"\"\"\n",
    "    # Use xarray's selection capability if data supports it\n",
    "    return data.sel(time=times, method=\"nearest\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_rotation_angle(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - v1: The first vector(s).\n",
    "    - v2: The second vector(s).\n",
    "    \"\"\"\n",
    "    \n",
    "    if v1.shape != v2.shape:\n",
    "        raise ValueError(\"Vectors must have the same shape.\")\n",
    "    \n",
    "    # Normalize the vectors\n",
    "    v1_u = v1 / np.linalg.norm(v1, axis=-1, keepdims=True)\n",
    "    v2_u = v2 / np.linalg.norm(v2, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Calculate the cosine of the angle for each time step\n",
    "    cosine_angle = np.sum(v1_u * v2_u, axis=-1)\n",
    "    \n",
    "    # Clip the values to handle potential floating point errors\n",
    "    cosine_angle = np.clip(cosine_angle, -1, 1)\n",
    "    \n",
    "    angle = np.arccos(cosine_angle)\n",
    "    \n",
    "    # Convert the angles from radians to degrees\n",
    "    return np.degrees(angle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_events_rotation_angle(events, data: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle(s) at two different time steps.\n",
    "    \"\"\"\n",
    "    tstart = events['d_tstart'].to_numpy()\n",
    "    tstop = events['d_tstop'].to_numpy()\n",
    "\n",
    "    vecs_before = get_data_at_times(data, tstart)\n",
    "    vecs_after = get_data_at_times(data, tstop)\n",
    "\n",
    "    rotation_angles = calc_rotation_angle(vecs_before, vecs_after)\n",
    "    return rotation_angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_normal_direction(v1, v2, normalize=True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the normal direction of two vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    v1 : array_like \n",
    "        The first vector(s).\n",
    "    v2 : array_like \n",
    "        The second vector(s).\n",
    "    \"\"\"\n",
    "    c = np.cross(v1, v2)\n",
    "    return c / np.linalg.norm(c, axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def calc_events_normal_direction(events, data: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the normal directions(s) at two different time steps.\n",
    "    \"\"\"\n",
    "    tstart = events['d_tstart'].to_numpy()\n",
    "    tstop = events['d_tstop'].to_numpy()\n",
    "\n",
    "    vecs_before = get_data_at_times(data, tstart)\n",
    "    vecs_after = get_data_at_times(data, tstop)\n",
    "\n",
    "    normal_directions = calc_normal_direction(vecs_before, vecs_after)\n",
    "    # need to convert to list first, as only 1D array is supported\n",
    "    return normal_directions.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def calc_events_vec_change(events, data: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Utils function to calculate features related to the change of the magnetic field\n",
    "    \"\"\"\n",
    "    tstart = events['d_tstart'].to_numpy()\n",
    "    tstop = events['d_tstop'].to_numpy()\n",
    "    \n",
    "    vecs_before = get_data_at_times(data, tstart)\n",
    "    vecs_after = get_data_at_times(data, tstop)\n",
    "    return (vecs_after - vecs_before).tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patch `pdp.ApplyToRows` to work with `modin` and `xorbits` DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _transform(self: pdp.ApplyToRows, X, verbose):\n",
    "    new_cols = X.apply(self._func, axis=1)\n",
    "    if isinstance(new_cols, (pd.Series, pandas.Series)):\n",
    "        loc = len(X.columns)\n",
    "        if self._follow_column:\n",
    "            loc = X.columns.get_loc(self._follow_column) + 1\n",
    "        return out_of_place_col_insert(\n",
    "            X=X, series=new_cols, loc=loc, column_name=self._colname\n",
    "        )\n",
    "    if isinstance(new_cols, (mpd.DataFrame, pandas.DataFrame)):\n",
    "        sorted_cols = sorted(list(new_cols.columns))\n",
    "        new_cols = new_cols[sorted_cols]\n",
    "        if self._follow_column:\n",
    "            inter_X = X\n",
    "            loc = X.columns.get_loc(self._follow_column) + 1\n",
    "            for colname in new_cols.columns:\n",
    "                inter_X = out_of_place_col_insert(\n",
    "                    X=inter_X,\n",
    "                    series=new_cols[colname],\n",
    "                    loc=loc,\n",
    "                    column_name=colname,\n",
    "                )\n",
    "                loc += 1\n",
    "            return inter_X\n",
    "        assign_map = {\n",
    "            colname: new_cols[colname] for colname in new_cols.columns\n",
    "        }\n",
    "        return X.assign(**assign_map)\n",
    "    raise TypeError(  # pragma: no cover\n",
    "        \"Unexpected type generated by applying a function to a DataFrame.\"\n",
    "        \" Only Series and DataFrame are allowed.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipelines` Class for processing IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class IDsPipeline:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def calc_duration(self, data: xr.DataArray):\n",
    "        return pdp.ApplyToRows(\n",
    "            lambda candidate: calc_candidate_duration(candidate, data),\n",
    "            func_desc=\"calculating duration parameters\",\n",
    "        )\n",
    "\n",
    "    def calc_mva_features(self, data):\n",
    "        return pdp.ApplyToRows(\n",
    "            lambda candidate: calc_candidate_mva_features(candidate, data),\n",
    "            func_desc='calculating MVA features',\n",
    "        )\n",
    "\n",
    "    def calc_vec_change(self, data):\n",
    "        return pdp.ColByFrameFunc(\n",
    "            \"dB\",\n",
    "            lambda candidate: calc_events_vec_change(candidate, data),\n",
    "            func_desc='calculating compound change',\n",
    "        )\n",
    "\n",
    "    def calc_rotation_angle(self, data):\n",
    "        return pdp.ColByFrameFunc(\n",
    "            \"rotation_angle\",\n",
    "            lambda df: calc_events_rotation_angle(df, data),\n",
    "            func_desc=\"calculating rotation angle\",\n",
    "        )\n",
    "\n",
    "    def calc_normal_direction(self, data):\n",
    "        return pdp.ColByFrameFunc(\n",
    "            \"normal_direction\",\n",
    "            lambda df: calc_events_normal_direction(df, data),\n",
    "            func_desc=\"calculating normal direction\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from discontinuitypy.utils.polars import convert_to_pd_dataframe, decompose_vector  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def process_events(\n",
    "    candidates_pl: pl.DataFrame,  # potential candidates DataFrame\n",
    "    sat_fgm: xr.DataArray,  # satellite FGM data\n",
    "    data_resolution: timedelta,  # time resolution of the data\n",
    "    modin=True,\n",
    "    **kwargs,\n",
    ") -> pl.DataFrame:\n",
    "    \"Process candidates DataFrame\"\n",
    "\n",
    "    candidates = convert_to_pd_dataframe(candidates_pl, modin=modin)\n",
    "\n",
    "    id_pipelines = IDsPipeline()\n",
    "    candidates = id_pipelines.calc_duration(sat_fgm).apply(candidates)\n",
    "\n",
    "    ids = (\n",
    "        id_pipelines.calc_mva_features(sat_fgm)\n",
    "        + id_pipelines.calc_vec_change(sat_fgm)\n",
    "        + id_pipelines.calc_rotation_angle(sat_fgm)\n",
    "        + id_pipelines.calc_normal_direction(sat_fgm)\n",
    "    ).apply(\n",
    "        candidates.dropna()  # Remove candidates with NaN values)\n",
    "    )\n",
    "\n",
    "    if isinstance(ids, mpd.DataFrame):\n",
    "        ids = ids._to_pandas()\n",
    "\n",
    "    return (\n",
    "        pl.DataFrame(ids)\n",
    "        .pipe(decompose_vector, \"dB\")\n",
    "        .pipe(decompose_vector, \"dB_lmn\")\n",
    "        .pipe(decompose_vector, \"normal_direction\", name=\"k\")\n",
    "        .pipe(decompose_vector, \"Vl\")\n",
    "        .pipe(decompose_vector, \"Vn\")\n",
    "        .drop([\"dB\", \"dB_lmn\", \"normal_direction\", \"Vl\", \"Vn\"])\n",
    "    )\n",
    "    # ValueError: Data type fixed_size_list[pyarrow] not supported by interchange protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test parallelization\n",
    "\n",
    "\n",
    "Generally `mapply` and `modin` are the fastest. `xorbits` is expected to be the fastest but it is not and it is the slowest one.\n",
    "\n",
    "```python\n",
    "#| notest\n",
    "sat = 'jno'\n",
    "coord = 'se'\n",
    "cols = [\"BX\", \"BY\", \"BZ\"]\n",
    "tau = timedelta(seconds=60)\n",
    "data_resolution = timedelta(seconds=1)\n",
    "\n",
    "if True:\n",
    "    year = 2012\n",
    "    files = f'../data/{sat}_data_{year}.parquet'\n",
    "    output = f'../data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n",
    "\n",
    "    data = pl.scan_parquet(files).set_sorted('time').collect()\n",
    "\n",
    "    indices = compute_indices(data, tau)\n",
    "    # filter condition\n",
    "    sparse_num = tau / data_resolution // 3\n",
    "    filter_condition = filter_indices(sparse_num = sparse_num)\n",
    "\n",
    "    candidates = indices.filter(filter_condition).with_columns(pl_format_time(tau)).sort('time')\n",
    "    \n",
    "    data_c = compress_data_by_cands(data, candidates, tau)\n",
    "    sat_fgm = df2ts(data_c, cols, attrs={\"units\": \"nT\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "candidates_pd = candidates.to_pandas()\n",
    "candidates_modin = mpd.DataFrame(candidates_pd)\n",
    "# candidates_x = xpd.DataFrame(candidates_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Test different libraries to parallelize the computation\n",
    "#| notest\n",
    "if True:\n",
    "    pdp_test = pdp.ApplyToRows(\n",
    "        lambda candidate: calc_candidate_duration(candidate, sat_fgm),  # fast a little bit\n",
    "        # lambda candidate: calc_duration(get_candidate_data_xr(candidate, sat_fgm)),\n",
    "        # lambda candidate: calc_duration(sat_fgm.sel(time=slice(candidate['tstart'], candidate['tstop']))),\n",
    "        func_desc=\"calculating duration parameters\",\n",
    "    )\n",
    "    \n",
    "    # process_events(candidates_modin, sat_fgm, sat_state, data_resolution)\n",
    "    \n",
    "    # ---\n",
    "    # successful cases\n",
    "    # ---\n",
    "    # candidates_pd.mapply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works, 4.2 secs\n",
    "    # candidates_pd.mapply(calc_candidate_duration, axis=1, data=sat_fgm) # this works, but a little bit slower, 6.7 secs\n",
    "    \n",
    "    # candidates_pd.apply(calc_candidate_duration, axis=1, data=sat_fgm) # Standard case: 24+s secs\n",
    "    # candidates_pd.swifter.apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 80 secs\n",
    "    # candidates_pd.swifter.set_dask_scheduler(scheduler=\"threads\").apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 60 secs\n",
    "    # candidates_modin.apply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works with ray, 6 secs # NOTE: can not work with dask\n",
    "    # candidates_x.apply(calc_candidate_duration, axis=1, data=sat_fgm) # 30 seconds\n",
    "    # pdp_test(candidates_modin) # this works, 8 secs\n",
    "    \n",
    "    # ---\n",
    "    # failed cases\n",
    "    # ---\n",
    "    # candidates_modin.apply(calc_candidate_duration, axis=1, data=sat_fgm) # AttributeError: 'DataFrame' object has no attribute 'sel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(task_dict, number=1):\n",
    "    results = {}\n",
    "    for name, (data, task) in task_dict.items():\n",
    "        try:\n",
    "            time_taken = timeit.timeit(\n",
    "                lambda: task(data),\n",
    "                number=number\n",
    "            )\n",
    "            results[name] = time_taken / number\n",
    "        except Exception as e:\n",
    "            results[name] = str(e)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "def benchmark_results(results, sat_fgm):\n",
    "    func = partial(calc_candidate_duration, data=sat_fgm)\n",
    "    task_dict = {\n",
    "        'pandas': (candidates_pd, lambda _: _.apply(func, axis=1)),\n",
    "        'pandas-mapply': (candidates_pd, lambda _: _.mapply(func, axis=1)),\n",
    "        'modin': (candidates_modin, lambda _: _.apply(func, axis=1)),\n",
    "        # 'xorbits': (candidates_x, lambda _: _.apply(func, axis=1)),\n",
    "    }\n",
    "\n",
    "    results = benchmark(task_dict)\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_planet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
