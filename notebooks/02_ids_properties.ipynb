{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: ID properties\n",
    "subtitle: Full feature extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core/propeties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| code-summary: \"Import all the packages needed for the project\"\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "from ids_finder.utils.basic import *\n",
    "import polars as pl\n",
    "import xarray as xr\n",
    "from ids_finder.core.detection import *\n",
    "\n",
    "try:\n",
    "    import modin.pandas as pd\n",
    "    import modin.pandas as mpd\n",
    "    from modin.config import ProgressBar\n",
    "    ProgressBar.enable()\n",
    "except ImportError:\n",
    "    import pandas as pd\n",
    "import pandas\n",
    "    \n",
    "import numpy as np\n",
    "from xarray_einstats import linalg\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "import pdpipe as pdp\n",
    "from multipledispatch import dispatch\n",
    "\n",
    "from typing import Any, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "@dispatch(object, xr.DataArray)\n",
    "def get_candidate_data(\n",
    "    candidate, data, neighbor: int = 0\n",
    ") -> xr.DataArray:\n",
    "    duration = candidate[\"tstop\"] - candidate[\"tstart\"]\n",
    "    offset = neighbor * duration\n",
    "    temp_tstart = candidate[\"tstart\"] - offset\n",
    "    temp_tstop = candidate[\"tstop\"] + offset\n",
    "\n",
    "    return data.sel(time=slice(temp_tstart, temp_tstop))\n",
    "\n",
    "\n",
    "@dispatch(object, pl.DataFrame)\n",
    "def get_candidate_data(\n",
    "    candidate, data, neighbor: int = 0, bcols=[\"BX\", \"BY\", \"BZ\"]\n",
    ") -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Notes\n",
    "    -----\n",
    "    much slower than `get_candidate_data_xr`\n",
    "    \"\"\"\n",
    "    duration = candidate[\"tstart\"] - candidate[\"tstop\"]\n",
    "    offset = neighbor * duration\n",
    "    temp_tstart = candidate[\"tstart\"] - offset\n",
    "    temp_tstop = candidate[\"tstop\"] + offset\n",
    "\n",
    "    temp_data = data.filter(pl.col(\"time\").is_between(temp_tstart, temp_tstop))\n",
    "\n",
    "    return df2ts(temp_data, bcols, attrs={\"units\": \"nT\"})\n",
    "\n",
    "\n",
    "def get_candidates(candidates: pd.DataFrame, candidate_type=None, num: int = 4):\n",
    "    if candidate_type is not None:\n",
    "        _candidates = candidates[candidates[\"type\"] == candidate_type]\n",
    "    else:\n",
    "        _candidates = candidates\n",
    "\n",
    "    # Sample a specific number of candidates if num is provided and it's less than the total number\n",
    "    if num < len(_candidates):\n",
    "        logger.info(\n",
    "            f\"Sampling {num} {candidate_type} candidates out of {len(_candidates)}\"\n",
    "        )\n",
    "        return _candidates.sample(num)\n",
    "    else:\n",
    "        return _candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitions of duration\n",
    "- Define $d^* = \\max( | dB / dt | ) $, and then define time interval where $| dB/dt |$ decreases to $d^*/4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "THRESHOLD_RATIO  = 1/4\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def calc_duration(vec: xr.DataArray, threshold_ratio=THRESHOLD_RATIO) -> pandas.Series:\n",
    "    # NOTE: gradient calculated at the edge is not reliable.\n",
    "    vec_diff = vec.differentiate(\"time\", datetime_unit=\"s\").isel(time=slice(1,-1))\n",
    "    vec_diff_mag = linalg.norm(vec_diff, dims='v_dim')\n",
    "\n",
    "    # Determine d_star based on trend\n",
    "    if vec_diff_mag.isnull().all():\n",
    "        raise ValueError(\"The differentiated vector magnitude contains only NaN values. Cannot compute duration.\")\n",
    "    \n",
    "    d_star_index = vec_diff_mag.argmax(dim=\"time\")\n",
    "    d_star = vec_diff_mag[d_star_index]\n",
    "    d_time = vec_diff_mag.time[d_star_index]\n",
    "    \n",
    "    threshold = d_star * threshold_ratio\n",
    "\n",
    "    start_time, end_time = find_start_end_times(vec_diff_mag, d_time, threshold)\n",
    "\n",
    "    dict = {\n",
    "        'd_star': d_star.item(),\n",
    "        'd_time': d_time.values,\n",
    "        'threshold': threshold.item(),\n",
    "        'd_tstart': start_time,\n",
    "        'd_tstop': end_time,\n",
    "    }\n",
    "\n",
    "    return pandas.Series(dict)\n",
    "\n",
    "def calc_d_duration(vec: xr.DataArray, d_time, threshold) -> pd.Series:\n",
    "    vec_diff = vec.differentiate(\"time\", datetime_unit=\"s\")\n",
    "    vec_diff_mag = linalg.norm(vec_diff, dims='v_dim')\n",
    "\n",
    "    start_time, end_time = find_start_end_times(vec_diff_mag, d_time, threshold)\n",
    "\n",
    "    return pandas.Series({\n",
    "        'd_tstart': start_time,\n",
    "        'd_tstop': end_time,\n",
    "    })\n",
    " \n",
    "def find_start_end_times(vec_diff_mag: xr.DataArray, d_time, threshold) -> tuple[pd.Timestamp, pd.Timestamp]:\n",
    "    # Determine start time\n",
    "    pre_vec_mag = vec_diff_mag.sel(time=slice(None, d_time))\n",
    "    start_time = get_time_from_condition(pre_vec_mag, threshold, \"last_below\")\n",
    "\n",
    "    # Determine stop time\n",
    "    post_vec_mag = vec_diff_mag.sel(time=slice(d_time, None))\n",
    "    end_time = get_time_from_condition(post_vec_mag, threshold, \"first_below\")\n",
    "\n",
    "    return start_time, end_time\n",
    "\n",
    "\n",
    "def get_time_from_condition(vec: xr.DataArray, threshold, condition_type) -> pd.Timestamp:\n",
    "    if condition_type == \"first_below\":\n",
    "        condition = vec < threshold\n",
    "        index_choice = 0\n",
    "    elif condition_type == \"last_below\":\n",
    "        condition = vec < threshold\n",
    "        index_choice = -1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown condition_type: {condition_type}\")\n",
    "\n",
    "    where_result = np.where(condition)[0]\n",
    "\n",
    "    if len(where_result) > 0:\n",
    "        return vec.time[where_result[index_choice]].values\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_candidate_duration(candidate: pd.Series, data) -> pd.Series:\n",
    "    try:\n",
    "        candidate_data = get_candidate_data(candidate, data)\n",
    "        return calc_duration(candidate_data)\n",
    "    except Exception as e:\n",
    "        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\") # can not be serialized\n",
    "        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n",
    "        raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrates candidate duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def calibrate_candidate_duration(\n",
    "    candidate: pd.Series, data:xr.DataArray, data_resolution, ratio = 3/4\n",
    "):\n",
    "    \"\"\"\n",
    "    Calibrates the candidate duration. \n",
    "    - If only one of 'd_tstart' or 'd_tstop' is provided, calculates the missing one based on the provided one and 'd_time'.\n",
    "    - Then if this is not enough points between 'd_tstart' and 'd_tstop', returns None for both.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - candidate (pd.Series): The input candidate with potential missing 'd_tstart' or 'd_tstop'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    - pd.Series: The calibrated candidate.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_notnull = pd.notnull(candidate['d_tstart'])\n",
    "    stop_notnull = pd.notnull(candidate['d_tstop']) \n",
    "    \n",
    "    match start_notnull, stop_notnull:\n",
    "        case (True, True):\n",
    "            d_tstart = candidate['d_tstart']\n",
    "            d_tstop = candidate['d_tstop']\n",
    "        case (True, False):\n",
    "            d_tstart = candidate['d_tstart']\n",
    "            d_tstop = candidate['d_time'] -  candidate['d_tstart'] + candidate['d_time']\n",
    "        case (False, True):\n",
    "            d_tstart = candidate['d_time'] -  candidate['d_tstop'] + candidate['d_time']\n",
    "            d_tstop = candidate['d_tstop']\n",
    "        case (False, False):\n",
    "            return pandas.Series({\n",
    "                'd_tstart': None,\n",
    "                'd_tstop': None,\n",
    "            })\n",
    "    \n",
    "    duration = d_tstop - d_tstart\n",
    "    num_of_points_between = data.time.sel(time=slice(d_tstart, d_tstop)).count().item()\n",
    "    \n",
    "    if num_of_points_between <= (duration/data_resolution) * ratio:\n",
    "        d_tstart = None\n",
    "        d_tstop = None\n",
    "    \n",
    "    return pandas.Series({\n",
    "        'd_tstart': d_tstart,\n",
    "        'd_tstop': d_tstop,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minimum variance analysis (MVA) features\n",
    "\n",
    "To ensure the accuracy of MVA, only when the ratio of the middle to the minimum eigenvalue (labeled QMVA for simplicity) is larger than 3 are the results used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exports\n",
    "def minvar(data):\n",
    "    \"\"\"\n",
    "    see `pyspedas.cotrans.minvar`\n",
    "    This program computes the principal variance directions and variances of a\n",
    "    vector quantity as well as the associated eigenvalues.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    data:\n",
    "        Vxyz, an (npoints, ndim) array of data(ie Nx3)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    vrot:\n",
    "        an array of (npoints, ndim) containing the rotated data in the new coordinate system, ijk.\n",
    "        Vi(maximum direction)=vrot[0,:]\n",
    "        Vj(intermediate direction)=vrot[1,:]\n",
    "        Vk(minimum variance direction)=Vrot[2,:]\n",
    "    v:\n",
    "        an (ndim,ndim) array containing the principal axes vectors\n",
    "        Maximum variance direction eigenvector, Vi=v[*,0]\n",
    "        Intermediate variance direction, Vj=v[*,1] (descending order)\n",
    "    w:\n",
    "        the eigenvalues of the computation\n",
    "    \"\"\"\n",
    "\n",
    "    #  Min var starts here\n",
    "    # data must be Nx3\n",
    "    vecavg = np.nanmean(np.nan_to_num(data, nan=0.0), axis=0)\n",
    "\n",
    "    mvamat = np.zeros((3, 3))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            mvamat[i, j] = np.nanmean(np.nan_to_num(data[:, i] * data[:, j], nan=0.0)) - vecavg[i] * vecavg[j]\n",
    "\n",
    "    # Calculate eigenvalues and eigenvectors\n",
    "    w, v = np.linalg.eigh(mvamat, UPLO='U')\n",
    "\n",
    "    # Sorting to ensure descending order\n",
    "    w = np.abs(w)\n",
    "    idx = np.flip(np.argsort(w))\n",
    "\n",
    "    # IDL compatability\n",
    "    if True:\n",
    "        if np.sum(w) == 0.0:\n",
    "            idx = [0, 2, 1]\n",
    "\n",
    "    w = w[idx]\n",
    "    v = v[:, idx]\n",
    "\n",
    "    # Rotate intermediate var direction if system is not Right Handed\n",
    "    YcrossZdotX = v[0, 0] * (v[1, 1] * v[2, 2] - v[2, 1] * v[1, 2])\n",
    "    if YcrossZdotX < 0:\n",
    "        v[:, 1] = -v[:, 1]\n",
    "        # v[:, 2] = -v[:, 2] # Should not it is being flipped at Z-axis?\n",
    "\n",
    "    # Ensure minvar direction is along +Z (for FAC system)\n",
    "    if v[2, 2] < 0:\n",
    "        v[:, 2] = -v[:, 2]\n",
    "        v[:, 1] = -v[:, 1]\n",
    "\n",
    "    vrot = np.array([np.dot(row, v) for row in data])\n",
    "\n",
    "    return vrot, v, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mva_features(data: np.ndarray) -> list[Any]:\n",
    "    \"\"\"\n",
    "    Compute MVA features based on the given data array.\n",
    "\n",
    "    Parameters:\n",
    "    - data (np.ndarray): Input data\n",
    "\n",
    "    Returns:\n",
    "    - List: Computed features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute variance properties\n",
    "    vrot, v, w = minvar(data)\n",
    "\n",
    "    # Maximum variance direction eigenvector\n",
    "    Vl = v[:, 0]\n",
    "\n",
    "    vec_mag = np.linalg.norm(vrot, axis=1)\n",
    "    \n",
    "    # Compute changes in each component of B_rot\n",
    "    dvec = [vrot[0, i] - vrot[-1, i] for i in range(3)]\n",
    "    \n",
    "    # Compute mean values\n",
    "    vec_mag_mean = np.mean(vec_mag)\n",
    "    vec_n_mean = np.mean(vrot[:, 2])\n",
    "    VnOverVmag = vec_n_mean / vec_mag_mean\n",
    "\n",
    "    # Compute relative changes in magnitude\n",
    "    dvec_mag = vec_mag[-1] - vec_mag[0]\n",
    "    dBOverB = np.abs(dvec_mag / vec_mag_mean)\n",
    "    dBOverB_max = (np.max(vec_mag) - np.min(vec_mag)) / vec_mag_mean\n",
    "    \n",
    "    \n",
    "    results = [\n",
    "        Vl[0], Vl[1], Vl[2],\n",
    "        w[0], w[1], w[2],\n",
    "        w[1] / w[2],\n",
    "        vec_mag_mean,\n",
    "        vec_n_mean,\n",
    "        dvec_mag,\n",
    "        VnOverVmag, \n",
    "        dBOverB,\n",
    "        dBOverB_max,\n",
    "        dvec[0], dvec[1], dvec[2]\n",
    "    ]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "from fastcore.test import *\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)  # for reproducibility\n",
    "data = np.random.rand(100, 3)  # 100 time points, 3-dimensional data\n",
    "# Call the mva_features function\n",
    "features = mva_features(data)\n",
    "_features = [0.3631060892452051, 0.8978455426527485, -0.24905290500542857, 0.09753158579102299, 0.086943767300213, 0.07393142040422575, 1.1760056390752571, 0.9609421690770317, 0.6152039820297959, -0.5922397773398479, 0.6402091632847049, 0.61631157045453, 1.2956351134759623, 0.19091785005728523, 0.5182488424049534, 0.4957624347593598]\n",
    "test_eq(features, _features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field rotation angles\n",
    "The PDF of the field rotation angles across the solar-wind IDs is well fitted by the exponential function exp(−θ/)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_rotation_angle(v1: np.ndarray, v2: np.ndarray):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - v1: The first vector.\n",
    "    - v2: The second vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    if v1.shape != v2.shape:\n",
    "        raise ValueError(\"Vectors must have the same shape.\")\n",
    "    \n",
    "    # Normalize the vectors\n",
    "    v1_u = v1 / np.linalg.norm(v1, axis=-1, keepdims=True)\n",
    "    v2_u = v2 / np.linalg.norm(v2, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Calculate the cosine of the angle for each time step\n",
    "    cosine_angle = np.sum(v1_u * v2_u, axis=-1)\n",
    "    \n",
    "    # Clip the values to handle potential floating point errors\n",
    "    cosine_angle = np.clip(cosine_angle, -1, 1)\n",
    "    \n",
    "    angle = np.arccos(cosine_angle)\n",
    "    \n",
    "    # Convert the angles from radians to degrees\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def calc_candidate_rotation_angle(candidates, data:  xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle(s) at two different time steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    tstart = candidates['d_tstart']\n",
    "    tstop = candidates['d_tstop']\n",
    "    \n",
    "    # Convert Series to numpy arrays if necessary\n",
    "    if isinstance(tstart, pd.Series):\n",
    "        tstart = tstart.to_numpy()\n",
    "        tstop = tstop.to_numpy()\n",
    "        # no need to Handle NaT values (as `calibrate_candidate_duration` will handle this)\n",
    "    \n",
    "    # Get the vectors at the two time steps\n",
    "    vecs_before = data.sel(time=tstart, method=\"nearest\").to_numpy()\n",
    "    vecs_after = data.sel(time=tstop, method=\"nearest\").to_numpy()\n",
    "    \n",
    "    # Compute the rotation angle(s)\n",
    "    rotation_angles = calc_rotation_angle(vecs_before, vecs_after)\n",
    "    return rotation_angles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cool_planet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
