{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: ID properties\n",
    "subtitle: Full feature extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core/propeties\n",
    "#| export\n",
    "#| code-summary: \"Import all the packages needed for the project\"\n",
    "import polars as pl\n",
    "import xarray as xr\n",
    "from fastcore.all import patch\n",
    "\n",
    "try:\n",
    "    import modin.pandas as pd\n",
    "    import modin.pandas as mpd\n",
    "    from modin.config import ProgressBar\n",
    "    ProgressBar.enable()\n",
    "except ImportError:\n",
    "    import pandas as pd\n",
    "import pandas\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "import pdpipe as pdp\n",
    "from pdpipe.util import out_of_place_col_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def get_candidate_data(\n",
    "    candidate: dict, data: xr.DataArray, neighbor: int = 0\n",
    ") -> xr.DataArray:\n",
    "    duration = candidate[\"tstop\"] - candidate[\"tstart\"]\n",
    "    offset = neighbor * duration\n",
    "    temp_tstart = candidate[\"tstart\"] - offset\n",
    "    temp_tstop = candidate[\"tstop\"] + offset\n",
    "\n",
    "    return data.sel(time=slice(temp_tstart, temp_tstop))\n",
    "\n",
    "\n",
    "def get_candidates(candidates: pd.DataFrame, candidate_type=None, num: int = 4):\n",
    "    if candidate_type is not None:\n",
    "        _candidates = candidates[candidates[\"type\"] == candidate_type]\n",
    "    else:\n",
    "        _candidates = candidates\n",
    "\n",
    "    # Sample a specific number of candidates if num is provided and it's less than the total number\n",
    "    if num < len(_candidates):\n",
    "        logger.info(\n",
    "            f\"Sampling {num} {candidate_type} candidates out of {len(_candidates)}\"\n",
    "        )\n",
    "        return _candidates.sample(num)\n",
    "    else:\n",
    "        return _candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from discontinuitypy.propeties.duration import calc_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def calc_candidate_duration(candidate, data, **kwargs):\n",
    "    try:\n",
    "        candidate_data = get_candidate_data(candidate, data)\n",
    "        result = calc_duration(candidate_data, **kwargs)\n",
    "        return pandas.Series(result)\n",
    "    except Exception as e:\n",
    "        logger.debug(\n",
    "            f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\"\n",
    "        )\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum variance analysis (MVA) features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from discontinuitypy.propeties.mva import calc_candidate_mva_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field rotation angles\n",
    "The PDF of the field rotation angles across the solar-wind IDs is well fitted by the exponential function exp(−θ/)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_data_at_times(data: xr.DataArray, times) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Select data at specified times.\n",
    "    \"\"\"\n",
    "    # Use xarray's selection capability if data supports it\n",
    "    return data.sel(time=times, method=\"nearest\").to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_rotation_angle(v1, v2):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle between two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - v1: The first vector(s).\n",
    "    - v2: The second vector(s).\n",
    "    \"\"\"\n",
    "    \n",
    "    if v1.shape != v2.shape:\n",
    "        raise ValueError(\"Vectors must have the same shape.\")\n",
    "    \n",
    "    # Normalize the vectors\n",
    "    v1_u = v1 / np.linalg.norm(v1, axis=-1, keepdims=True)\n",
    "    v2_u = v2 / np.linalg.norm(v2, axis=-1, keepdims=True)\n",
    "    \n",
    "    # Calculate the cosine of the angle for each time step\n",
    "    cosine_angle = np.sum(v1_u * v2_u, axis=-1)\n",
    "    \n",
    "    # Clip the values to handle potential floating point errors\n",
    "    cosine_angle = np.clip(cosine_angle, -1, 1)\n",
    "    \n",
    "    angle = np.arccos(cosine_angle)\n",
    "    \n",
    "    # Convert the angles from radians to degrees\n",
    "    return np.degrees(angle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_events_rotation_angle(events, data: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the rotation angle(s) at two different time steps.\n",
    "    \"\"\"\n",
    "    tstart = events['t.d_start'].to_numpy()\n",
    "    tstop = events['t.d_end'].to_numpy()\n",
    "\n",
    "    vecs_before = get_data_at_times(data, tstart)\n",
    "    vecs_after = get_data_at_times(data, tstop)\n",
    "\n",
    "    rotation_angles = calc_rotation_angle(vecs_before, vecs_after)\n",
    "    return rotation_angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calc_normal_direction(v1, v2, normalize=True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the normal direction of two vectors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    v1 : array_like \n",
    "        The first vector(s).\n",
    "    v2 : array_like \n",
    "        The second vector(s).\n",
    "    \"\"\"\n",
    "    c = np.cross(v1, v2)\n",
    "    return c / np.linalg.norm(c, axis=-1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def calc_events_normal_direction(events, data: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Computes the normal directions(s) at two different time steps.\n",
    "    \"\"\"\n",
    "    tstart = events['t.d_start'].to_numpy()\n",
    "    tstop = events['t.d_end'].to_numpy()\n",
    "\n",
    "    vecs_before = get_data_at_times(data, tstart)\n",
    "    vecs_after = get_data_at_times(data, tstop)\n",
    "\n",
    "    normal_directions = calc_normal_direction(vecs_before, vecs_after)\n",
    "    # need to convert to list first, as only 1D array is supported\n",
    "    return normal_directions.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def calc_events_vec_change(events, data: xr.DataArray):\n",
    "    \"\"\"\n",
    "    Utils function to calculate features related to the change of the magnetic field\n",
    "    \"\"\"\n",
    "    tstart = events['t.d_start'].to_numpy()\n",
    "    tstop = events['t.d_end'].to_numpy()\n",
    "    \n",
    "    vecs_before = get_data_at_times(data, tstart)\n",
    "    vecs_after = get_data_at_times(data, tstop)\n",
    "    return (vecs_after - vecs_before).tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "patch `pdp.ApplyToRows` to work with `modin` and `xorbits` DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _transform(self: pdp.ApplyToRows, X, verbose):\n",
    "    new_cols = X.apply(self._func, axis=1)\n",
    "    if isinstance(new_cols, (pd.Series, pandas.Series)):\n",
    "        loc = len(X.columns)\n",
    "        if self._follow_column:\n",
    "            loc = X.columns.get_loc(self._follow_column) + 1\n",
    "        return out_of_place_col_insert(\n",
    "            X=X, series=new_cols, loc=loc, column_name=self._colname\n",
    "        )\n",
    "    if isinstance(new_cols, (mpd.DataFrame, pandas.DataFrame)):\n",
    "        sorted_cols = sorted(list(new_cols.columns))\n",
    "        new_cols = new_cols[sorted_cols]\n",
    "        if self._follow_column:\n",
    "            inter_X = X\n",
    "            loc = X.columns.get_loc(self._follow_column) + 1\n",
    "            for colname in new_cols.columns:\n",
    "                inter_X = out_of_place_col_insert(\n",
    "                    X=inter_X,\n",
    "                    series=new_cols[colname],\n",
    "                    loc=loc,\n",
    "                    column_name=colname,\n",
    "                )\n",
    "                loc += 1\n",
    "            return inter_X\n",
    "        assign_map = {\n",
    "            colname: new_cols[colname] for colname in new_cols.columns\n",
    "        }\n",
    "        return X.assign(**assign_map)\n",
    "    raise TypeError(  # pragma: no cover\n",
    "        \"Unexpected type generated by applying a function to a DataFrame.\"\n",
    "        \" Only Series and DataFrame are allowed.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pipelines` Class for processing IDs\n",
    "\n",
    "Notes: Using `lambda` function instead of `partial` because of `partial` freezeing the args decreasing the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class IDsPdPipeline:\n",
    "    @staticmethod\n",
    "    def calc_duration(data: xr.DataArray, **kwargs):\n",
    "        return pdp.ApplyToRows(\n",
    "            lambda df: calc_candidate_duration(df, data, **kwargs),\n",
    "            func_desc=\"calculating pre-duration parameters\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_mva_features(data, **kwargs):\n",
    "        return pdp.ApplyToRows(\n",
    "            lambda df: calc_candidate_mva_features(df, data, **kwargs),\n",
    "            func_desc=\"calculating MVA features\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_vec_change(data, **kwargs):\n",
    "        return pdp.ColByFrameFunc(\n",
    "            \"dB\",\n",
    "            lambda df: calc_events_vec_change(df, data, **kwargs),\n",
    "            func_desc=\"calculating compound change\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_rotation_angle(data, **kwargs):\n",
    "        return pdp.ColByFrameFunc(\n",
    "            \"rotation_angle\",\n",
    "            lambda df: calc_events_rotation_angle(df, data, **kwargs),\n",
    "            func_desc=\"calculating rotation angle\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_normal_direction(data, name=\"normal_direction\", **kwargs):\n",
    "        return pdp.ColByFrameFunc(\n",
    "            name,\n",
    "            lambda df: calc_events_normal_direction(df, data, **kwargs),\n",
    "            func_desc=\"calculating normal direction\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from beforerr.polars import convert_to_pd_dataframe, decompose_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def process_events(\n",
    "    candidates_pl: pl.DataFrame,  # potential candidates DataFrame\n",
    "    sat_fgm: xr.DataArray,  # satellite FGM data\n",
    "    data_resolution: timedelta,  # time resolution of the data\n",
    "    modin=True,\n",
    "    method: Literal[\"fit\", \"derivative\"]= \"fit\",\n",
    "    **kwargs,\n",
    ") -> pl.DataFrame:\n",
    "    \"Process candidates DataFrame\"\n",
    "\n",
    "    candidates = pd.DataFrame(convert_to_pd_dataframe(candidates_pl, modin=modin))\n",
    "    \n",
    "    if method == \"fit\":\n",
    "        duration_method = \"distance\"  \n",
    "    else:\n",
    "        duration_method = \"derivative\"\n",
    "\n",
    "    candidates = (\n",
    "        IDsPdPipeline.calc_duration(sat_fgm, method=duration_method, **kwargs)\n",
    "        .apply(candidates)\n",
    "        .dropna()\n",
    "    )  # Remove candidates with NaN values)\n",
    "\n",
    "    ids = (\n",
    "        IDsPdPipeline.calc_mva_features(sat_fgm, method=method, **kwargs)\n",
    "        + IDsPdPipeline.calc_vec_change(sat_fgm)\n",
    "        + IDsPdPipeline.calc_rotation_angle(sat_fgm)\n",
    "        + IDsPdPipeline.calc_normal_direction(sat_fgm, name=\"k\")\n",
    "    ).apply(candidates)\n",
    "\n",
    "    if isinstance(ids, mpd.DataFrame):\n",
    "        ids = ids._to_pandas()\n",
    "\n",
    "    vectors2decompose = [\"dB\", \"dB_lmn\", \"k\", \"Vl\", \"Vn\"]\n",
    "\n",
    "    df = pl.DataFrame(\n",
    "        ids.dropna(), schema_overrides={vec: pl.List for vec in vectors2decompose}\n",
    "    )  # ArrowInvalid: Could not convert [0.9799027968348948, -0.17761542644940076, -0.07309766783111293] with type list: tried to convert to double\n",
    "\n",
    "    if method == \"fit\":\n",
    "        duration_expr = pl.col(\"fit.vars.sigma\") * 2\n",
    "    else:\n",
    "        duration_expr = (\n",
    "            pl.col(\"t.d_end\") - pl.col(\"t.d_start\")\n",
    "        ).dt.total_nanoseconds() / 1e9  # convert to seconds\n",
    "\n",
    "    for vec in vectors2decompose:\n",
    "        df = decompose_vector(df, vec)\n",
    "\n",
    "    return df.with_columns(duration=duration_expr).drop(vectors2decompose)\n",
    "    # ValueError: Data type fixed_size_list[pyarrow] not supported by interchange protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export\n",
    "nbdev_export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test parallelization\n",
    "\n",
    "\n",
    "Generally `mapply` and `modin` are the fastest. `xorbits` is expected to be the fastest but it is not and it is the slowest one.\n",
    "\n",
    "```python\n",
    "#| notest\n",
    "sat = 'jno'\n",
    "coord = 'se'\n",
    "cols = [\"BX\", \"BY\", \"BZ\"]\n",
    "tau = timedelta(seconds=60)\n",
    "data_resolution = timedelta(seconds=1)\n",
    "\n",
    "if True:\n",
    "    year = 2012\n",
    "    files = f'../data/{sat}_data_{year}.parquet'\n",
    "    output = f'../data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n",
    "\n",
    "    data = pl.scan_parquet(files).set_sorted('time').collect()\n",
    "\n",
    "    indices = compute_indices(data, tau)\n",
    "    # filter condition\n",
    "    sparse_num = tau / data_resolution // 3\n",
    "    filter_condition = filter_indices(sparse_num = sparse_num)\n",
    "\n",
    "    candidates = indices.filter(filter_condition).with_columns(pl_format_time(tau)).sort('time')\n",
    "    \n",
    "    data_c = compress_data_by_events(data, candidates, tau)\n",
    "    sat_fgm = df2ts(data_c, cols, attrs={\"units\": \"nT\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "candidates_pd = candidates.to_pandas()\n",
    "candidates_modin = mpd.DataFrame(candidates_pd)\n",
    "# candidates_x = xpd.DataFrame(candidates_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: Test different libraries to parallelize the computation\n",
    "#| notest\n",
    "if True:\n",
    "    pdp_test = pdp.ApplyToRows(\n",
    "        lambda candidate: calc_candidate_duration(candidate, sat_fgm),  # fast a little bit\n",
    "        # lambda candidate: calc_duration(get_candidate_data_xr(candidate, sat_fgm)),\n",
    "        # lambda candidate: calc_duration(sat_fgm.sel(time=slice(candidate['tstart'], candidate['tstop']))),\n",
    "        func_desc=\"calculating duration parameters\",\n",
    "    )\n",
    "    \n",
    "    # process_events(candidates_modin, sat_fgm, sat_state, data_resolution)\n",
    "    \n",
    "    # ---\n",
    "    # successful cases\n",
    "    # ---\n",
    "    # candidates_pd.mapply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works, 4.2 secs\n",
    "    # candidates_pd.mapply(calc_candidate_duration, axis=1, data=sat_fgm) # this works, but a little bit slower, 6.7 secs\n",
    "    \n",
    "    # candidates_pd.apply(calc_candidate_duration, axis=1, data=sat_fgm) # Standard case: 24+s secs\n",
    "    # candidates_pd.swifter.apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 80 secs\n",
    "    # candidates_pd.swifter.set_dask_scheduler(scheduler=\"threads\").apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 60 secs\n",
    "    # candidates_modin.apply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works with ray, 6 secs # NOTE: can not work with dask\n",
    "    # candidates_x.apply(calc_candidate_duration, axis=1, data=sat_fgm) # 30 seconds\n",
    "    # pdp_test(candidates_modin) # this works, 8 secs\n",
    "    \n",
    "    # ---\n",
    "    # failed cases\n",
    "    # ---\n",
    "    # candidates_modin.apply(calc_candidate_duration, axis=1, data=sat_fgm) # AttributeError: 'DataFrame' object has no attribute 'sel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(task_dict, number=1):\n",
    "    results = {}\n",
    "    for name, (data, task) in task_dict.items():\n",
    "        try:\n",
    "            time_taken = timeit.timeit(\n",
    "                lambda: task(data),\n",
    "                number=number\n",
    "            )\n",
    "            results[name] = time_taken / number\n",
    "        except Exception as e:\n",
    "            results[name] = str(e)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "\n",
    "def benchmark_results(results, sat_fgm):\n",
    "    func = partial(calc_candidate_duration, data=sat_fgm)\n",
    "    task_dict = {\n",
    "        'pandas': (candidates_pd, lambda _: _.apply(func, axis=1)),\n",
    "        'pandas-mapply': (candidates_pd, lambda _: _.mapply(func, axis=1)),\n",
    "        'modin': (candidates_modin, lambda _: _.apply(func, axis=1)),\n",
    "        # 'xorbits': (candidates_x, lambda _: _.apply(func, axis=1)),\n",
    "    }\n",
    "\n",
    "    results = benchmark(task_dict)\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
