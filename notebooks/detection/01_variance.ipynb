{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Variance method\n",
    "description: Large variance in the magnetic field compared with neighboring intervals\n",
    "---\n",
    "\n",
    "References: [@liuMagneticDiscontinuitiesSolar2022]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For each sampling instant $t$, we define three intervals: the pre-interval $[-1,-1/2]\\cdot T+t$, the middle interval $[-1/,1/2]\\cdot T+t$, and the post-interval $[1/2,1]\\cdot T+t$, in which $T$ are time lags. Let time series of the magnetic field data in these three intervals are labeled ${\\mathbf B}_-$, ${\\mathbf B}_0$, ${\\mathbf B}_+$, respectively. Compute the following indices:\n",
    "\n",
    "$$\n",
    "I_1 = \\frac{\\sigma(B_0)}{Max(\\sigma(B_-),\\sigma(B_+))}\n",
    "$$\n",
    "\n",
    "$$\n",
    "I_2 = \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "I_3 = \\frac{| \\Delta \\vec{B} |}{|B_{bg}|}\n",
    "$$\n",
    "\n",
    "By selecting a large and reasonable threshold for the ﬁrst two indices ($I_1>2, I_2>1$) , we could guarantee that the ﬁeld changes of the IDs identiﬁed are large enough to be distinguished from the stochastic ﬂuctuations on magnetic ﬁelds, while the third is a supplementary condition to reduce the uncertainty of recognition. While the third index (relative field jump) is a supplementary condition to reduce the uncertainty of recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of the standard deviation\n",
    "\n",
    "$$\n",
    "I_1 = \\frac{\\sigma(B_0)}{Max(\\sigma(B_-),\\sigma(B_+))}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp detection/variance\n",
    "# | export\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "from datetime import timedelta\n",
    "from beforerr.polars import pl_norm, format_time\n",
    "from discontinuitypy.utils.basic import _expand_selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compute_std(\n",
    "    df: pl.LazyFrame,\n",
    "    period: timedelta,  # period to group by\n",
    "    index_column=\"time\",\n",
    "    cols: list[str] = [\"BX\", \"BY\", \"BZ\"],\n",
    "    every: timedelta = None,  # every to group by (default: period / 2)\n",
    "    result_column=\"std\",\n",
    "):\n",
    "    every = every or period / 2\n",
    "\n",
    "    std_cols = [col_name + \"_std\" for col_name in cols]\n",
    "\n",
    "    std_df = (\n",
    "        df.group_by_dynamic(index_column, every=every, period=period)\n",
    "        .agg(\n",
    "            pl.len(),\n",
    "            pl.col(cols).std(ddof=0).name.suffix(\"_std\"),\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl_norm(std_cols).alias(result_column),\n",
    "        )\n",
    "        .drop(std_cols)\n",
    "    )\n",
    "    return std_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def add_neighbor_std(\n",
    "    df: pl.LazyFrame,\n",
    "    tau: timedelta,\n",
    "    join_strategy=\"inner\",\n",
    "    std_column=\"std\",\n",
    "    time_column=\"time\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the neighbor standard deviations\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - df (pl.LazyFrame): The input DataFrame.\n",
    "    - tau : The time interval value.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Simply shift would not work correctly if data is missing, like `std_next = pl.col(\"B_std\").shift(-2)`.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the standard deviation index\n",
    "    prev_std_df = df.select(\n",
    "        pl.col(time_column) + tau,\n",
    "        cs.by_name(std_column, \"len\").name.suffix(\"_prev\"),\n",
    "    ).pipe(format_time)\n",
    "\n",
    "    next_std_df = df.select(\n",
    "        pl.col(time_column) - tau,\n",
    "        cs.by_name(std_column, \"len\").name.suffix(\"_next\"),\n",
    "    ).pipe(format_time)\n",
    "\n",
    "    return df.join(prev_std_df, on=time_column, how=join_strategy).join(\n",
    "        next_std_df, on=time_column, how=join_strategy\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compute_index_std(\n",
    "    df: pl.LazyFrame,\n",
    "    std_column=\"std\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the standard deviation index based on the given DataFrame\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    - df (pl.LazyFrame): The input DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    - pl.LazyFrame: DataFrame with calculated 'index_std' column.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> index_std_df = compute_index_std_pl(df)\n",
    "    >>> index_std_df\n",
    "    \"\"\"\n",
    "\n",
    "    return df.with_columns(\n",
    "        index_std=pl.col(std_column)\n",
    "        / pl.max_horizontal(f\"{std_column}_prev\", f\"{std_column}_next\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of fluctuation\n",
    "\n",
    "$$\n",
    "I_2 = \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compute_combinded_std(\n",
    "    df: pl.LazyFrame,\n",
    "    cols: list[str],\n",
    "    every: timedelta,  # every to group by (default: period / 2)\n",
    "    period: timedelta = None,  # period to group by\n",
    "    index_column=\"time\",\n",
    "    result_column=\"std_combined\",\n",
    "):\n",
    "    prev_df = df.with_columns(pl.col(index_column) + period)\n",
    "    next_df = df.with_columns(pl.col(index_column) - period)\n",
    "    return (\n",
    "        pl.concat([prev_df, next_df])\n",
    "        .sort(index_column)\n",
    "        .group_by_dynamic(index_column, every=every, period=period)\n",
    "        .agg(cs.by_name(cols).std(ddof=0).name.suffix(\"_combined\"))\n",
    "        .select(\n",
    "            index_column,\n",
    "            pl_norm([col_name + \"_combined\" for col_name in cols]).alias(result_column),\n",
    "        )\n",
    "        .pipe(format_time)\n",
    "    )\n",
    "\n",
    "\n",
    "# | export\n",
    "def compute_index_fluctuation(df: pl.LazyFrame, std_column=\"std\", clean=True):\n",
    "    std_combined = pl.col(f\"{std_column}_combined\")\n",
    "    std_added = pl.sum_horizontal(f\"{std_column}_prev\", f\"{std_column}_next\")\n",
    "\n",
    "    index_df = df.with_columns(index_fluctuation=std_combined / std_added)\n",
    "    return index_df.drop(f\"{std_column}_combined\") if clean else index_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of the relative field jump\n",
    "\n",
    "$$\n",
    "I_3 = \\frac{| \\Delta \\vec{B} |}{|B_{bg}|}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def pl_dvec(columns, *more_columns):\n",
    "    all_columns = _expand_selectors(columns, *more_columns)\n",
    "    return [\n",
    "        (pl.col(column).first() - pl.col(column).last()).alias(f\"d{column}_vec\")\n",
    "        for column in all_columns\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compute_index_diff(\n",
    "    df: pl.LazyFrame,\n",
    "    every: timedelta,\n",
    "    cols: list[str],\n",
    "    period: timedelta = None,\n",
    "    clean=True,\n",
    "):\n",
    "    db_cols = [\"d\" + col + \"_vec\" for col in cols]\n",
    "\n",
    "    index_diff = (\n",
    "        df.with_columns(pl_norm(cols).alias(\"_vec_mag\"))\n",
    "        .group_by_dynamic(\"time\", every=every, period=period)\n",
    "        .agg(\n",
    "            pl.col(\"_vec_mag\").mean().name.suffix(\"_mean\"),\n",
    "            *pl_dvec(cols),\n",
    "        )\n",
    "        .with_columns(pl_norm(db_cols).alias(\"_dvec_mag\"))\n",
    "        .with_columns(index_diff=pl.col(\"_dvec_mag\") / pl.col(\"_vec_mag_mean\"))\n",
    "    )\n",
    "\n",
    "    if clean:\n",
    "        return index_diff.drop(\"_vec_mag_mean\", \"_dvec_mag\", *db_cols)\n",
    "    else:\n",
    "        return index_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def compute_indices(\n",
    "    df: pl.LazyFrame,\n",
    "    tau: timedelta,\n",
    "    cols: list[str],\n",
    "    clean=True,\n",
    "    join_strategy=\"inner\",\n",
    "    on=\"time\",\n",
    ") -> pl.LazyFrame:\n",
    "    \"\"\"\n",
    "    Compute all index based on the given DataFrame and tau value.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pl.DataFrame\n",
    "        Input DataFrame.\n",
    "    tau : datetime.timedelta\n",
    "        Time interval value.\n",
    "    cols : list\n",
    "        List of column names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple :\n",
    "        Tuple containing DataFrame results for fluctuation index,\n",
    "        standard deviation index, and 'index_num'.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> indices = compute_indices(df, tau)\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This is a wrapper for `_compute_indices` with `pl.LazyFrame` input.\n",
    "    - Simply shift to calculate index_std would not work correctly if data is missing,\n",
    "        like `std_next = pl.col(\"B_std\").shift(-2)`.\n",
    "    - Drop null though may lose some IDs (using the default `join_strategy`).\n",
    "        Because we could not tell if it is a real ID or just a partial wave\n",
    "        from incomplete data without previous or/and next std.\n",
    "        Hopefully we can pick up the lost ones with smaller tau.\n",
    "    - TODO: Can be optimized further, but this is already fast enough.\n",
    "        - TEST: if `join` can be improved by shift after filling the missing values.\n",
    "        - TEST: if `list` in `polars` really fast?\n",
    "    \"\"\"\n",
    "\n",
    "    every = tau / 2\n",
    "    period = tau\n",
    "\n",
    "    df = df.pipe(format_time).sort(on)\n",
    "\n",
    "    stds_df = df.pipe(compute_std, period=period, cols=cols).pipe(\n",
    "        add_neighbor_std, tau=tau\n",
    "    )\n",
    "\n",
    "    combined_std_df = compute_combinded_std(df, cols, every=every, period=period)\n",
    "\n",
    "    indices = (\n",
    "        df.pipe(compute_index_diff, every=every, period=period, cols=cols)\n",
    "        .join(stds_df, on=on)\n",
    "        .join(combined_std_df, on=on, how=join_strategy)\n",
    "        .pipe(compute_index_std)\n",
    "        .pipe(compute_index_fluctuation, clean=clean)\n",
    "    )\n",
    "\n",
    "    if clean:\n",
    "        return indices.drop([\"std_prev\", \"std_next\"])\n",
    "    else:\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "INDEX_STD_THRESHOLD = 2\n",
    "INDEX_FLUC_THRESHOLD = 1\n",
    "INDEX_DIFF_THRESHOLD = 0.1\n",
    "SPARSE_THRESHOLD = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def filter_indices(\n",
    "    df: pl.LazyFrame,\n",
    "    index_std_threshold: float = INDEX_STD_THRESHOLD,\n",
    "    index_fluc_threshold: float = INDEX_FLUC_THRESHOLD,\n",
    "    index_diff_threshold: float = INDEX_DIFF_THRESHOLD,\n",
    "    sparse_num: int = SPARSE_THRESHOLD,\n",
    ") -> pl.LazyFrame:\n",
    "    # filter indices to get possible IDs\n",
    "\n",
    "    return df.filter(\n",
    "        pl.col(\"index_std\") > index_std_threshold,\n",
    "        pl.col(\"index_fluctuation\") > index_fluc_threshold,\n",
    "        pl.col(\"index_diff\") > index_diff_threshold,\n",
    "        pl.col(\n",
    "            \"index_std\"\n",
    "        ).is_finite(),  # for cases where neighboring groups have std=0\n",
    "        pl.col(\"len\") > sparse_num,\n",
    "        pl.col(\"len_prev\")\n",
    "        > sparse_num,  # filter out sparse intervals, which may give unreasonable results.\n",
    "        pl.col(\"len_next\")\n",
    "        > sparse_num,  # filter out sparse intervals, which may give unreasonable results.\n",
    "    ).drop([\"len_prev\", \"len_next\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_combinded_std(df: pl.LazyFrame, tau, cols: list[str]):\n",
    "    combined_std_cols = [col_name + \"_combined_std\" for col_name in cols]\n",
    "    offsets = [0 * tau, tau / 2]\n",
    "    combined_std_dfs = []\n",
    "\n",
    "    for offset in offsets:\n",
    "        truncated_df = df.select(\n",
    "            (pl.col(\"time\") - offset).dt.truncate(tau, offset=offset).alias(\"time\"),\n",
    "            pl.col(cols),\n",
    "        )\n",
    "\n",
    "        prev_df = truncated_df.select(\n",
    "            (pl.col(\"time\") + tau),\n",
    "            pl.col(cols),\n",
    "        )\n",
    "\n",
    "        next_df = truncated_df.select(\n",
    "            (pl.col(\"time\") - tau),\n",
    "            pl.col(cols),\n",
    "        )\n",
    "\n",
    "        temp_combined_std_df = (\n",
    "            pl.concat([prev_df, next_df])\n",
    "            .group_by(\"time\")\n",
    "            .agg(pl.col(cols).std(ddof=0).name.suffix(\"_combined_std\"))\n",
    "            .with_columns(B_std_combined=pl_norm(combined_std_cols))\n",
    "            .drop(combined_std_cols)\n",
    "            .sort(\"time\")\n",
    "        )\n",
    "\n",
    "        combined_std_dfs.append(temp_combined_std_df)\n",
    "\n",
    "    combined_std_df = pl.concat(combined_std_dfs)\n",
    "    return combined_std_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
