{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: IDs from ARTHEMIS\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "output-file: artemis.html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: import all the packages needed for the project\n",
    "#| output: hide\n",
    "\n",
    "from ids_finder.utils import *\n",
    "from ids_finder.core import *\n",
    "from fastcore.utils import *\n",
    "from fastcore.test import *\n",
    "\n",
    "import polars as pl\n",
    "try:\n",
    "    import modin.pandas as pd\n",
    "    import modin.pandas as mpd\n",
    "except ImportError:\n",
    "    import pandas as pd\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "from datetime import timedelta\n",
    "from loguru import logger\n",
    "import speasy as spz\n",
    "from multipledispatch import dispatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speasy.products import SpeasyVariable\n",
    "from humanize import naturalsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@threaded\n",
    "def download_data(products, trange):\n",
    "    logger.info(\"Downloading data\")\n",
    "    spz.get_data(products, trange, progress=True, disable_proxy=True)\n",
    "    logger.info(\"Data downloaded\")\n",
    "    # spz.get_data(products, jno_start_date, jno_end_date)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def data_preview(data: SpeasyVariable):\n",
    "    print(\"===========================================\")\n",
    "    print(f\"Name:         {data.name}\")\n",
    "    print(f\"Columns:      {data.columns}\")\n",
    "    print(f\"Values Unit:  {data.unit}\")\n",
    "    print(f\"Memory usage: {naturalsize(data.nbytes)}\")\n",
    "    print(f\"Axes Labels:  {data.axes_labels}\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(f\"Meta-data:    {data.meta}\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(f\"Time Axis:    {data.time[:3]}\")\n",
    "    print(\"-------------------------------------------\")\n",
    "    print(f\"Values:       {data.values[:3]}\")\n",
    "    print(\"===========================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artemis_probes = [\"b\", \"c\"]\n",
    "probe = artemis_probes[0]\n",
    "\n",
    "jno_start_date = \"2011-08-25\"\n",
    "jno_end_date = \"2016-06-30\" \n",
    "\n",
    "trange = [jno_start_date, jno_end_date]\n",
    "test_trange = [\"2011-08-25\", \"2011-09-25\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat = 'thb'\n",
    "coord = 'gse'\n",
    "datatype  = 'fgs'\n",
    "\n",
    "sat_fgm_product = f'cda/{sat.upper()}_L2_FGM/{sat}_fgs_gse'\n",
    "sat_pos_sse_product = f'cda/{sat.upper()}_L1_STATE/{sat}_pos_sse'\n",
    "sat_pos_gse_product = f'cda/{sat.upper()}_L1_STATE/{sat}_pos_gse'\n",
    "\n",
    "products = [\n",
    "    sat_fgm_product,\n",
    "    sat_pos_sse_product,\n",
    "    sat_pos_gse_product\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download data in a background thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#| eval: false\n",
       "download_data(products, trange)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "#| eval: false\n",
    "download_data(products, trange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to `parquet` for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spz2parquet(data, force=False):\n",
    "    output = f\"../data/{data.name}.parquet\"\n",
    "    if Path(output).exists() and not force:\n",
    "        logger.info(\"Data already converted to parquet\")\n",
    "    else: \n",
    "        df = pandas.DataFrame(\n",
    "            data.values, index=pandas.Series(data.time, name=\"time\"), columns=data.columns\n",
    "        )\n",
    "        \n",
    "        df.to_parquet(output)\n",
    "        logger.info(\"Data converted to parquet successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "dataset = spz.get_data(products, trange)\n",
       "\n",
       "for data in dataset:\n",
       "    spz2parquet(data, force=False)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "\n",
    "dataset = spz.get_data(products, trange)\n",
    "\n",
    "for data in dataset:\n",
    "    spz2parquet(data, force=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thm_rename_col(col: str):\n",
    "    if \",\" in col:\n",
    "        col = col.split(\",\")[0]\n",
    "    return col.split()[0].upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check and preprocess the data\n",
    "\n",
    "As we are only interested in the data when THEMIS is in the solar wind, for simplicity we will only keep the data when `X, SSE` and `X, GSE` is positive.\n",
    "\n",
    "- State data time resolution is 1 minute...\n",
    "\n",
    "- FGS data time resolution is 4 second..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thm_state(sat):\n",
    "    sat_pos_sse_files = f\"../data/{sat}_pos_sse.parquet\"\n",
    "    sat_pos_sse = pl.scan_parquet(sat_pos_sse_files).set_sorted(\"time\")\n",
    "    sat_pos_gse_files = f\"../data/{sat}_pos_gse.parquet\"\n",
    "    sat_pos_gse = pl.scan_parquet(sat_pos_gse_files).set_sorted(\"time\")\n",
    "    sat_state = sat_pos_sse.join(sat_pos_gse, on=\"time\", how=\"inner\")\n",
    "    return sat_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dispatch(pl.DataFrame)\n",
    "def calc_time_diff(data: pl.DataFrame): \n",
    "    return data.get_column('time').diff(null_behavior=\"drop\").unique().sort()\n",
    "\n",
    "@dispatch(pl.LazyFrame)\n",
    "def calc_time_diff(\n",
    "    data: pl.LazyFrame\n",
    ") -> pl.Series: \n",
    "    return calc_time_diff(data.collect())\n",
    "\n",
    "# get_time_dff(sat_state)\n",
    "# get_time_dff(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sat = \"thb\"\n",
    "coord = \"gse\"\n",
    "datatype = \"fgs\"\n",
    "files = f\"../data/{sat}_{datatype}_{coord}.parquet\"\n",
    "rename_mapping = {\n",
    "    \"Bx FGS-D\": \"BX\",\n",
    "    \"By FGS-D\": \"BY\",\n",
    "    \"Bz FGS-D\": \"BZ\",\n",
    "}\n",
    "\n",
    "\n",
    "output = f\"../data/{sat}_data_sw.parquet\"\n",
    "if Path(output).exists():\n",
    "    pass\n",
    "else:\n",
    "    sat_state = get_thm_state(sat).collect()\n",
    "    sat_state_sw = sat_state.filter((pl.col(\"X, SSE\") >= 0) & (pl.col(\"X, GSE\") >= 0))\n",
    "    data = pl.scan_parquet(files).rename(rename_mapping).unique(\"time\").sort(\"time\")\n",
    "    data_sw = data.join_asof(sat_state_sw, on=\"time\", tolerance=\"1m\").drop_nulls().collect()\n",
    "    data_sw.write_parquet(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "df = (\n",
       "    sat_state_sw.upsample(\"time\", every=\"1m\")\n",
       "    .group_by_dynamic(\"time\", every=\"1d\")\n",
       "    .agg(pl.col(\"X, SSE\").null_count().alias(\"null_count\"))\n",
       "    .with_columns(\n",
       "        pl.when(pl.col(\"null_count\") > 720).then(0).otherwise(1).alias(\"availablity\")\n",
       "    )\n",
       ")\n",
       "\n",
       "properties = {\n",
       "    'width': 800,\n",
       "}\n",
       "\n",
       "chart1 = alt.Chart(df).mark_point().encode(\n",
       "    x='time',\n",
       "    y='null_count'\n",
       ").properties(**properties)\n",
       "\n",
       "chart2  = alt.Chart(df).mark_point().encode(\n",
       "    x='time',\n",
       "    y='availablity'\n",
       ").properties(**properties)\n",
       "\n",
       "(chart1 & chart2)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "df = (\n",
    "    sat_state_sw.upsample(\"time\", every=\"1m\")\n",
    "    .group_by_dynamic(\"time\", every=\"1d\")\n",
    "    .agg(pl.col(\"X, SSE\").null_count().alias(\"null_count\"))\n",
    "    .with_columns(\n",
    "        pl.when(pl.col(\"null_count\") > 720).then(0).otherwise(1).alias(\"availablity\")\n",
    "    )\n",
    ")\n",
    "\n",
    "properties = {\n",
    "    'width': 800,\n",
    "}\n",
    "\n",
    "chart1 = alt.Chart(df).mark_point().encode(\n",
    "    x='time',\n",
    "    y='null_count'\n",
    ").properties(**properties)\n",
    "\n",
    "chart2  = alt.Chart(df).mark_point().encode(\n",
    "    x='time',\n",
    "    y='availablity'\n",
    ").properties(**properties)\n",
    "\n",
    "(chart1 & chart2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-09-27 11:57:07.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_memory_usage\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1m741.8 MB (DataFrame)\u001b[0m\n",
      "\u001b[32m2023-09-27 11:57:07.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_memory_usage\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1m222.6 MB (DataArray)\u001b[0m\n",
      "27-Sep-23 11:57:09: UserWarning: Ray execution environment not yet initialized. Initializing...\n",
      "To remove this warning, run the following python code before doing dataframe operations:\n",
      "\n",
      "    import ray\n",
      "    ray.init()\n",
      "\n",
      "\n",
      "27-Sep-23 11:57:11: Unable to poll TPU GCE metadata: HTTPConnectionPool(host='metadata.google.internal', port=80): Max retries exceeded with url: /computeMetadata/v1/instance/attributes/accelerator-type (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))\n",
      "27-Sep-23 11:57:11: Failed to detect number of TPUs: [Errno 2] No such file or directory: '/dev/vfio'\n",
      "2023-09-27 11:57:12,367\tINFO worker.py:1642 -- Started a local Ray instance.\n",
      "27-Sep-23 11:57:13: UserWarning: Distributing <class 'pandas.core.frame.DataFrame'> object. This may take some time.\n",
      "\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "Estimated completion of line 17: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "Distributing Dataframe: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n",
      "\u001b[32m2023-09-27 11:57:13.471\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_memory_usage\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1m6.3 MB (DataFrame)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6335828"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "sat = \"thb\"\n",
    "bcols = [\"BX\", \"BY\", \"BZ\"]\n",
    "tau = timedelta(seconds=60)\n",
    "data_resolution = timedelta(seconds=4)\n",
    "\n",
    "files = f\"../data/{sat}_data_sw.parquet\"\n",
    "output = f'../data/{sat}_candidates_sw_tau_{tau.seconds}.parquet'\n",
    "\n",
    "data = pl.scan_parquet(files).set_sorted('time').collect()\n",
    "sat_fgm = df2ts(\n",
    "    data, [\"BX\", \"BY\", \"BZ\"], attrs={\"coordinate_system\": coord, \"units\": \"nT\"}\n",
    ")\n",
    "get_memory_usage(data)\n",
    "get_memory_usage(sat_fgm)\n",
    "\n",
    "indices = compute_indices(data, tau)\n",
    "\n",
    "# filter condition\n",
    "sparse_num = tau / data_resolution // 3\n",
    "filter_condition = get_ID_filter_condition(sparse_num = sparse_num)\n",
    "\n",
    "candidates_pl = indices.filter(filter_condition).with_columns(pl_format_time(tau))\n",
    "candidates = convert_to_dataframe(candidates_pl)\n",
    "get_memory_usage(candidates)\n",
    "# del indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Estimated completion of line 17: 100%██████████ Elapsed time: 00:00, estimated remaining time: 00:00\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "ids = process_candidates(candidates, sat_fgm, data, data_resolution)\n",
    "ids = ids.unique([\"d_time\", \"d_tstart\", \"d_tstop\"])\n",
    "ids.write_parquet(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "test_eq(ids.unique([\"d_time\", \"d_tstart\", \"d_tstop\"]).shape, ids.unique(\"d_time\").shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obsolete codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import pycdfpp\n",
    "import pyspedas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "def convert_thm_state_to_parquet(\n",
    "    probe: str, trange\n",
    "):\n",
    "    file_name = f\"./data/th{probe}_state.parquet\"\n",
    "    if os.path.exists(file_name):\n",
    "        return file_name\n",
    "\n",
    "    start = trange.start.to_string()\n",
    "    end = trange.end.to_string()\n",
    "\n",
    "    files = pyspedas.themis.state(\n",
    "        probe=probe,\n",
    "        trange=[start, end],\n",
    "        downloadonly=True,\n",
    "        no_update=True,\n",
    "    )\n",
    "\n",
    "    thm_pos_sse_Xs = []\n",
    "    thm_pos_gse_Xs = []\n",
    "    thm_state_times = []\n",
    "    for file in files:\n",
    "        thm_state = pycdfpp.load(file)\n",
    "        epoch_dt64 = thm_state[\n",
    "            f\"time\"\n",
    "        ].values  #  CATDESC: \"thm_state_time, UTC, in seconds since 01-Jan-1970 00:00:00\"\n",
    "        thm_pos_sse_Xs.append(thm_state[f\"th{probe}_pos_sse\"].values[:, 0])\n",
    "        thm_pos_gse_Xs.append(thm_state[f\"th{probe}_pos_gse\"].values[:, 0])\n",
    "        thm_state_times.append(epoch_dt64)\n",
    "\n",
    "    thm_pos_sse_X = np.concatenate(thm_pos_sse_Xs)\n",
    "    thm_pos_gse_X = np.concatenate(thm_pos_gse_Xs)\n",
    "    thm_state_time = np.concatenate(thm_state_times)\n",
    "\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"thm_state_time\": thm_state_time,\n",
    "            \"thm_pos_gse_X\": thm_pos_gse_X,\n",
    "            \"thm_pos_sse_X\": thm_pos_sse_X,\n",
    "        }\n",
    "    ).with_columns(\n",
    "        pl.from_epoch(pl.col(\"thm_state_time\"), time_unit=\"s\")\n",
    "    ).write_parquet(\n",
    "        file_name\n",
    "    )\n",
    "\n",
    "    return file_name\n",
    "\n",
    "\n",
    "def convert_thm_fgm_to_parquet(probe, trange):\n",
    "    file_name = f\"./data/th{probe}_fgm.parquet\"\n",
    "    if os.path.exists(file_name):\n",
    "        return file_name\n",
    "\n",
    "    start = trange.start.to_string()\n",
    "    end = trange.end.to_string()\n",
    "    \n",
    "    files = pyspedas.themis.fgm(\n",
    "        probe=probe,\n",
    "        trange=[start, end],\n",
    "        downloadonly=True,\n",
    "        no_update=True,\n",
    "    )\n",
    "\n",
    "    thm_fgl_gses = []\n",
    "    thm_fgl_btotals = []\n",
    "    thm_fgl_times = []\n",
    "\n",
    "    for file in files:\n",
    "        cdf = pycdfpp.load(file)\n",
    "        thm_fgl_gses.append(cdf[f\"th{probe}_fgl_gse\"].values)\n",
    "        thm_fgl_btotals.append(cdf[f\"th{probe}_fgl_btotal\"].values)\n",
    "        thm_fgl_times.append(cdf[f\"th{probe}_fgl_time\"].values)\n",
    "\n",
    "    thm_fgl_gse = np.concatenate(thm_fgl_gses)\n",
    "    thm_fgl_btotal = np.concatenate(thm_fgl_btotals)\n",
    "    thm_fgl_time = np.concatenate(thm_fgl_times)\n",
    "\n",
    "    pl.DataFrame(\n",
    "        {\n",
    "            \"time\": thm_fgl_time,\n",
    "            \"BX\": thm_fgl_gse[:,0],\n",
    "            \"BY\": thm_fgl_gse[:,1],\n",
    "            \"BZ\": thm_fgl_gse[:,2],\n",
    "            \"B\": thm_fgl_btotal,\n",
    "        }\n",
    "    ).with_columns(\n",
    "        pl.from_epoch(pl.col(\"thm_fgl_time\"), time_unit=\"s\"),\n",
    "    ).write_parquet(   \n",
    "        file_name\n",
    "    )\n",
    "    \n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "convert_thm_state_to_parquet(probe, trange)\n",
    "convert_thm_fgm_to_parquet(probe, trange)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
