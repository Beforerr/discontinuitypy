[
  {
    "objectID": "20_datasets.html#datasets",
    "href": "20_datasets.html#datasets",
    "title": "Datasets",
    "section": "Datasets",
    "text": "Datasets\nFoundational Dataset Class\n\nsource\n\nIDsDataset\n\n IDsDataset (sat_id:str, tau:datetime.timedelta,\n             ts:datetime.timedelta=datetime.timedelta(seconds=1),\n             candidates:polars.dataframe.frame.DataFrame|None=None,\n             data:polars.lazyframe.frame.LazyFrame|None=None,\n             bcols:list[str]=['B_x', 'B_y', 'B_z'], **extra_data:Any)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\nExtended Dataset Class with support for kedro\n\nsource\n\n\ncIDsDataset\n\n cIDsDataset (sat_id:str, tau:datetime.timedelta,\n              ts:datetime.timedelta=datetime.timedelta(seconds=1),\n              candidates:polars.dataframe.frame.DataFrame|None=None,\n              data:polars.lazyframe.frame.LazyFrame|None=None,\n              bcols:list[str]=['B_x', 'B_y', 'B_z'],\n              catalog:kedro.io.data_catalog.DataCatalog,\n              or_df:polars.dataframe.frame.DataFrame|None=None,\n              or_df_normalized:polars.dataframe.frame.DataFrame|None=None,\n              **data_)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "20_datasets.html#candidate-class",
    "href": "20_datasets.html#candidate-class",
    "title": "Datasets",
    "section": "Candidate class",
    "text": "Candidate class\n\nsource\n\nCandidateID\n\n CandidateID (time, df:polars.dataframe.frame.DataFrame)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nCode\nfrom ids_finder.utils.basic import df2ts, pmap\nfrom fastcore.utils import *\n\n\n\n\nCode\ndef plot_candidate(candidate, mag_data: pl.LazyFrame, b_cols = ['BX', 'BY', 'BZ']):\n    temp_tstart = candidate[\"tstart\"]\n    tmep_tstop = candidate[\"tstop\"]\n    tau = tmep_tstop - temp_tstart\n\n    temp_mag_data = (\n        mag_data.filter(pl.col(\"time\").is_between(temp_tstart - tau, tmep_tstop + tau))\n        .with_columns(pl.col(\"time\").dt.cast_time_unit(\"ns\"))\n        .collect()\n    )\n    \n    sat_fgm = df2ts(temp_mag_data, b_cols)\n    plot_candidate_xr(candidate, sat_fgm, tau)\n\n\n\n\nCode\nn = 3\n# list(sta_candidates_1s.sample(n).iter_rows(named=True) | pmap(plot_candidate, mag_data=sta_mag))\n\ncandidates = jno_candidates_1s.sample(n)\nlist(candidates.iter_rows(named=True) | pmap(plot_candidate, mag_data=jno_mag))",
    "crumbs": [
      "Datasets"
    ]
  },
  {
    "objectID": "00_ids_finder.html",
    "href": "00_ids_finder.html",
    "title": "Finding magnetic discontinuities",
    "section": "",
    "text": "It can be divided into two parts:",
    "crumbs": [
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#install",
    "href": "00_ids_finder.html#install",
    "title": "Finding magnetic discontinuities",
    "section": "Install",
    "text": "Install\npip install ids-finder",
    "crumbs": [
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#how-to-use",
    "href": "00_ids_finder.html#how-to-use",
    "title": "Finding magnetic discontinuities",
    "section": "How to use",
    "text": "How to use\nImport the package\nfrom ids_finder.utils.basic import *\nfrom ids_finder.core import *",
    "crumbs": [
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#processing-stages",
    "href": "00_ids_finder.html#processing-stages",
    "title": "Finding magnetic discontinuities",
    "section": "Processing Stages",
    "text": "Processing Stages\n\nSmoothing\nInterpolating",
    "crumbs": [
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#processing-the-whole-dataset",
    "href": "00_ids_finder.html#processing-the-whole-dataset",
    "title": "Finding magnetic discontinuities",
    "section": "Processing the whole dataset",
    "text": "Processing the whole dataset\nNotes that the candidates only require a small portion of the data so we can compress the data to speed up the processing.\n\nsource\n\ncompress_data_by_cands\n\n compress_data_by_cands (data:polars.dataframe.frame.DataFrame,\n                         candidates:polars.dataframe.frame.DataFrame)\n\nCompress the data for parallel processing\n\nsource\n\n\nextract_features\n\n extract_features (partitioned_input:dict[str,typing.Callable[...,polars.l\n                   azyframe.frame.LazyFrame]], tau:float, ts:float,\n                   bcols:list[str]=['B_x', 'B_y', 'B_z'])\n\nwrapper function for partitioned input\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npartitioned_input\ndict\n\n\n\n\ntau\nfloat\n\nin seconds, yaml input\n\n\nts\nfloat\n\nin seconds, yaml input\n\n\nbcols\nlist\n[‘B_x’, ‘B_y’, ‘B_z’]\n\n\n\nReturns\nDataFrame\n\n\n\n\n\n\nsource\n\n\nids_finder\n\n ids_finder (ldata:polars.lazyframe.frame.LazyFrame,\n             tau:datetime.timedelta, ts:datetime.timedelta, bcols)",
    "crumbs": [
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#conventions",
    "href": "00_ids_finder.html#conventions",
    "title": "Finding magnetic discontinuities",
    "section": "Conventions",
    "text": "Conventions\nAs we are dealing with multiple spacecraft, we need to be careful about naming conventions. Here are the conventions we use in this project.\n\nsat_id: name of the spacecraft. We also use abbreviation, for example\n\nsta for STEREO-A\nthb for ARTEMIS-B\n\nsat_state: state data of the spacecraft\nb_vl: maximum variance vector of the magnetic field, (major eigenvector)\n\nData Level\n\nl0: unprocessed\nl1: cleaned data, fill null value, add useful columns\nl2: time-averaged data\n\n\nColumns naming conventions\n\nradial_distance: radial distance of the spacecraft, in units of \\(AU\\)\nplasma_speed: solar wind plasma speed, in units of \\(km/s\\)\nsw_elevation: solar wind elevation angle, in units of \\(\\degree\\)\nsw_azimuth: solar wind azimuth angle, in units of \\(\\degree\\)\nv_{x,y,z} or sw_vel_{X,Y,Z}: solar wind plasma speed in the ANY coordinate system, in units of \\(km/s\\)\n\nsw_vel_{r,t,n}: solar wind plasma speed in the RTN coordinate system, in units of \\(km/s\\)\nsw_vel_gse_{x,y,z}: solar wind plasma speed in the GSE coordinate system, in units of \\(km/s\\)\nsw_vel_lmn_{x,y,z}: solar wind plasma speed in the LMN coordinate system, in units of \\(km/s\\)\n\nv_l or sw_vel_l: abbreviation for sw_vel_lmn_1\nv_mn or sw_vel_mn (deprecated)\n\n\nplasma_density: plasma density, in units of \\(1/cm^{3}\\)\nplasma_temperature: plasma temperature, in units of \\(K\\)\nB_{x,y,z}: magnetic field in ANY coordinate system\n\nb_rtn_{x,y,z} or b_{r,t,n}: magnetic field in the RTN coordinate system\nb_gse_{x,y,z}: magnetic field in the GSE coordinate system\n\nB_mag: magnetic field magnitude\nVl_{x,y,z} or b_vecL_{X,Y,Z}: maxium variance vector of the magnetic field in ANY coordinate system\n\nb_vecL_{r,t,n}: maxium variance vector of the magnetic field in the RTN coordinate system\n\nmodel_b_{r,t,n}: modelled magnetic field in the RTN coordinate system\nstate : 1 for solar wind, 0 for non-solar wind\nL_mn{_norm}: thickness of the current sheet in MN direction, in units of \\(km\\)\nj0{_norm}: current density, in units of \\(nA/m^2\\)\n\nNotes: we recommend use unique names for each variable, for example, plasma_speed instead of speed. Because it is easier to search and replace the variable names in the code whenever necessary.\nFor the unit, by default we use\n\nlength : \\(km\\)\ntime : \\(s\\)\nmagnetic field : \\(nT\\)\ncurrent : \\(nA/m^2\\)",
    "crumbs": [
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#test",
    "href": "00_ids_finder.html#test",
    "title": "Finding magnetic discontinuities",
    "section": "Test",
    "text": "Test\n\nTest feature engineering\n\n\nCode\n# from tsflex.features import MultipleFeatureDescriptors, FeatureCollection\n\n# from tsflex.features.integrations import catch22_wrapper\n# from pycatch22 import catch22_all\n\n\n\n\nCode\n# tau_pd = pd.Timedelta(tau)\n\n# catch22_feats = MultipleFeatureDescriptors(\n#     functions=catch22_wrapper(catch22_all),\n#     series_names=bcols,  # list of signal names\n#     windows = tau_pd, strides=tau_pd/2,\n# )\n\n# fc = FeatureCollection(catch22_feats)\n# features = fc.calculate(data, return_df=True)  # calculate the features on your data\n\n\n\n\nCode\n# features_pl = pl.DataFrame(features.reset_index()).sort('time')\n# df = candidates_pl.join_asof(features_pl, on='time').to_pandas()\n\n\n\n\nCode\n# profile = ProfileReport(df, title=\"JUNO Candidates Report\")\n# profile.to_file(\"jno.html\")\n\n\n\n\nBenchmark",
    "crumbs": [
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#notes",
    "href": "00_ids_finder.html#notes",
    "title": "Finding magnetic discontinuities",
    "section": "Notes",
    "text": "Notes\n\nTODOs\n\nFeature engineering\nFeature selection",
    "crumbs": [
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#obsolete-codes",
    "href": "00_ids_finder.html#obsolete-codes",
    "title": "Finding magnetic discontinuities",
    "section": "Obsolete codes",
    "text": "Obsolete codes",
    "crumbs": [
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "utils/20_pds.html",
    "href": "utils/20_pds.html",
    "title": "Planetary Data System (PDS)",
    "section": "",
    "text": "References:",
    "crumbs": [
      "Utils",
      "Planetary Data System (PDS)"
    ]
  },
  {
    "objectID": "utils/20_pds.html#pds-planetary-plasma-interactions-node",
    "href": "utils/20_pds.html#pds-planetary-plasma-interactions-node",
    "title": "Planetary Data System (PDS)",
    "section": "PDS Planetary Plasma Interactions Node",
    "text": "PDS Planetary Plasma Interactions Node\nSee configuration\n\nsource\n\npds_download\n\n pds_download (mission, instrument, dataset, coord, datatype, path,\n               sat=None)\n\nDownloading file from PDS server\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmission\n\n\nplanetary missions\n\n\ninstrument\n\n\n\n\n\ndataset\n\n\nDataset may correspond to a phase of the mission\n\n\ncoord\n\n\n\n\n\ndatatype\n\n\n\n\n\npath\n\n\n\n\n\nsat\nNoneType\nNone\nsatellite name, for missions with multiple satellites\n\n\nReturns\nlist",
    "crumbs": [
      "Utils",
      "Planetary Data System (PDS)"
    ]
  },
  {
    "objectID": "utils/01_plotting.html",
    "href": "utils/01_plotting.html",
    "title": "MVA plotting",
    "section": "",
    "text": "source\n\nsavefig\n\n savefig (name, **kwargs)\n\n\nsource\n\n\nsetup_mva_plot\n\n setup_mva_plot (data:xarray.core.dataarray.DataArray,\n                 tstart:datetime.datetime, tstop:datetime.datetime,\n                 mva_tstart:datetime.datetime=None,\n                 mva_tstop:datetime.datetime=None)\n\n\nsource\n\n\ntime_stamp\n\n time_stamp (ts)\n\nReturn POSIX timestamp as float.\n\n\nCode\ndef format_candidate_title(candidate: dict):\n    format_float = (\n        lambda x: rf\"$\\bf {x:.2f} $\" if isinstance(x, (float, int)) else rf\"$\\bf {x} $\"\n    )\n\n    base_line = rf'$\\bf {candidate.get(\"type\", \"N/A\")} $ candidate (time: {candidate.get(\"time\", \"N/A\")}) with index '\n    index_line = rf'i1: {format_float(candidate.get(\"index_std\", \"N/A\"))}, i2: {format_float(candidate.get(\"index_fluctuation\", \"N/A\"))}, i3: {format_float(candidate.get(\"index_diff\", \"N/A\"))}'\n    info_line = rf'$B_n/B$: {format_float(candidate.get(\"BnOverB\", \"N/A\"))}, $dB/B$: {format_float(candidate.get(\"dBOverB\", \"N/A\"))}, $(dB/B)_{{max}}$: {format_float(candidate.get(\"dBOverB_max\", \"N/A\"))},  $Q_{{mva}}$: {format_float(candidate.get(\"Q_mva\", \"N/A\"))}'\n    title = rf\"\"\"{base_line}\n    {index_line}\n    {info_line}\"\"\"\n    return title\n\n\n\nsource\n\n\nplot_candidate\n\n plot_candidate (candidate:dict, sat_fgm:xarray.core.dataarray.DataArray,\n                 **kwargs)\n\n\n\nCode\ndef plot_candidates(\n    candidates: pd.DataFrame, candidate_type=None, num=4, plot_func=plot_candidate\n):\n    \"\"\"Plot a set of candidates.\n\n    Parameters:\n    - candidates (pd.DataFrame): DataFrame containing the candidates.\n    - candidate_type (str, optional): Filter candidates based on a specific type.\n    - num (int): Number of candidates to plot, selected randomly.\n    - plot_func (callable): Function used to plot an individual candidate.\n\n    \"\"\"\n\n    # Filter by candidate_type if provided\n    \n    candidates = get_candidates(candidates, candidate_type, num)\n\n    # Plot each candidate using the provided plotting function\n    for _, candidate in candidates.iterrows():\n        plot_func(candidate)\n\n\n\n\nCode\ndef plot_basic(\n    data: xr.DataArray, \n    tstart: datetime, \n    tstop: datetime,\n    tau: timedelta, \n    mva_tstart=None, mva_tstop=None, neighbor: int = 1\n):\n    if mva_tstart is None:\n        mva_tstart = tstart\n    if mva_tstop is None:\n        mva_tstop = tstop\n\n    mva_b = data.sel(time=slice(mva_tstart, mva_tstop))\n    store_data(\"fgm\", data={\"x\": mva_b.time, \"y\": mva_b})\n    minvar_matrix_make(\"fgm\")  # get the MVA matrix\n\n    temp_tstart = tstart - neighbor * tau\n    temp_tstop = tstop + neighbor * tau\n\n    temp_b = data.sel(time=slice(temp_tstart, temp_tstop))\n    store_data(\"fgm\", data={\"x\": temp_b.time, \"y\": temp_b})\n    temp_btotal = calc_vec_mag(temp_b)\n    store_data(\"fgm_btotal\", data={\"x\": temp_btotal.time, \"y\": temp_btotal})\n\n    tvector_rotate(\"fgm_mva_mat\", \"fgm\")\n    split_vec(\"fgm_rot\")\n    pytplot.data_quants[\"fgm_btotal\"][\"time\"] = pytplot.data_quants[\"fgm_rot\"][\n        \"time\"\n    ]  # NOTE: whenever using `get_data`, we may lose precision in the time values. This is a workaround.\n    join_vec(\n        [\n            \"fgm_rot_x\",\n            \"fgm_rot_y\",\n            \"fgm_rot_z\",\n            \"fgm_btotal\",\n        ],\n        new_tvar=\"fgm_all\",\n    )\n\n    options(\"fgm\", \"legend_names\", [r\"$B_x$\", r\"$B_y$\", r\"$B_z$\"])\n    options(\"fgm_all\", \"legend_names\", [r\"$B_l$\", r\"$B_m$\", r\"$B_n$\", r\"$B_{total}$\"])\n    options(\"fgm_all\", \"ysubtitle\", \"[nT LMN]\")\n    tstart_ts = time_stamp(tstart)\n    tstop_ts = time_stamp(tstop)\n    # .replace(tzinfo=ZoneInfo('UTC')).timestamp()\n    highlight([\"fgm\", \"fgm_all\"], [tstart_ts, tstop_ts])\n    \n    degap(\"fgm\")\n    degap(\"fgm_all\")",
    "crumbs": [
      "Utils",
      "MVA plotting"
    ]
  },
  {
    "objectID": "utils/basic.html",
    "href": "utils/basic.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n pmap (func, *args, **kwargs)\n\nmap with partial",
    "crumbs": [
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "utils/basic.html#utilities-functions",
    "href": "utils/basic.html#utilities-functions",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n pmap (func, *args, **kwargs)\n\nmap with partial",
    "crumbs": [
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "utils/basic.html#utils",
    "href": "utils/basic.html#utils",
    "title": "Utils",
    "section": "Utils",
    "text": "Utils",
    "crumbs": [
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "utils/basic.html#configurations",
    "href": "utils/basic.html#configurations",
    "title": "Utils",
    "section": "Configurations",
    "text": "Configurations\n\nsource\n\nDataConfig\n\n DataConfig (sat_id:str=None, start:datetime.datetime=None,\n             end:datetime.datetime=None, ts:datetime.timedelta=None,\n             coord:str=None)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.",
    "crumbs": [
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "utils/basic.html#polars",
    "href": "utils/basic.html#polars",
    "title": "Utils",
    "section": "Polars",
    "text": "Polars\n\nsource\n\nfilter_tranges_df\n\n filter_tranges_df (df:polars.dataframe.frame.DataFrame,\n                    tranges:Tuple[list,list], time_col:str='time')\n\n\nFilter data by time ranges\n\n\nsource\n\n\nfilter_tranges\n\n filter_tranges (time:polars.series.series.Series,\n                 tranges:Tuple[list,list])\n\n\nFilter data by time ranges, return the indices of the time that are in the time ranges (left inclusive, right exclusive)\n\n\n\n\nDataFrame.plot\n\n DataFrame.plot (*args, **kwargs)\n\n\nsource\n\n\npl_norm\n\n pl_norm (columns, *more_columns)\n\nComputes the square root of the sum of squares for the given columns.\nArgs: *columns (str): Names of the columns.\nReturns: pl.Expr: Expression representing the square root of the sum of squares.\n\nPartition the dataset by time\n\nsource\n\n\n\npartition_data_by_time\n\n partition_data_by_time\n                         (df:polars.lazyframe.frame.LazyFrame|polars.dataf\n                         rame.frame.DataFrame, method)\n\nPartition the dataset by time\nArgs: df: Input DataFrame. method: The method to partition the data.\nReturns: Partitioned DataFrame.\n\nsource\n\n\npartition_data_by_year_month\n\n partition_data_by_year_month (df:polars.dataframe.frame.DataFrame)\n\nPartition the dataset by year\nArgs: df: Input DataFrame.\nReturns: Partitioned DataFrame.\n\nsource\n\n\npartition_data_by_year\n\n partition_data_by_year (df:polars.dataframe.frame.DataFrame)\n\nPartition the dataset by year\nArgs: df: Input DataFrame.\nReturns: Partitioned DataFrame.\n\nsource\n\n\npartition_data_by_ts\n\n partition_data_by_ts (df:polars.dataframe.frame.DataFrame,\n                       ts:datetime.timedelta)\n\nPartition the dataset by time\nArgs: df: Input DataFrame. ts: Time interval.\nReturns: Partitioned DataFrame.\n\nsource\n\n\nconcat_partitions\n\n concat_partitions (partitioned_input:Dict[str,Callable])\n\nConcatenate input partitions into one DataFrame.\nArgs: partitioned_input: A dictionary with partition ids as keys and load functions as values.\n\nsource\n\n\nconcat_df\n\n concat_df (dfs:list[typing.Union[polars.dataframe.frame.DataFrame,polars.\n            lazyframe.frame.LazyFrame,pandas.core.frame.DataFrame]])\n\nConcatenate a list of DataFrames into one DataFrame.\n\nResample data\n\nsource\n\n\n\nformat_timedelta\n\n format_timedelta (time)\n\nFormat timedelta to timedelta\n\nsource\n\n\nresample\n\n resample\n           (df:polars.lazyframe.frame.LazyFrame|polars.dataframe.frame.Dat\n           aFrame, every:datetime.timedelta,\n           period:datetime.timedelta=None, offset:datetime.timedelta=None,\n           shift:datetime.timedelta=None, time_column='time')\n\nResample the DataFrame\n\nsource\n\n\nresample\n\n resample\n           (df:polars.lazyframe.frame.LazyFrame|polars.dataframe.frame.Dat\n           aFrame, every:datetime.timedelta,\n           period:datetime.timedelta=None, offset:datetime.timedelta=None,\n           shift:datetime.timedelta=None, time_column='time')\n\nResample the DataFrame\n\nsource\n\n\nresample\n\n resample\n           (df:polars.lazyframe.frame.LazyFrame|polars.dataframe.frame.Dat\n           aFrame, every:datetime.timedelta,\n           period:datetime.timedelta=None, offset:datetime.timedelta=None,\n           shift:datetime.timedelta=None, time_column='time')\n\nResample the DataFrame\n\nsource\n\n\nget_memory_usage\n\n get_memory_usage (data)\n\n\nsource\n\n\ncalc_vec_mag\n\n calc_vec_mag (vec)\n\n\nsource\n\n\ndf2ts\n\n df2ts (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFr\n        ame,polars.lazyframe.frame.LazyFrame], cols, attrs=None,\n        name=None)\n\n\nsource\n\n\ncheck_fgm\n\n check_fgm (vec:xarray.core.dataarray.DataArray)",
    "crumbs": [
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "utils/02_analysis_utils.html",
    "href": "utils/02_analysis_utils.html",
    "title": "Utils",
    "section": "",
    "text": "source",
    "crumbs": [
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "utils/02_analysis_utils.html#common-codes-used-across-notebooks",
    "href": "utils/02_analysis_utils.html#common-codes-used-across-notebooks",
    "title": "Utils",
    "section": "Common codes used across notebooks",
    "text": "Common codes used across notebooks",
    "crumbs": [
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "pipelines/2_data_mag.html",
    "href": "pipelines/2_data_mag.html",
    "title": "Magnetic field data pipeline",
    "section": "",
    "text": "In additional to the general data pipeline, we add feature extraction to the pipeline…\nThe product of this pipeline is a data set of interesting magnetic field events.",
    "crumbs": [
      "Pipelines",
      "Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "pipelines/2_data_mag.html#processing-data",
    "href": "pipelines/2_data_mag.html#processing-data",
    "title": "Magnetic field data pipeline",
    "section": "Processing data",
    "text": "Processing data\n\nsource\n\nprocess_data\n\n process_data\n               (raw_data:Dict[str,Callable[...,polars.lazyframe.frame.Lazy\n               Frame]], ts=None, partition_time_method='year')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nDict\n\n\n\n\nts\nNoneType\nNone\ntime resolution, in seconds, optional\n\n\npartition_time_method\nstr\nyear\npartition time resolution, in seconds, optional\n\n\n\n\nsource\n\n\ncreate_pipeline_template\n\n create_pipeline_template (sat_id:str, source:str,\n                           extract_features_fn:Optional[Callable]=&lt;functio\n                           n extract_features&gt;,\n                           params:Optional[dict]={'experiments': [{'name':\n                           'discontinuities spatial evolution'}, {'name':\n                           'time resolution effect', 'datasets': [{'name':\n                           'wind_ts_high', 'start': '2012-01-01', 'end':\n                           '2012-12-31', 'tau': 60, 'Wind': {'MAG':\n                           {'datatype': 'h4-rtn', 'coords': 'rtn',\n                           'bcols': ['BRTN_0', 'BRTN_1', 'BRTN_2']}},\n                           'output_path': 'data/04_feature/Wind_ts_high'},\n                           {'name': 'wind_ts_low', 'start': '2012-01-01',\n                           'end': '2012-12-31', 'MAG': {'time_resolution':\n                           1, 'data_source': 'Wind'}, 'output_path':\n                           'data/04_feature/Wind_ts_low'}]}], 'JNO':\n                           {'MAG': {'datatype': '1SEC', 'time_resolution':\n                           1, 'bcols': ['BX SE', 'BY SE', 'BZ SE'],\n                           'coords': 'rtn'}, 'STATE': {'datatype':\n                           'hourly', 'time_resolution': 3600}}, 'STEREO':\n                           {'MAG': {'datatype': '8hz', 'time_resolution':\n                           1, 'coords': 'rtn', 'bcols': ['BFIELD_0',\n                           'BFIELD_1', 'BFIELD_2']}, 'STATE': {'datatype':\n                           'hourly', 'time_resolution': 3600}}, 'STA':\n                           {'MAG': {'datatype': '8hz', 'time_resolution':\n                           1, 'coords': 'rtn', 'bcols': ['BFIELD_0',\n                           'BFIELD_1', 'BFIELD_2']}, 'STATE': {'datatype':\n                           'hourly', 'time_resolution': 3600}}, 'STB':\n                           {'MAG': {'datatype': '8hz', 'time_resolution':\n                           1, 'coords': 'rtn', 'bcols': ['BFIELD_0',\n                           'BFIELD_1', 'BFIELD_2']}, 'STATE': {'datatype':\n                           'hourly', 'time_resolution': 3600}}, 'pds-\n                           download-url': 'https://pds-ppi.igpp.ucla.edu/d\n                           itdos/download?id=pds://PPI', 'Juno': {'FGM':\n                           {'CRUISE': {'datatypes': ['1MIN', '1SEC'],\n                           'url_format': 'JNO-SS-3-FGM-\n                           CAL-V1.0/DATA/CRUISE'}, 'JUPYTER':\n                           {'datatypes': ['1MIN', '1SEC'], 'url_format':\n                           'JNO-J-3-FGM-CAL-V1.0/DATA/JUPITER'}}}, 'Wind':\n                           {'MAG': {'start': '2016-01-01', 'end':\n                           '2016-06-30', 'datatype': 'h4-rtn', 'tau': 60,\n                           'time_resolution': 0.1, 'coords': 'rtn',\n                           'bcols': ['BRTN_0', 'BRTN_1', 'BRTN_2']},\n                           'STATE': {'time_resolution': 3600}}, 'tau': 60,\n                           'detection': {'index_std_threshold': 2,\n                           'index_fluc_threshold': 1,\n                           'index_diff_threshold': 0.1}, 'jno_start_date':\n                           '2011-08-25', 'jno_end_date': '2016-06-30',\n                           'mission': {'source': {'time_resolution': 1}},\n                           'THEMIS': {'MAG': {'datatype': 'fgl',\n                           'time_resolution': 1, 'coords': 'gse', 'bcols':\n                           ['B_x', 'B_y', 'B_z']}, 'STATE': {'datatype':\n                           'hourly', 'time_resolution': 3600}}, 'THA':\n                           {'MAG': {'datatype': 'fgl', 'time_resolution':\n                           1, 'coords': 'gse', 'bcols': ['B_x', 'B_y',\n                           'B_z']}, 'STATE': {'datatype': 'hourly',\n                           'time_resolution': 3600}}, 'THB': {'MAG':\n                           {'datatype': 'fgl', 'time_resolution': 1,\n                           'coords': 'gse', 'bcols': ['B_x', 'B_y',\n                           'B_z']}, 'STATE': {'datatype': 'hourly',\n                           'time_resolution': 3600}}, 'OMNI': {'LowRes':\n                           {'datatype': 'hourly', 'time_resolution':\n                           3600}}, 'omni_vars': {'N': {'COLNAME':\n                           'plasma_density', 'FIELDNAM': 'Ion density',\n                           'UNITS': 'Per cc'}, 'T': {'COLNAME':\n                           'plasma_temperature', 'FIELDNAM': 'Plasma\n                           temperature', 'UNITS': 'K'}, 'V': {'COLNAME':\n                           'plasma_speed', 'FIELDNAM': 'Flow speed',\n                           'UNITS': 'km/s'}, 'THETA-V': {'COLNAME':\n                           'sw_vel_theta', 'FIELDNAM': 'Flow latitude',\n                           'UNITS': 'Deg'}, 'PHI-V': {'COLNAME':\n                           'sw_vel_phi', 'FIELDNAM': 'Flow longitude',\n                           'UNITS': 'Deg'}, 'BX_GSE': {'COLNAME':\n                           'B_background_x'}, 'BY_GSE': {'COLNAME':\n                           'B_background_y'}, 'BZ_GSE': {'COLNAME':\n                           'B_background_z'}}}, **kwargs)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\nstr\n\nsatellite id, used for namespace\n\n\nsource\nstr\n\nsource data, like “mag” or “plasma”\n\n\nextract_features_fn\nOptional\nextract_features\n\n\n\nparams\nOptional\n{‘experiments’: [{‘name’: ‘discontinuities spatial evolution’}, {‘name’: ‘time resolution effect’, ‘datasets’: [{‘name’: ‘wind_ts_high’, ‘start’: ‘2012-01-01’, ‘end’: ‘2012-12-31’, ‘tau’: 60, ‘Wind’: {‘MAG’: {‘datatype’: ‘h4-rtn’, ‘coords’: ‘rtn’, ‘bcols’: [‘BRTN_0’, ‘BRTN_1’, ‘BRTN_2’]}}, ‘output_path’: ‘data/04_feature/Wind_ts_high’}, {‘name’: ‘wind_ts_low’, ‘start’: ‘2012-01-01’, ‘end’: ‘2012-12-31’, ‘MAG’: {‘time_resolution’: 1, ‘data_source’: ‘Wind’}, ‘output_path’: ‘data/04_feature/Wind_ts_low’}]}], ‘JNO’: {‘MAG’: {‘datatype’: ‘1SEC’, ‘time_resolution’: 1, ‘bcols’: [‘BX SE’, ‘BY SE’, ‘BZ SE’], ‘coords’: ‘rtn’}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘STEREO’: {‘MAG’: {‘datatype’: ‘8hz’, ‘time_resolution’: 1, ‘coords’: ‘rtn’, ‘bcols’: [‘BFIELD_0’, ‘BFIELD_1’, ‘BFIELD_2’]}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘STA’: {‘MAG’: {‘datatype’: ‘8hz’, ‘time_resolution’: 1, ‘coords’: ‘rtn’, ‘bcols’: [‘BFIELD_0’, ‘BFIELD_1’, ‘BFIELD_2’]}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘STB’: {‘MAG’: {‘datatype’: ‘8hz’, ‘time_resolution’: 1, ‘coords’: ‘rtn’, ‘bcols’: [‘BFIELD_0’, ‘BFIELD_1’, ‘BFIELD_2’]}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘pds-download-url’: ‘https://pds-ppi.igpp.ucla.edu/ditdos/download?id=pds://PPI’, ‘Juno’: {‘FGM’: {‘CRUISE’: {‘datatypes’: [‘1MIN’, ‘1SEC’], ‘url_format’: ‘JNO-SS-3-FGM-CAL-V1.0/DATA/CRUISE’}, ‘JUPYTER’: {‘datatypes’: [‘1MIN’, ‘1SEC’], ‘url_format’: ‘JNO-J-3-FGM-CAL-V1.0/DATA/JUPITER’}}}, ‘Wind’: {‘MAG’: {‘start’: ‘2016-01-01’, ‘end’: ‘2016-06-30’, ‘datatype’: ‘h4-rtn’, ‘tau’: 60, ‘time_resolution’: 0.1, ‘coords’: ‘rtn’, ‘bcols’: [‘BRTN_0’, ‘BRTN_1’, ‘BRTN_2’]}, ‘STATE’: {‘time_resolution’: 3600}}, ‘tau’: 60, ‘detection’: {‘index_std_threshold’: 2, ‘index_fluc_threshold’: 1, ‘index_diff_threshold’: 0.1}, ‘jno_start_date’: ‘2011-08-25’, ‘jno_end_date’: ‘2016-06-30’, ‘mission’: {‘source’: {‘time_resolution’: 1}}, ‘THEMIS’: {‘MAG’: {‘datatype’: ‘fgl’, ‘time_resolution’: 1, ‘coords’: ‘gse’, ‘bcols’: [‘B_x’, ‘B_y’, ‘B_z’]}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘THA’: {‘MAG’: {‘datatype’: ‘fgl’, ‘time_resolution’: 1, ‘coords’: ‘gse’, ‘bcols’: [‘B_x’, ‘B_y’, ‘B_z’]}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘THB’: {‘MAG’: {‘datatype’: ‘fgl’, ‘time_resolution’: 1, ‘coords’: ‘gse’, ‘bcols’: [‘B_x’, ‘B_y’, ‘B_z’]}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘OMNI’: {‘LowRes’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘omni_vars’: {‘N’: {‘COLNAME’: ‘plasma_density’, ‘FIELDNAM’: ‘Ion density’, ‘UNITS’: ‘Per cc’}, ‘T’: {‘COLNAME’: ‘plasma_temperature’, ‘FIELDNAM’: ‘Plasma temperature’, ‘UNITS’: ‘K’}, ‘V’: {‘COLNAME’: ‘plasma_speed’, ‘FIELDNAM’: ‘Flow speed’, ‘UNITS’: ‘km/s’}, ‘THETA-V’: {‘COLNAME’: ‘sw_vel_theta’, ‘FIELDNAM’: ‘Flow latitude’, ‘UNITS’: ‘Deg’}, ‘PHI-V’: {‘COLNAME’: ‘sw_vel_phi’, ‘FIELDNAM’: ‘Flow longitude’, ‘UNITS’: ‘Deg’}, ‘BX_GSE’: {‘COLNAME’: ‘B_background_x’}, ‘BY_GSE’: {‘COLNAME’: ‘B_background_y’}, ‘BZ_GSE’: {‘COLNAME’: ‘B_background_z’}}}\n\n\n\nkwargs\n\n\n\n\n\nReturns\nPipeline\n\n\n\n\n\n\nsource\n\n\ncreate_extra_pipeline\n\n create_extra_pipeline (sat_id:str, source:str,\n                        extract_features_fn:Optional[Callable]=&lt;function\n                        extract_features&gt;,\n                        params:Optional[dict]={'experiments': [{'name':\n                        'discontinuities spatial evolution'}, {'name':\n                        'time resolution effect', 'datasets': [{'name':\n                        'wind_ts_high', 'start': '2012-01-01', 'end':\n                        '2012-12-31', 'tau': 60, 'Wind': {'MAG':\n                        {'datatype': 'h4-rtn', 'coords': 'rtn', 'bcols':\n                        ['BRTN_0', 'BRTN_1', 'BRTN_2']}}, 'output_path':\n                        'data/04_feature/Wind_ts_high'}, {'name':\n                        'wind_ts_low', 'start': '2012-01-01', 'end':\n                        '2012-12-31', 'MAG': {'time_resolution': 1,\n                        'data_source': 'Wind'}, 'output_path':\n                        'data/04_feature/Wind_ts_low'}]}], 'JNO': {'MAG':\n                        {'datatype': '1SEC', 'time_resolution': 1,\n                        'bcols': ['BX SE', 'BY SE', 'BZ SE'], 'coords':\n                        'rtn'}, 'STATE': {'datatype': 'hourly',\n                        'time_resolution': 3600}}, 'STEREO': {'MAG':\n                        {'datatype': '8hz', 'time_resolution': 1,\n                        'coords': 'rtn', 'bcols': ['BFIELD_0', 'BFIELD_1',\n                        'BFIELD_2']}, 'STATE': {'datatype': 'hourly',\n                        'time_resolution': 3600}}, 'STA': {'MAG':\n                        {'datatype': '8hz', 'time_resolution': 1,\n                        'coords': 'rtn', 'bcols': ['BFIELD_0', 'BFIELD_1',\n                        'BFIELD_2']}, 'STATE': {'datatype': 'hourly',\n                        'time_resolution': 3600}}, 'STB': {'MAG':\n                        {'datatype': '8hz', 'time_resolution': 1,\n                        'coords': 'rtn', 'bcols': ['BFIELD_0', 'BFIELD_1',\n                        'BFIELD_2']}, 'STATE': {'datatype': 'hourly',\n                        'time_resolution': 3600}}, 'pds-download-url':\n                        'https://pds-\n                        ppi.igpp.ucla.edu/ditdos/download?id=pds://PPI',\n                        'Juno': {'FGM': {'CRUISE': {'datatypes': ['1MIN',\n                        '1SEC'], 'url_format': 'JNO-SS-3-FGM-\n                        CAL-V1.0/DATA/CRUISE'}, 'JUPYTER': {'datatypes':\n                        ['1MIN', '1SEC'], 'url_format': 'JNO-J-3-FGM-\n                        CAL-V1.0/DATA/JUPITER'}}}, 'Wind': {'MAG':\n                        {'start': '2016-01-01', 'end': '2016-06-30',\n                        'datatype': 'h4-rtn', 'tau': 60,\n                        'time_resolution': 0.1, 'coords': 'rtn', 'bcols':\n                        ['BRTN_0', 'BRTN_1', 'BRTN_2']}, 'STATE':\n                        {'time_resolution': 3600}}, 'tau': 60,\n                        'detection': {'index_std_threshold': 2,\n                        'index_fluc_threshold': 1, 'index_diff_threshold':\n                        0.1}, 'jno_start_date': '2011-08-25',\n                        'jno_end_date': '2016-06-30', 'mission':\n                        {'source': {'time_resolution': 1}}, 'THEMIS':\n                        {'MAG': {'datatype': 'fgl', 'time_resolution': 1,\n                        'coords': 'gse', 'bcols': ['B_x', 'B_y', 'B_z']},\n                        'STATE': {'datatype': 'hourly', 'time_resolution':\n                        3600}}, 'THA': {'MAG': {'datatype': 'fgl',\n                        'time_resolution': 1, 'coords': 'gse', 'bcols':\n                        ['B_x', 'B_y', 'B_z']}, 'STATE': {'datatype':\n                        'hourly', 'time_resolution': 3600}}, 'THB':\n                        {'MAG': {'datatype': 'fgl', 'time_resolution': 1,\n                        'coords': 'gse', 'bcols': ['B_x', 'B_y', 'B_z']},\n                        'STATE': {'datatype': 'hourly', 'time_resolution':\n                        3600}}, 'OMNI': {'LowRes': {'datatype': 'hourly',\n                        'time_resolution': 3600}}, 'omni_vars': {'N':\n                        {'COLNAME': 'plasma_density', 'FIELDNAM': 'Ion\n                        density', 'UNITS': 'Per cc'}, 'T': {'COLNAME':\n                        'plasma_temperature', 'FIELDNAM': 'Plasma\n                        temperature', 'UNITS': 'K'}, 'V': {'COLNAME':\n                        'plasma_speed', 'FIELDNAM': 'Flow speed', 'UNITS':\n                        'km/s'}, 'THETA-V': {'COLNAME': 'sw_vel_theta',\n                        'FIELDNAM': 'Flow latitude', 'UNITS': 'Deg'},\n                        'PHI-V': {'COLNAME': 'sw_vel_phi', 'FIELDNAM':\n                        'Flow longitude', 'UNITS': 'Deg'}, 'BX_GSE':\n                        {'COLNAME': 'B_background_x'}, 'BY_GSE':\n                        {'COLNAME': 'B_background_y'}, 'BZ_GSE':\n                        {'COLNAME': 'B_background_z'}}}, **kwargs)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\nstr\n\nsatellite id, used for namespace\n\n\nsource\nstr\n\nsource data, like “mag” or “plasma”\n\n\nextract_features_fn\nOptional\nextract_features\n\n\n\nparams\nOptional\n{‘experiments’: [{‘name’: ‘discontinuities spatial evolution’}, {‘name’: ‘time resolution effect’, ‘datasets’: [{‘name’: ‘wind_ts_high’, ‘start’: ‘2012-01-01’, ‘end’: ‘2012-12-31’, ‘tau’: 60, ‘Wind’: {‘MAG’: {‘datatype’: ‘h4-rtn’, ‘coords’: ‘rtn’, ‘bcols’: [‘BRTN_0’, ‘BRTN_1’, ‘BRTN_2’]}}, ‘output_path’: ‘data/04_feature/Wind_ts_high’}, {‘name’: ‘wind_ts_low’, ‘start’: ‘2012-01-01’, ‘end’: ‘2012-12-31’, ‘MAG’: {‘time_resolution’: 1, ‘data_source’: ‘Wind’}, ‘output_path’: ‘data/04_feature/Wind_ts_low’}]}], ‘JNO’: {‘MAG’: {‘datatype’: ‘1SEC’, ‘time_resolution’: 1, ‘bcols’: [‘BX SE’, ‘BY SE’, ‘BZ SE’], ‘coords’: ‘rtn’}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘STEREO’: {‘MAG’: {‘datatype’: ‘8hz’, ‘time_resolution’: 1, ‘coords’: ‘rtn’, ‘bcols’: [‘BFIELD_0’, ‘BFIELD_1’, ‘BFIELD_2’]}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘STA’: {‘MAG’: {‘datatype’: ‘8hz’, ‘time_resolution’: 1, ‘coords’: ‘rtn’, ‘bcols’: [‘BFIELD_0’, ‘BFIELD_1’, ‘BFIELD_2’]}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘STB’: {‘MAG’: {‘datatype’: ‘8hz’, ‘time_resolution’: 1, ‘coords’: ‘rtn’, ‘bcols’: [‘BFIELD_0’, ‘BFIELD_1’, ‘BFIELD_2’]}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘pds-download-url’: ‘https://pds-ppi.igpp.ucla.edu/ditdos/download?id=pds://PPI’, ‘Juno’: {‘FGM’: {‘CRUISE’: {‘datatypes’: [‘1MIN’, ‘1SEC’], ‘url_format’: ‘JNO-SS-3-FGM-CAL-V1.0/DATA/CRUISE’}, ‘JUPYTER’: {‘datatypes’: [‘1MIN’, ‘1SEC’], ‘url_format’: ‘JNO-J-3-FGM-CAL-V1.0/DATA/JUPITER’}}}, ‘Wind’: {‘MAG’: {‘start’: ‘2016-01-01’, ‘end’: ‘2016-06-30’, ‘datatype’: ‘h4-rtn’, ‘tau’: 60, ‘time_resolution’: 0.1, ‘coords’: ‘rtn’, ‘bcols’: [‘BRTN_0’, ‘BRTN_1’, ‘BRTN_2’]}, ‘STATE’: {‘time_resolution’: 3600}}, ‘tau’: 60, ‘detection’: {‘index_std_threshold’: 2, ‘index_fluc_threshold’: 1, ‘index_diff_threshold’: 0.1}, ‘jno_start_date’: ‘2011-08-25’, ‘jno_end_date’: ‘2016-06-30’, ‘mission’: {‘source’: {‘time_resolution’: 1}}, ‘THEMIS’: {‘MAG’: {‘datatype’: ‘fgl’, ‘time_resolution’: 1, ‘coords’: ‘gse’, ‘bcols’: [‘B_x’, ‘B_y’, ‘B_z’]}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘THA’: {‘MAG’: {‘datatype’: ‘fgl’, ‘time_resolution’: 1, ‘coords’: ‘gse’, ‘bcols’: [‘B_x’, ‘B_y’, ‘B_z’]}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘THB’: {‘MAG’: {‘datatype’: ‘fgl’, ‘time_resolution’: 1, ‘coords’: ‘gse’, ‘bcols’: [‘B_x’, ‘B_y’, ‘B_z’]}, ‘STATE’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘OMNI’: {‘LowRes’: {‘datatype’: ‘hourly’, ‘time_resolution’: 3600}}, ‘omni_vars’: {‘N’: {‘COLNAME’: ‘plasma_density’, ‘FIELDNAM’: ‘Ion density’, ‘UNITS’: ‘Per cc’}, ‘T’: {‘COLNAME’: ‘plasma_temperature’, ‘FIELDNAM’: ‘Plasma temperature’, ‘UNITS’: ‘K’}, ‘V’: {‘COLNAME’: ‘plasma_speed’, ‘FIELDNAM’: ‘Flow speed’, ‘UNITS’: ‘km/s’}, ‘THETA-V’: {‘COLNAME’: ‘sw_vel_theta’, ‘FIELDNAM’: ‘Flow latitude’, ‘UNITS’: ‘Deg’}, ‘PHI-V’: {‘COLNAME’: ‘sw_vel_phi’, ‘FIELDNAM’: ‘Flow longitude’, ‘UNITS’: ‘Deg’}, ‘BX_GSE’: {‘COLNAME’: ‘B_background_x’}, ‘BY_GSE’: {‘COLNAME’: ‘B_background_y’}, ‘BZ_GSE’: {‘COLNAME’: ‘B_background_z’}}}\n\n\n\nkwargs",
    "crumbs": [
      "Pipelines",
      "Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "pipelines/10_mission.html",
    "href": "pipelines/10_mission.html",
    "title": "Mission Pipeline",
    "section": "",
    "text": "Generally, it includes the following steps:\nAdditional components:",
    "crumbs": [
      "Pipelines",
      "Mission Pipeline"
    ]
  },
  {
    "objectID": "pipelines/10_mission.html#combine-features",
    "href": "pipelines/10_mission.html#combine-features",
    "title": "Mission Pipeline",
    "section": "Combine features",
    "text": "Combine features\n\nsource\n\ncombine_features\n\n combine_features (candidates:polars.lazyframe.frame.LazyFrame,\n                   states_data:polars.lazyframe.frame.LazyFrame)",
    "crumbs": [
      "Pipelines",
      "Mission Pipeline"
    ]
  },
  {
    "objectID": "pipelines/10_mission.html#combining-magnetic-field-data-and-state-data",
    "href": "pipelines/10_mission.html#combining-magnetic-field-data-and-state-data",
    "title": "Mission Pipeline",
    "section": "Combining magnetic field data and state data",
    "text": "Combining magnetic field data and state data\nWith combined dataset, we calculate additional features for each candidate.\nLength\nthe length along the n direction of LMN coordinate system.\n\\[L_{n} = v_{n}  T_{duration}\\]\nHowever this may not be accurate due to the MVA method.\n\\[L_{mn} = v_{mn}  T_{duration}\\]\nIf we have the normal vector of the current sheet, we can calculate the length along the normal direction.\n\\[L_{normal} = L_{k} = v_{normal}  T_{duration}\\]\nAdditionally, we can calculate the length projected into RTN coordinate system.\n\\[L_{R} = L_{k} \\cos \\theta\\]\n\\[ j_0 = (\\frac{d B}{d t})_{max} \\frac{1}{v_{mn}}\\]\n\nCalculating additional features for the combined dataset\n\nsource\n\n\nvector_project_pl\n\n vector_project_pl (df:polars.lazyframe.frame.LazyFrame, v1_cols, v2_cols,\n                    name=None)\n\n\nsource\n\n\nvector_project\n\n vector_project (v1, v2, dim='v_dim')\n\n\nsource\n\n\ncalc_rotation_angle_pl\n\n calc_rotation_angle_pl (ldf:polars.lazyframe.frame.LazyFrame, v1_cols,\n                         v2_cols, name)\n\n\n\nInertial length\n\nsource\n\n\ncompute_inertial_length\n\n compute_inertial_length (ldf:polars.lazyframe.frame.LazyFrame,\n                          density_col='plasma_density')\n\n\n\nAlfven current\n\nsource\n\n\ncompute_Alfven_current\n\n compute_Alfven_current (ldf:polars.lazyframe.frame.LazyFrame)\n\n\nsource\n\n\ncompute_Alfven_speed\n\n compute_Alfven_speed (ldf:polars.lazyframe.frame.LazyFrame)",
    "crumbs": [
      "Pipelines",
      "Mission Pipeline"
    ]
  },
  {
    "objectID": "pipelines/10_mission.html#pipelines",
    "href": "pipelines/10_mission.html#pipelines",
    "title": "Mission Pipeline",
    "section": "Pipelines",
    "text": "Pipelines\n\nsource\n\ncalc_combined_features\n\n calc_combined_features (df:polars.lazyframe.frame.LazyFrame)",
    "crumbs": [
      "Pipelines",
      "Mission Pipeline"
    ]
  },
  {
    "objectID": "pipelines/10_mission.html#pipelines-1",
    "href": "pipelines/10_mission.html#pipelines-1",
    "title": "Mission Pipeline",
    "section": "Pipelines",
    "text": "Pipelines\n\nsource\n\ncreate_combined_data_pipeline\n\n create_combined_data_pipeline (sat_id, params:Optional[dict]=None,\n                                **kwargs)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\n\n\nsatellite id, used for namespace\n\n\nparams\nOptional\nNone\n\n\n\nkwargs\n\n\n\n\n\nReturns\nPipeline",
    "crumbs": [
      "Pipelines",
      "Mission Pipeline"
    ]
  },
  {
    "objectID": "pipelines/1_data.html",
    "href": "pipelines/1_data.html",
    "title": "Data Pipeline",
    "section": "",
    "text": "Roughly speaking every data source corresponds to an instrument in the mission.\nGenerally, it includes the following steps:",
    "crumbs": [
      "Pipelines",
      "Data Pipeline"
    ]
  },
  {
    "objectID": "pipelines/1_data.html#loading-data",
    "href": "pipelines/1_data.html#loading-data",
    "title": "Data Pipeline",
    "section": "Loading data",
    "text": "Loading data\n\n\nCode\ndef download_data(\n    start: str = None,\n    end: str = None,\n    datatype=None,\n    ts=None,  # time resolution\n    probe: str = None,\n    coord: str = None,\n):\n    \"\"\"Downloading data\"\"\"\n    ...\n\n\ndef load_data(\n    start: str = None,\n    end: str = None,\n    datatype=None,\n    ts=None,  # time resolution\n    probe: str = None,\n    coord: str = None,\n    vars: dict = None,\n):\n    \"\"\"Load data into a proper data structure, like dataframe.\n\n    - Downloading data\n    - Converting data structure\n    - Parsing original data (dealing with delimiters, missing values, etc.)\n    \"\"\"\n    ...",
    "crumbs": [
      "Pipelines",
      "Data Pipeline"
    ]
  },
  {
    "objectID": "pipelines/1_data.html#preprocessing-data",
    "href": "pipelines/1_data.html#preprocessing-data",
    "title": "Data Pipeline",
    "section": "Preprocessing data",
    "text": "Preprocessing data\n\n\nCode\ndef preprocess_data(\n    raw_data: Any | pl.DataFrame = None,\n    start: str = None,\n    end: str = None,\n    ts=None,  # time resolution\n    coord: str = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Preprocess the raw dataset (only minor transformations)\n\n    - Applying naming conventions for columns\n    - Parsing and typing data (like from string to datetime for time columns)\n    - Structuring the data (like pivoting, unpivoting, etc.)\n    - Changing storing format (like from `csv` to `parquet`)\n    - Dropping null columns\n    - Dropping duplicate time\n    - Resampling data to a given time resolution (better to do in the next stage)\n    - ... other 'transformations' commonly performed at this stage.\n    \"\"\"\n    pass",
    "crumbs": [
      "Pipelines",
      "Data Pipeline"
    ]
  },
  {
    "objectID": "pipelines/1_data.html#processing-data",
    "href": "pipelines/1_data.html#processing-data",
    "title": "Data Pipeline",
    "section": "Processing data",
    "text": "Processing data\nSome common preprocessing steps are:\n\nPartition data by year, see ids_finder.utils.basic.partition_data_by_year\n\nNote: we process the data every year to minimize the memory usage and to avoid the failure of the processing (so need to process all the data again if only fails sometimes).\n\n\nCode\ndef process_data(\n    raw_data: Any | pl.DataFrame,\n    ts: str = None,  # time resolution\n    coord: str = None,\n) -&gt; pl.DataFrame | Dict[str, pl.DataFrame]:\n    \"\"\"\n    Corresponding to primary data layer, where source data models are transformed into domain data models\n\n    - Transforming coordinate system if needed\n    - Discarding unnecessary columns\n    - Smoothing data\n    - Resampling data to a given time resolution\n    - Partitioning data, for the sake of memory\n    \"\"\"\n    pass\n\n\ndef extract_features():\n    pass",
    "crumbs": [
      "Pipelines",
      "Data Pipeline"
    ]
  },
  {
    "objectID": "pipelines/1_data.html#pipeline",
    "href": "pipelines/1_data.html#pipeline",
    "title": "Data Pipeline",
    "section": "Pipeline",
    "text": "Pipeline\n\n\nCode\ndef create_pipeline_template(\n    sat_id: str,  # satellite id, used for namespace\n    source: str,  # source data, like \"mag\" or \"plasma\", used for namespace\n    load_data_fn: Callable,\n    preprocess_data_fn: Callable,\n    process_data_fn: Callable,\n    load_inputs: dict = DEFAULT_LOAD_INPUTS,\n    process_inputs: dict = None,\n    params: Optional[dict] = None,\n    namespace=None,\n    **kwargs,\n) -&gt; Pipeline:\n    if params is None:\n        params = PARAMS\n    if namespace is None:\n        namespace = f\"{sat_id}.{source}\"\n\n    ts = params[sat_id][source].get(\"time_resolution\", 0)\n    datatype = params[sat_id][source][\"datatype\"]\n\n    ts_str = f\"ts_{ts}s\"\n\n    if process_inputs is None:\n        process_inputs = dict(\n            raw_data=f\"inter_data_{datatype}\",\n            ts=\"params:time_resolution\",\n        )\n\n    node_load_data = node(\n        load_data_fn,\n        inputs=load_inputs,\n        outputs=\"raw_data\",\n        name=\"load_data\",\n    )\n\n    node_preprocess_data = node(\n        preprocess_data_fn,\n        inputs=\"raw_data\",\n        outputs=f\"inter_data_{datatype}\",\n        name=\"preprocess_data\",\n    )\n\n    node_process_data = node(\n        process_data_fn,\n        inputs=process_inputs,\n        outputs=f\"primary_data_{ts_str}\",\n        name=\"process_data\",\n    )\n\n    nodes = [\n        node_load_data,\n        node_preprocess_data,\n        node_process_data,\n    ]\n\n    pipelines = pipeline(\n        nodes,\n        namespace=namespace,\n        # parameters={\n        #     \"params:start_date\": \"params:jno_start_date\",\n        #     \"params:end_date\": \"params:jno_end_date\",\n        # },\n    )\n\n    return pipelines\n\n\n\nsource\n\ncreate_pipeline_template\n\n create_pipeline_template (sat_id:str, source:str, load_data_fn:Callable,\n                           preprocess_data_fn:Callable,\n                           process_data_fn:Callable,\n                           load_inputs:dict={'start': 'params:start',\n                           'end': 'params:end', 'datatype':\n                           'params:datatype'}, process_inputs:dict=None,\n                           params:Optional[dict]=None, namespace=None,\n                           **kwargs)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\nstr\n\nsatellite id, used for namespace\n\n\nsource\nstr\n\nsource data, like “mag” or “plasma”, used for namespace\n\n\nload_data_fn\nCallable\n\n\n\n\npreprocess_data_fn\nCallable\n\n\n\n\nprocess_data_fn\nCallable\n\n\n\n\nload_inputs\ndict\n{‘start’: ‘params:start’, ‘end’: ‘params:end’, ‘datatype’: ‘params:datatype’}\n\n\n\nprocess_inputs\ndict\nNone\n\n\n\nparams\nOptional\nNone\n\n\n\nnamespace\nNoneType\nNone\n\n\n\nkwargs\n\n\n\n\n\nReturns\nPipeline",
    "crumbs": [
      "Pipelines",
      "Data Pipeline"
    ]
  },
  {
    "objectID": "01_ids_detection.html",
    "href": "01_ids_detection.html",
    "title": "ID identification",
    "section": "",
    "text": "The first index is \\[ \\frac{\\sigma(B)}{Max(\\sigma(B_-),\\sigma(B_+))} \\]\nThe second index is \\[ \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)} \\]\nThe ﬁrst two conditions guarantee that the ﬁeld changes of the IDs identiﬁed are large enough to be distinguished from the stochastic ﬂuctuations on magnetic ﬁelds, while the third is a supplementary condition toreduce the uncertainty of recognition.\nthird index (relative field jump) is \\[ \\frac{| \\Delta \\vec{B} |}{|B_{bg}|} \\] a supplementary condition to reduce the uncertainty of recognition\nsource",
    "crumbs": [
      "ID identification"
    ]
  },
  {
    "objectID": "01_ids_detection.html#index-of-the-standard-deviation",
    "href": "01_ids_detection.html#index-of-the-standard-deviation",
    "title": "ID identification",
    "section": "Index of the standard deviation",
    "text": "Index of the standard deviation\n[01/09/24 20:28:46] WARNING UserWarning: Unknown section Notes warnings.py:109\n\nsource\n\nadd_neighbor_std\n\n add_neighbor_std (df:polars.lazyframe.frame.LazyFrame,\n                   tau:datetime.timedelta, join_strategy='inner')\n\nGet the neighbor standard deviations\n                WARNING  UserWarning: Unknown section Examples           warnings.py:109\n                                                                                        \n\nsource\n\n\ncompute_index_std\n\n compute_index_std (df:polars.lazyframe.frame.LazyFrame)\n\nCompute the standard deviation index based on the given DataFrame\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\nLazyFrame\nnoqa: F811\n\n\nReturns\n- pl.LazyFrame: DataFrame with calculated ‘index_std’ column.",
    "crumbs": [
      "ID identification"
    ]
  },
  {
    "objectID": "01_ids_detection.html#index-of-fluctuation",
    "href": "01_ids_detection.html#index-of-fluctuation",
    "title": "ID identification",
    "section": "Index of fluctuation",
    "text": "Index of fluctuation\n\nsource\n\ncompute_index_fluctuation\n\n compute_index_fluctuation (df:polars.lazyframe.frame.LazyFrame,\n                            base_col='B_std')",
    "crumbs": [
      "ID identification"
    ]
  },
  {
    "objectID": "01_ids_detection.html#index-of-the-relative-field-jump",
    "href": "01_ids_detection.html#index-of-the-relative-field-jump",
    "title": "ID identification",
    "section": "Index of the relative field jump",
    "text": "Index of the relative field jump\n\nsource\n\npl_dvec\n\n pl_dvec (columns, *more_columns)\n\n\nsource\n\n\ncompute_index_diff\n\n compute_index_diff (df:polars.lazyframe.frame.LazyFrame,\n                     period:datetime.timedelta, cols)\n\n\nsource\n\n\ncompute_indices\n\n compute_indices (df:polars.dataframe.frame.DataFrame,\n                  tau:datetime.timedelta, bcols:list[str]=['BX', 'BY',\n                  'BZ'])\n\nCompute all index based on the given DataFrame and tau value.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nInput DataFrame.\n\n\ntau\ntimedelta\n\nTime interval value.\n\n\nbcols\nlist\n[‘BX’, ‘BY’, ‘BZ’]\n\n\n\nReturns\nDataFrame\n\nTuple containing DataFrame results for fluctuation index,standard deviation index, and ‘index_num’.",
    "crumbs": [
      "ID identification"
    ]
  },
  {
    "objectID": "01_ids_detection.html#pipelines",
    "href": "01_ids_detection.html#pipelines",
    "title": "ID identification",
    "section": "Pipelines",
    "text": "Pipelines\n\nsource\n\nfilter_indices\n\n filter_indices\n                 (df:polars.dataframe.frame.DataFrame|polars.lazyframe.fra\n                 me.LazyFrame, index_std_threshold:float=2,\n                 index_fluc_threshold:float=1,\n                 index_diff_threshold:float=0.1, sparse_num:int=15)\n\n\nsource\n\n\ndetect_events\n\n detect_events (data:polars.dataframe.frame.DataFrame,\n                tau:datetime.timedelta, ts:datetime.timedelta, bcols)",
    "crumbs": [
      "ID identification"
    ]
  },
  {
    "objectID": "missions/juno/index.html",
    "href": "missions/juno/index.html",
    "title": "IDs from Juno",
    "section": "",
    "text": "See following notebooks for details:",
    "crumbs": [
      "Missions",
      "IDs from Juno"
    ]
  },
  {
    "objectID": "missions/juno/index.html#background",
    "href": "missions/juno/index.html#background",
    "title": "IDs from Juno",
    "section": "Background",
    "text": "Background\nSpacecraft-Solar equatorial\nhttps://pds-ppi.igpp.ucla.edu/data/JNO-SS-3-FGM-CAL-V1.0/INDEX/INDEX.TAB\n------------------------------------------------------------------------------\nJuno Mission Phases                                                           \n------------------------------------------------------------------------------\nStart       Mission                                                           \nDate        Phase                                                             \n==============================================================================\n2011-08-05  Launch                                                            \n2011-08-08  Inner Cruise 1                                                    \n2011-10-10  Inner Cruise 2                                                    \n2013-05-28  Inner Cruise 3                                                    \n2013-11-05  Quiet Cruise                                                      \n2016-01-05  Jupiter Approach                                                  \n2016-06-30  Jupiter Orbital Insertion                                         \n2016-07-05  Capture Orbit                                                     \n2016-10-19  Period Reduction Maneuver                                         \n2016-10-20  Orbits 1-2                                                        \n2016-11-09  Science Orbits                                                    \n2017-10-11  Deorbit\n\nCoordinate System of Data\n\nSE (Solar Equatorial)\n\nCode: se\nResampling options:\n\nNumber of seconds (1 or 60): se_rN[N]s\nResampled 1 hour: se_r1h\n\n\nPC (Planetocentric)\n\nCode: pc\nResampling options:\n\nNumber of seconds (1 or 60): pc_rN[N]s\n\n\nSS (Sun-State)\n\nCode: ss\nResampling options:\n\nNumber of seconds (1 or 60): ss_rN[N]s\n\n\nPL (Payload)\n\nCode: pl\nResampling options:\n\nNumber of seconds (1 or 60): pl_rN[N]s\n\n\n\nThere are three principal coordinate systems used to represent the data in this archive.\n\nThe SE coordinate system is a Spacecraft- Solar equatorial system and it will be used for cruise data only.\nThe sun-state (ss) and planetocentric (pc) will be used for Earth Fly By (EFB) and Jupiter orbital data.\nCartesian representations are used for all three coordinate systems. These coordinate systems are specified relative to a “target body” which may be any solar system object (but for this orbital operations will Jupiter). In what follows we will reference Jupiter as the target body, but, for example, if observations near a satellite (such as Io) are desired in Io-centric coordinates, the satellite Io may be specified as the target body.\n\nThe SE coordinate system is defined using the sun-spacecraft vector as the primary reference vector; sun’s rotation axis as the secondary reference vector (z). The x axis lies along the sun-spacecraft vector, the z axis is in the plane defined by the Sun’s rotation axis and the spacecraft-sun vector. The y axis completes the system.\nThe ss coordinate system is defined using the instantaneous Jupiter-Sun vector as the primary reference vector (x direction). The X-axis lies along this vector and is taken to be positive toward the Sun. The Jupiter orbital velocity vector is the second vector used to define the coordinate system; the y axis lies in the plane determined by the Jupiter-Sun vector and the velocity vector and is orthogonal to the x axis (very nearly the negative of the velocity vector). The vector cross product of x and y yields a vector z parallel to the northward (upward) normal of the orbit plane of Jupiter. This system is sometimes called a sun-state (ss) coordinate system since its principal vectors are the Sun vector and the Jupiter state vector.",
    "crumbs": [
      "Missions",
      "IDs from Juno"
    ]
  },
  {
    "objectID": "missions/juno/index.html#setup",
    "href": "missions/juno/index.html#setup",
    "title": "IDs from Juno",
    "section": "Setup",
    "text": "Setup\nNeed to run command in shell first as pipeline is project-specific command\nkedro pipeline create juno",
    "crumbs": [
      "Missions",
      "IDs from Juno"
    ]
  },
  {
    "objectID": "missions/juno/index.html#processing-the-whole-data",
    "href": "missions/juno/index.html#processing-the-whole-data",
    "title": "IDs from Juno",
    "section": "Processing the whole data",
    "text": "Processing the whole data\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='JNO')",
    "crumbs": [
      "Missions",
      "IDs from Juno"
    ]
  },
  {
    "objectID": "missions/juno/index.html#obsolete",
    "href": "missions/juno/index.html#obsolete",
    "title": "IDs from Juno",
    "section": "Obsolete",
    "text": "Obsolete\n\nEstimate\n1 day of data resampled by 1 sec is about 12 MB.\nSo 1 year of data is about 4 GB, and 6 years of JUNO Cruise data is about 24 GB.\nDownloading rate is about 250 KB/s, so it will take about 3 days to download all the data.\n\n\nCode\nnum_of_files = 6*365\njno_file_size = 12e3\nthm_file_size = 40e3\nfiles_size = jno_file_size + thm_file_size\ndownloading_rate = 250\nprocessing_rate = 1/60\n\ntime_to_download = num_of_files * files_size / downloading_rate / 60 / 60\nspace_required = num_of_files * files_size / 1e6\ntime_to_process = num_of_files / processing_rate / 60 / 60\n\nprint(f\"Time to download: {time_to_download:.2f} hours\")\nprint(f\"Disk space required: {space_required:.2f} GB\")\nprint(f\"Time to process: {time_to_process:.2f} hours\")\n\n\nTime to download: 126.53 hours\nDisk space required: 113.88 GB\nTime to process: 36.50 hours",
    "crumbs": [
      "Missions",
      "IDs from Juno"
    ]
  },
  {
    "objectID": "missions/juno/state.html",
    "href": "missions/juno/state.html",
    "title": "JUNO State data pipeline",
    "section": "",
    "text": "JUNO state data is composed of two parts: the plasma data from MHD model and 1h-average magnetic field data from FGM, which providing background interplanetary magnetic field (IMF) information.",
    "crumbs": [
      "Missions",
      "IDs from Juno",
      "JUNO State data pipeline"
    ]
  },
  {
    "objectID": "missions/juno/state.html#getting-background-magnetic-field",
    "href": "missions/juno/state.html#getting-background-magnetic-field",
    "title": "JUNO State data pipeline",
    "section": "Getting background magnetic field",
    "text": "Getting background magnetic field\n\nsource\n\nprocess_IMF_data\n\n process_IMF_data\n                   (raw_data:Dict[str,Callable[...,polars.lazyframe.frame.\n                   LazyFrame]], ts:str=3600)\n\nResampling data to provide background magnetic field\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nDict\n\n\n\n\nts\nstr\n3600\ntime resolution\n\n\nReturns\npolars.dataframe.frame.DataFrame | dict[str, polars.dataframe.frame.DataFrame]",
    "crumbs": [
      "Missions",
      "IDs from Juno",
      "JUNO State data pipeline"
    ]
  },
  {
    "objectID": "missions/juno/state.html#loading-data",
    "href": "missions/juno/state.html#loading-data",
    "title": "JUNO State data pipeline",
    "section": "Loading data",
    "text": "Loading data\nFor interpolated solar wind at JUNO’s location, see model output file.\n\nsource\n\nload_data\n\n load_data (raw_data:pandas.core.frame.DataFrame, start:str, end:str)",
    "crumbs": [
      "Missions",
      "IDs from Juno",
      "JUNO State data pipeline"
    ]
  },
  {
    "objectID": "missions/juno/state.html#preprocessing-data",
    "href": "missions/juno/state.html#preprocessing-data",
    "title": "JUNO State data pipeline",
    "section": "Preprocessing data",
    "text": "Preprocessing data\nCoordinate System:  HGI\nVariables:\n  Date_Time: date and time in ISO format [UT]\n  hour: elapsed time since trajectory start [hr]\n  r: radial coordinate in HGI [AU]\n  phi: longitude coordinate in HGI [deg]\n  Rho: density [amu/cm^3]\n  Ux, Uy, Uz: bulk velocity components in HGI [km/s]\n  Bx, By, Bz: magnetic field components in HGI [nT]\n  Ti: ion temperature [K]\n\nsource\n\npreprocess_data\n\n preprocess_data (raw_data:polars.lazyframe.frame.LazyFrame)\n\nPreprocess the raw dataset (only minor transformations)\n\nParsing and typing data (like from string to datetime for time columns)\nChanging storing format (like from csv to parquet)",
    "crumbs": [
      "Missions",
      "IDs from Juno",
      "JUNO State data pipeline"
    ]
  },
  {
    "objectID": "missions/juno/state.html#processing-data",
    "href": "missions/juno/state.html#processing-data",
    "title": "JUNO State data pipeline",
    "section": "Processing data",
    "text": "Processing data\nCombining plasma data and background magnetic field\n\nsource\n\nhgi2rtn\n\n hgi2rtn\n          (df:polars.lazyframe.frame.LazyFrame|polars.dataframe.frame.Data\n          Frame)\n\nTransform coordinates from HGI to RTN\n\nsource\n\n\nprocess_data\n\n process_data (model_data:polars.lazyframe.frame.LazyFrame,\n               imf_data:polars.lazyframe.frame.LazyFrame)\n\nCorresponding to primary data layer, where source data models are transformed into domain data models\n\nTransforming data to RTN (Radial-Tangential-Normal) coordinate system\nApplying naming conventions for columns",
    "crumbs": [
      "Missions",
      "IDs from Juno",
      "JUNO State data pipeline"
    ]
  },
  {
    "objectID": "missions/juno/state.html#pipeline",
    "href": "missions/juno/state.html#pipeline",
    "title": "JUNO State data pipeline",
    "section": "Pipeline",
    "text": "Pipeline\n\nsource\n\ncreate_IMF_pipeline\n\n create_IMF_pipeline ()\n\n\nsource\n\n\ncreate_pipeline\n\n create_pipeline (sat_id='JNO', source='STATE')",
    "crumbs": [
      "Missions",
      "IDs from Juno",
      "JUNO State data pipeline"
    ]
  },
  {
    "objectID": "missions/themis/mag.html",
    "href": "missions/themis/mag.html",
    "title": "THEMIS Magnetic field data pipeline",
    "section": "",
    "text": "For convenience, we choose magnetic field data in GSE coordinate system\nThe fgs data are in 3-4s resolution",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/themis/mag.html#loading-data",
    "href": "missions/themis/mag.html#loading-data",
    "title": "THEMIS Magnetic field data pipeline",
    "section": "Loading data",
    "text": "Loading data\n\nsource\n\nload_data\n\n load_data (start, end, sat='THB', instrument='FGM', datatype='fgs',\n            coord='gse')\n\n\nsource\n\n\nspz2df\n\n spz2df (raw_data:speasy.products.variable.SpeasyVariable)\n\n\nsource\n\n\ndownload_data\n\n download_data (trange, mission, instrument, sat, datatype, coord)",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/themis/mag.html#preprocessing-data",
    "href": "missions/themis/mag.html#preprocessing-data",
    "title": "THEMIS Magnetic field data pipeline",
    "section": "Preprocessing data",
    "text": "Preprocessing data\n\nsource\n\npreprocess_data\n\n preprocess_data (raw_data:polars.lazyframe.frame.LazyFrame,\n                  datatype:str=None)\n\nPreprocess the raw dataset (only minor transformations)\n\nApplying naming conventions for columns\nDropping duplicate time\nChanging storing format to parquet",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/themis/mag.html#processing-data",
    "href": "missions/themis/mag.html#processing-data",
    "title": "THEMIS Magnetic field data pipeline",
    "section": "Processing data",
    "text": "Processing data\n\nsource\n\nprocess_data\n\n process_data (raw_data:polars.lazyframe.frame.LazyFrame, ts)\n\n\n\n\n\nType\nDetails\n\n\n\n\nraw_data\nLazyFrame\n\n\n\nts\n\ntime resolution, in seconds",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/themis/mag.html#pipeline",
    "href": "missions/themis/mag.html#pipeline",
    "title": "THEMIS Magnetic field data pipeline",
    "section": "Pipeline",
    "text": "Pipeline\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='THB', source='MAG')",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/index.html",
    "href": "missions/index.html",
    "title": "Missions",
    "section": "",
    "text": "We try to employ our method and find IDs from different missions.",
    "crumbs": [
      "Missions"
    ]
  },
  {
    "objectID": "missions/wind/mag.html",
    "href": "missions/wind/mag.html",
    "title": "Wind Magnetic field data pipeline",
    "section": "",
    "text": "We use magnetic field data in RTN coordinate system",
    "crumbs": [
      "Missions",
      "IDs from Wind",
      "Wind Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/wind/mag.html#loading-data",
    "href": "missions/wind/mag.html#loading-data",
    "title": "Wind Magnetic field data pipeline",
    "section": "Loading data",
    "text": "Loading data\n\nsource\n\ndownload_data\n\n download_data (start:str, end:str, datatype='h4-rtn')\n\n\nsource\n\n\nload_data\n\n load_data (start:str=None, end:str=None, datatype='h4-rtn',\n            var_names='BRTN')",
    "crumbs": [
      "Missions",
      "IDs from Wind",
      "Wind Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/wind/mag.html#preprocessing-data",
    "href": "missions/wind/mag.html#preprocessing-data",
    "title": "Wind Magnetic field data pipeline",
    "section": "Preprocessing data",
    "text": "Preprocessing data\n\nsource\n\npreprocess_data\n\n preprocess_data (raw_data, var_names='BRTN')\n\nPreprocess the raw dataset (only minor transformations) - Applying naming conventions for columns",
    "crumbs": [
      "Missions",
      "IDs from Wind",
      "Wind Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/wind/mag.html#pipeline",
    "href": "missions/wind/mag.html#pipeline",
    "title": "Wind Magnetic field data pipeline",
    "section": "Pipeline",
    "text": "Pipeline\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='Wind', source='MAG', params=None, **kwargs)",
    "crumbs": [
      "Missions",
      "IDs from Wind",
      "Wind Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/stereo/index.html",
    "href": "missions/stereo/index.html",
    "title": "IDs from STEREO",
    "section": "",
    "text": "See following notebooks for details:",
    "crumbs": [
      "Missions",
      "IDs from STEREO"
    ]
  },
  {
    "objectID": "missions/stereo/index.html#setup",
    "href": "missions/stereo/index.html#setup",
    "title": "IDs from STEREO",
    "section": "Setup",
    "text": "Setup\nNeed to run command in shell first as pipeline is project-specific command\nkedro pipeline create stearo\nTo get STEREO-A state data, run kedro run --to-outputs=sta.primary_state_rtn_1h\nTo get candidates data, run kedro run --from-inputs=sta.feature_1s --to-outputs=candidates.sta_1s",
    "crumbs": [
      "Missions",
      "IDs from STEREO"
    ]
  },
  {
    "objectID": "missions/stereo/index.html#pipelines",
    "href": "missions/stereo/index.html#pipelines",
    "title": "IDs from STEREO",
    "section": "Pipelines",
    "text": "Pipelines\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='STA')",
    "crumbs": [
      "Missions",
      "IDs from STEREO"
    ]
  },
  {
    "objectID": "missions/stereo/state.html",
    "href": "missions/stereo/state.html",
    "title": "STEREO State data pipeline",
    "section": "",
    "text": "For STEREO’s mission, We use 1-hour averaged merged data from COHOWeb.\nSee STEREO ASCII merged data and one sample file [here](https://spdf.gsfc.nasa.gov/pub/data/stereo/ahead/l2/merged/stereoa2011.asc\nPlasma in RTN (Radial-Tangential-Normal) coordinate system - Proton Flow Speed, km/sec - Proton Flow Elevation Angle/Latitude, deg. - Proton Flow Azimuth Angle/Longitude, deg. - Proton Density, n/cc - Proton Temperature, K)\nNotes - Note1: There is a big gap 2014/12/16 - 2015/07/20 in plasma data - Note2: There is a big gap 2015/03/21 - 2015/07/09 and 2015/10/27 - 2015/11/15 in mag data - Note that for missing data, fill values consisting of a blank followed by 9’s which together constitute the format are used",
    "crumbs": [
      "Missions",
      "IDs from STEREO",
      "STEREO State data pipeline"
    ]
  },
  {
    "objectID": "missions/stereo/state.html#loading-data",
    "href": "missions/stereo/state.html#loading-data",
    "title": "STEREO State data pipeline",
    "section": "Loading data",
    "text": "Loading data\n\nsource\n\ndownload_data\n\n download_data (start:str, end:str, datatype)\n\n\nsource\n\n\nload_data\n\n load_data (start:str, end:str, datatype='hourly')\n\n\nDownloading data\nReading data into a proper data structure, like dataframe.\n\nParsing original data (dealing with delimiters, missing values, etc.)",
    "crumbs": [
      "Missions",
      "IDs from STEREO",
      "STEREO State data pipeline"
    ]
  },
  {
    "objectID": "missions/stereo/state.html#preprocessing-data",
    "href": "missions/stereo/state.html#preprocessing-data",
    "title": "STEREO State data pipeline",
    "section": "Preprocessing data",
    "text": "Preprocessing data\n\nsource\n\npreprocess_data\n\n preprocess_data (raw_data:polars.dataframe.frame.DataFrame)\n\nPreprocess the raw dataset (only minor transformations)\n\nParsing and typing data (like from string to datetime for time columns)\nChanging storing format (like from csv to parquet)",
    "crumbs": [
      "Missions",
      "IDs from STEREO",
      "STEREO State data pipeline"
    ]
  },
  {
    "objectID": "missions/stereo/state.html#processs-state-data",
    "href": "missions/stereo/state.html#processs-state-data",
    "title": "STEREO State data pipeline",
    "section": "Processs state data",
    "text": "Processs state data\n\nsource\n\nprocess_data\n\n process_data (raw_data:polars.dataframe.frame.DataFrame, ts=None,\n               columns:list[str]=['Radial Distance, AU', 'HGI Lat. of the\n               S/C', 'HGI Long. of the S/C', 'SW Plasma Speed, km/s', 'SW\n               Lat. Angle RTN, deg.', 'SW Long. Angle RTN, deg.', 'SW\n               Plasma Density, N/cm^3', 'SW Plasma Temperature, K', 'IMF\n               BR, nT (RTN)', 'IMF BT, nT (RTN)', 'IMF BN, nT (RTN)', 'IMF\n               B Scalar, nT'])\n\nCorresponding to primary data layer, where source data models are transformed into domain data models\n\nApplying naming conventions for columns\nTransforming data to RTN (Radial-Tangential-Normal) coordinate system\nDiscarding unnecessary columns\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nDataFrame\n\n\n\n\nts\nNoneType\nNone\ntime resolution\n\n\ncolumns\nlist\n[‘Radial Distance, AU’, ‘HGI Lat. of the S/C’, ‘HGI Long. of the S/C’, ‘SW Plasma Speed, km/s’, ‘SW Lat. Angle RTN, deg.’, ‘SW Long. Angle RTN, deg.’, ‘SW Plasma Density, N/cm^3’, ‘SW Plasma Temperature, K’, ‘IMF BR, nT (RTN)’, ‘IMF BT, nT (RTN)’, ‘IMF BN, nT (RTN)’, ‘IMF B Scalar, nT’]\n\n\n\nReturns\nDataFrame\n\n\n\n\n\n\nsource\n\n\nconvert_state_to_rtn\n\n convert_state_to_rtn (df:polars.dataframe.frame.DataFrame)\n\nConvert state data to RTN coordinates",
    "crumbs": [
      "Missions",
      "IDs from STEREO",
      "STEREO State data pipeline"
    ]
  },
  {
    "objectID": "missions/stereo/state.html#pipelines",
    "href": "missions/stereo/state.html#pipelines",
    "title": "STEREO State data pipeline",
    "section": "Pipelines",
    "text": "Pipelines\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='STA', source='STATE')",
    "crumbs": [
      "Missions",
      "IDs from STEREO",
      "STEREO State data pipeline"
    ]
  },
  {
    "objectID": "analysis/10_classification.html",
    "href": "analysis/10_classification.html",
    "title": "ID classification",
    "section": "",
    "text": "Code\nimport numpy as np\nIn this method, TDs and RDs satisfy $ &lt; 0.2$ and $ | | &gt; 0.4$ B BN bg ∣∣ ∣∣ , &lt; D 0.2 B B bg ∣∣ ∣ ∣ , respectively. Moreover, IDs with &lt; 0.4 B BN bg ∣∣ ∣∣ , &lt; D 0.2 B B bg ∣∣ ∣ ∣ could be either TDs or RDs, and so are termed EDs. Similarly, NDs are defined as &gt; 0.4 B BN bg ∣∣ ∣∣ , &gt; D 0.2 B B bg ∣∣ ∣ ∣ because they can be neither TDs nor RDs. It is worth noting that EDs and NDs here are not physical concepts like RDs and TDs. RDs or TDs correspond to specific types of structures in the MHD framework, while EDs and NDs are introduced just to better quantify the statistical results.\nCriteria Used to Classify Discontinuities on the Basis of Magnetic Data Type\nCode\nBnOverB_RD_lower_threshold = 0.4\ndBOverB_RD_upper_threshold = 0.2\n\nBnOverB_TD_upper_threshold = 0.2\ndBOverB_TD_lower_threshold = dBOverB_RD_upper_threshold\n\nBnOverB_ED_upper_threshold = BnOverB_RD_lower_threshold\ndBOverB_ED_upper_threshold = dBOverB_TD_lower_threshold\n\nBnOverB_ND_lower_threshold = BnOverB_TD_upper_threshold\ndBOverB_ND_lower_threshold = dBOverB_RD_upper_threshold\nCode\ndef classify_id(BnOverB, dBOverB):\n    BnOverB = np.abs(np.asarray(BnOverB))\n    dBOverB = np.asarray(dBOverB)\n\n    s1 = (BnOverB &gt; BnOverB_RD_lower_threshold)\n    s2 = (dBOverB &gt; dBOverB_RD_upper_threshold)\n    s3 = (BnOverB &gt; BnOverB_TD_upper_threshold)\n    s4 = s2 # note: s4 = (dBOverB &gt; dBOverB_TD_lower_threshold)\n    \n    RD = s1 & ~s2\n    TD = ~s3 & s4\n    ED = ~s1 & ~s4\n    ND = s3 & s2\n\n    # Create an empty result array with the same shape\n    result = np.empty_like(BnOverB, dtype=object)\n\n    result[RD] = \"RD\"\n    result[TD] = \"TD\"\n    result[ED] = \"ED\"\n    result[ND] = \"ND\"\n\n    return result",
    "crumbs": [
      "Results",
      "ID classification"
    ]
  },
  {
    "objectID": "analysis/10_classification.html#classification-of-candidates",
    "href": "analysis/10_classification.html#classification-of-candidates",
    "title": "ID classification",
    "section": "Classification of candidates",
    "text": "Classification of candidates\n\n\nCode\nsns.jointplot(\n    candidates_jno_tau_60s,\n    x='dBOverB', y='BnOverB',\n    # kind='kde',\n    kind=\"hex\",\n)\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:2                                                                                    │\n│                                                                                                  │\n│   1 sns.jointplot(                                                                               │\n│ ❱ 2 │   candidates_jno_tau_60s,                                                                  │\n│   3 │   x='dBOverB', y='BnOverB',                                                                │\n│   4 │   # kind='kde',                                                                            │\n│   5 │   kind=\"hex\",                                                                              │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'candidates_jno_tau_60s' is not defined\n\n\n\n\n\nCode\nsns.jointplot(\n    all_candidates,\n    x='dBOverB', y='BnOverB',\n    # kind='kde',\n    kind=\"hex\",\n)\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:2                                                                                    │\n│                                                                                                  │\n│   1 sns.jointplot(                                                                               │\n│ ❱ 2 │   all_candidates,                                                                          │\n│   3 │   x='dBOverB', y='BnOverB',                                                                │\n│   4 │   # kind='kde',                                                                            │\n│   5 │   kind=\"hex\",                                                                              │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'all_candidates' is not defined\n\n\n\n\nDistribution of the DD types\n\nPlot distribution of types for each missions\n\n\nCode\nalt.Chart(all_candidates).encode(\n    x=alt.X(\"count()\").stack(\"normalize\").title(\"Share of ID types\"),\n    y=alt.Y('sat').title(None),\n    color='type',\n).mark_bar()\n\n# alt.Chart(distributions).encode(\n#     alt.X('ratio:Q', title='DD type distribution').axis(format='.0%'),\n#     y=alt.Y('sat', title=None),\n#     color='type',\n# ).mark_bar()\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱  1 alt.Chart(all_candidates).encode(                                                           │\n│    2 │   x=alt.X(\"count()\").stack(\"normalize\").title(\"Share of ID types\"),                       │\n│    3 │   y=alt.Y('sat').title(None),                                                             │\n│    4 │   color='type',                                                                           │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'alt' is not defined\n\n\n\n\n\nCode\ndistributions = all_candidates.group_by(\"sat\", \"type\").agg(pl.count()).with_columns(\n    (pl.col(\"count\") / pl.sum(\"count\").over(\"sat\")).alias(\"ratio\")\n)\ncount_table = distributions.to_pandas().pivot(index='sat', columns='type', values='count')[['RD', 'TD', 'ED', 'ND']]\nratio_table = distributions.to_pandas().pivot(index='sat', columns='type', values='ratio')[['RD', 'TD', 'ED', 'ND']]\n# display(distributions.to_pandas().pivot(index='sat', columns='type', values='ratio')[['RD', 'TD', 'ED', 'ND']].style.format(\"{:.0%}\"))\ndisplay(count_table, ratio_table.style.format(\"{:.0%}\"))\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 distributions = all_candidates.group_by(\"sat\", \"type\").agg(pl.count()).with_columns(         │\n│   2 │   (pl.col(\"count\") / pl.sum(\"count\").over(\"sat\")).alias(\"ratio\")                           │\n│   3 )                                                                                            │\n│   4 count_table = distributions.to_pandas().pivot(index='sat', columns='type', values='count     │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'all_candidates' is not defined\n\n\n\n\n\nPlot distribution of types for each missions over time\n\n\nCode\nalt.Chart(all_candidates).mark_bar(binSpacing=0).encode(\n    x=\"yearmonth(time)\",\n    y=alt.Y(\"count()\").stack(\"normalize\").title(\"Share of ID types\"),\n    row=alt.Row(\"sat\").title(None),\n    color=\"type\",\n).configure_axis(grid=False).properties(height=100)\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 alt.Chart(all_candidates).mark_bar(binSpacing=0).encode(                                     │\n│   2 │   x=\"yearmonth(time)\",                                                                     │\n│   3 │   y=alt.Y(\"count()\").stack(\"normalize\").title(\"Share of ID types\"),                        │\n│   4 │   row=alt.Row(\"sat\").title(None),                                                          │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'alt' is not defined\n\n\n\n\n\nCode\nplot_type_distribution &lt;- function(data, bin_width = 30) { \n    data$date_only &lt;- as.Date(data$time)\n    \n    p &lt;- ggplot(data, aes(date_only, fill = type)) +\n        geom_histogram(binwidth = bin_width, position = \"fill\") + \n        theme_pubr(base_size = 16)\n        \n    return(p)\n}\n\np1 &lt;- plot_type_distribution(jno_candidates)\np2 &lt;- plot_type_distribution(thb_candidates)\np3 &lt;- plot_type_distribution(sta_candidates)\n\np &lt;- ggarrange(\n    p1 + rremove(\"xlab\"),\n    p2 + rremove(\"xlab\"), p3, \n    nrow = 3, align = 'hv', \n    labels=list(\"JUNO\", \"ARTEMIS-B\", \"STEREO-A\"), hjust=0, vjust=0,\n    legend = 'right', common.legend = TRUE\n)\n# save_plot(filename = \"type_distribution\")\np\n\n\nUsageError: Cell magic `%%R` not found.",
    "crumbs": [
      "Results",
      "ID classification"
    ]
  },
  {
    "objectID": "analysis/20_model.html",
    "href": "analysis/20_model.html",
    "title": "Models",
    "section": "",
    "text": "Need to run command in shell first as pipeline is project-specific command\nkedro pipeline create model\nkedro run --to-outputs=jno.primary_state_rtn_1h\n\n\nCode\nfrom loguru import logger\nfrom typing import Dict\n\nimport hvplot.polars\n\nimport numpy as np\nimport polars as pl\nimport polars.selectors as cs\nfrom ids_finder.utils.polars import pl_norm\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\n\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\n[01/11/24 17:26:45] WARNING  R[write to console]:                                                  callbacks.py:124\n                             Attaching package: ‘dplyr’                                                            \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:stats’:                                                                      \n                                                                                                                   \n                                 filter, lag                                                                       \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:base’:                                                                       \n                                                                                                                   \n                                 intersect, setdiff, setequal, union                                               \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]:   Please cite ggdensity! See                     callbacks.py:124\n                             citation(\"ggdensity\") for details.                                                    \n                                                                                                                   \n\n\n\n\n\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\n\n\n\nCode\ncatalog = load_catalog()\ncatalog.list()\n\n\n[01/11/24 17:26:46] WARNING  KedroDeprecationWarning: 'AbstractVersionedDataSet' has been renamed   warnings.py:109\n                             to 'AbstractVersionedDataset', and the alias will be removed in Kedro                 \n                             0.19.0                                                                                \n                                                                                                                   \n\n\n\n\n\n\n\n[\n    'sta.raw_state_merged',\n    'JNO.STATE.model_data',\n    'JNO.STATE.IMF_data',\n    'model.raw_jno_ss_se_1min',\n    'model.preprocessed_jno_ss_se_1min',\n    'JNO_index',\n    'THB.STATE.original_sw_data',\n    'parameters',\n    'params:JNO',\n    'params:JNO.MAG',\n    'params:JNO.MAG.datatype',\n    'params:JNO.MAG.time_resolution',\n    'params:JNO.MAG.bcols',\n    'params:JNO.MAG.coords',\n    'params:JNO.STATE',\n    'params:JNO.STATE.datatype',\n    'params:JNO.STATE.time_resolution',\n    'params:OMNI',\n    'params:OMNI.LowRes',\n    'params:OMNI.LowRes.datatype',\n    'params:OMNI.LowRes.time_resolution',\n    'params:omni_vars',\n    'params:omni_vars.N',\n    'params:omni_vars.N.COLNAME',\n    'params:omni_vars.N.FIELDNAM',\n    'params:omni_vars.N.UNITS',\n    'params:omni_vars.T',\n    'params:omni_vars.T.COLNAME',\n    'params:omni_vars.T.FIELDNAM',\n    'params:omni_vars.T.UNITS',\n    'params:omni_vars.V',\n    'params:omni_vars.V.COLNAME',\n    'params:omni_vars.V.FIELDNAM',\n    'params:omni_vars.V.UNITS',\n    'params:omni_vars.THETA-V',\n    'params:omni_vars.THETA-V.COLNAME',\n    'params:omni_vars.THETA-V.FIELDNAM',\n    'params:omni_vars.THETA-V.UNITS',\n    'params:omni_vars.PHI-V',\n    'params:omni_vars.PHI-V.COLNAME',\n    'params:omni_vars.PHI-V.FIELDNAM',\n    'params:omni_vars.PHI-V.UNITS',\n    'params:omni_vars.BX_GSE',\n    'params:omni_vars.BX_GSE.COLNAME',\n    'params:omni_vars.BY_GSE',\n    'params:omni_vars.BY_GSE.COLNAME',\n    'params:omni_vars.BZ_GSE',\n    'params:omni_vars.BZ_GSE.COLNAME',\n    'params:experiments',\n    'params:Wind',\n    'params:Wind.MAG',\n    'params:Wind.MAG.start',\n    'params:Wind.MAG.end',\n    'params:Wind.MAG.datatype',\n    'params:Wind.MAG.tau',\n    'params:Wind.MAG.time_resolution',\n    'params:Wind.MAG.coords',\n    'params:Wind.MAG.bcols',\n    'params:Wind.STATE',\n    'params:Wind.STATE.time_resolution',\n    'params:pds-download-url',\n    'params:Juno',\n    'params:Juno.FGM',\n    'params:Juno.FGM.CRUISE',\n    'params:Juno.FGM.CRUISE.datatypes',\n    'params:Juno.FGM.CRUISE.url_format',\n    'params:Juno.FGM.JUPYTER',\n    'params:Juno.FGM.JUPYTER.datatypes',\n    'params:Juno.FGM.JUPYTER.url_format',\n    'params:tau',\n    'params:detection',\n    'params:detection.index_std_threshold',\n    'params:detection.index_fluc_threshold',\n    'params:detection.index_diff_threshold',\n    'params:jno_start_date',\n    'params:jno_end_date',\n    'params:mission',\n    'params:mission.source',\n    'params:mission.source.time_resolution',\n    'params:THEMIS',\n    'params:THEMIS.MAG',\n    'params:THEMIS.MAG.datatype',\n    'params:THEMIS.MAG.time_resolution',\n    'params:THEMIS.MAG.coords',\n    'params:THEMIS.MAG.bcols',\n    'params:THEMIS.STATE',\n    'params:THEMIS.STATE.datatype',\n    'params:THEMIS.STATE.time_resolution',\n    'params:THA',\n    'params:THA.MAG',\n    'params:THA.MAG.datatype',\n    'params:THA.MAG.time_resolution',\n    'params:THA.MAG.coords',\n    'params:THA.MAG.bcols',\n    'params:THA.STATE',\n    'params:THA.STATE.datatype',\n    'params:THA.STATE.time_resolution',\n    'params:THB',\n    'params:THB.MAG',\n    'params:THB.MAG.datatype',\n    'params:THB.MAG.time_resolution',\n    'params:THB.MAG.coords',\n    'params:THB.MAG.bcols',\n    'params:THB.STATE',\n    'params:THB.STATE.datatype',\n    'params:THB.STATE.time_resolution',\n    'params:STEREO',\n    'params:STEREO.MAG',\n    'params:STEREO.MAG.datatype',\n    'params:STEREO.MAG.time_resolution',\n    'params:STEREO.MAG.coords',\n    'params:STEREO.MAG.bcols',\n    'params:STEREO.STATE',\n    'params:STEREO.STATE.datatype',\n    'params:STEREO.STATE.time_resolution',\n    'params:STA',\n    'params:STA.MAG',\n    'params:STA.MAG.datatype',\n    'params:STA.MAG.time_resolution',\n    'params:STA.MAG.coords',\n    'params:STA.MAG.bcols',\n    'params:STA.STATE',\n    'params:STA.STATE.datatype',\n    'params:STA.STATE.time_resolution',\n    'params:STB',\n    'params:STB.MAG',\n    'params:STB.MAG.datatype',\n    'params:STB.MAG.time_resolution',\n    'params:STB.MAG.coords',\n    'params:STB.MAG.bcols',\n    'params:STB.STATE',\n    'params:STB.STATE.datatype',\n    'params:STB.STATE.time_resolution'\n]",
    "crumbs": [
      "Results",
      "Models"
    ]
  },
  {
    "objectID": "analysis/20_model.html#setup",
    "href": "analysis/20_model.html#setup",
    "title": "Models",
    "section": "",
    "text": "Need to run command in shell first as pipeline is project-specific command\nkedro pipeline create model\nkedro run --to-outputs=jno.primary_state_rtn_1h\n\n\nCode\nfrom loguru import logger\nfrom typing import Dict\n\nimport hvplot.polars\n\nimport numpy as np\nimport polars as pl\nimport polars.selectors as cs\nfrom ids_finder.utils.polars import pl_norm\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\n\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\n[01/11/24 17:26:45] WARNING  R[write to console]:                                                  callbacks.py:124\n                             Attaching package: ‘dplyr’                                                            \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:stats’:                                                                      \n                                                                                                                   \n                                 filter, lag                                                                       \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:base’:                                                                       \n                                                                                                                   \n                                 intersect, setdiff, setequal, union                                               \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]:   Please cite ggdensity! See                     callbacks.py:124\n                             citation(\"ggdensity\") for details.                                                    \n                                                                                                                   \n\n\n\n\n\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\n\n\n\nCode\ncatalog = load_catalog()\ncatalog.list()\n\n\n[01/11/24 17:26:46] WARNING  KedroDeprecationWarning: 'AbstractVersionedDataSet' has been renamed   warnings.py:109\n                             to 'AbstractVersionedDataset', and the alias will be removed in Kedro                 \n                             0.19.0                                                                                \n                                                                                                                   \n\n\n\n\n\n\n\n[\n    'sta.raw_state_merged',\n    'JNO.STATE.model_data',\n    'JNO.STATE.IMF_data',\n    'model.raw_jno_ss_se_1min',\n    'model.preprocessed_jno_ss_se_1min',\n    'JNO_index',\n    'THB.STATE.original_sw_data',\n    'parameters',\n    'params:JNO',\n    'params:JNO.MAG',\n    'params:JNO.MAG.datatype',\n    'params:JNO.MAG.time_resolution',\n    'params:JNO.MAG.bcols',\n    'params:JNO.MAG.coords',\n    'params:JNO.STATE',\n    'params:JNO.STATE.datatype',\n    'params:JNO.STATE.time_resolution',\n    'params:OMNI',\n    'params:OMNI.LowRes',\n    'params:OMNI.LowRes.datatype',\n    'params:OMNI.LowRes.time_resolution',\n    'params:omni_vars',\n    'params:omni_vars.N',\n    'params:omni_vars.N.COLNAME',\n    'params:omni_vars.N.FIELDNAM',\n    'params:omni_vars.N.UNITS',\n    'params:omni_vars.T',\n    'params:omni_vars.T.COLNAME',\n    'params:omni_vars.T.FIELDNAM',\n    'params:omni_vars.T.UNITS',\n    'params:omni_vars.V',\n    'params:omni_vars.V.COLNAME',\n    'params:omni_vars.V.FIELDNAM',\n    'params:omni_vars.V.UNITS',\n    'params:omni_vars.THETA-V',\n    'params:omni_vars.THETA-V.COLNAME',\n    'params:omni_vars.THETA-V.FIELDNAM',\n    'params:omni_vars.THETA-V.UNITS',\n    'params:omni_vars.PHI-V',\n    'params:omni_vars.PHI-V.COLNAME',\n    'params:omni_vars.PHI-V.FIELDNAM',\n    'params:omni_vars.PHI-V.UNITS',\n    'params:omni_vars.BX_GSE',\n    'params:omni_vars.BX_GSE.COLNAME',\n    'params:omni_vars.BY_GSE',\n    'params:omni_vars.BY_GSE.COLNAME',\n    'params:omni_vars.BZ_GSE',\n    'params:omni_vars.BZ_GSE.COLNAME',\n    'params:experiments',\n    'params:Wind',\n    'params:Wind.MAG',\n    'params:Wind.MAG.start',\n    'params:Wind.MAG.end',\n    'params:Wind.MAG.datatype',\n    'params:Wind.MAG.tau',\n    'params:Wind.MAG.time_resolution',\n    'params:Wind.MAG.coords',\n    'params:Wind.MAG.bcols',\n    'params:Wind.STATE',\n    'params:Wind.STATE.time_resolution',\n    'params:pds-download-url',\n    'params:Juno',\n    'params:Juno.FGM',\n    'params:Juno.FGM.CRUISE',\n    'params:Juno.FGM.CRUISE.datatypes',\n    'params:Juno.FGM.CRUISE.url_format',\n    'params:Juno.FGM.JUPYTER',\n    'params:Juno.FGM.JUPYTER.datatypes',\n    'params:Juno.FGM.JUPYTER.url_format',\n    'params:tau',\n    'params:detection',\n    'params:detection.index_std_threshold',\n    'params:detection.index_fluc_threshold',\n    'params:detection.index_diff_threshold',\n    'params:jno_start_date',\n    'params:jno_end_date',\n    'params:mission',\n    'params:mission.source',\n    'params:mission.source.time_resolution',\n    'params:THEMIS',\n    'params:THEMIS.MAG',\n    'params:THEMIS.MAG.datatype',\n    'params:THEMIS.MAG.time_resolution',\n    'params:THEMIS.MAG.coords',\n    'params:THEMIS.MAG.bcols',\n    'params:THEMIS.STATE',\n    'params:THEMIS.STATE.datatype',\n    'params:THEMIS.STATE.time_resolution',\n    'params:THA',\n    'params:THA.MAG',\n    'params:THA.MAG.datatype',\n    'params:THA.MAG.time_resolution',\n    'params:THA.MAG.coords',\n    'params:THA.MAG.bcols',\n    'params:THA.STATE',\n    'params:THA.STATE.datatype',\n    'params:THA.STATE.time_resolution',\n    'params:THB',\n    'params:THB.MAG',\n    'params:THB.MAG.datatype',\n    'params:THB.MAG.time_resolution',\n    'params:THB.MAG.coords',\n    'params:THB.MAG.bcols',\n    'params:THB.STATE',\n    'params:THB.STATE.datatype',\n    'params:THB.STATE.time_resolution',\n    'params:STEREO',\n    'params:STEREO.MAG',\n    'params:STEREO.MAG.datatype',\n    'params:STEREO.MAG.time_resolution',\n    'params:STEREO.MAG.coords',\n    'params:STEREO.MAG.bcols',\n    'params:STEREO.STATE',\n    'params:STEREO.STATE.datatype',\n    'params:STEREO.STATE.time_resolution',\n    'params:STA',\n    'params:STA.MAG',\n    'params:STA.MAG.datatype',\n    'params:STA.MAG.time_resolution',\n    'params:STA.MAG.coords',\n    'params:STA.MAG.bcols',\n    'params:STA.STATE',\n    'params:STA.STATE.datatype',\n    'params:STA.STATE.time_resolution',\n    'params:STB',\n    'params:STB.MAG',\n    'params:STB.MAG.datatype',\n    'params:STB.MAG.time_resolution',\n    'params:STB.MAG.coords',\n    'params:STB.MAG.bcols',\n    'params:STB.STATE',\n    'params:STB.STATE.datatype',\n    'params:STB.STATE.time_resolution'\n]",
    "crumbs": [
      "Results",
      "Models"
    ]
  },
  {
    "objectID": "analysis/20_model.html#process-model-data",
    "href": "analysis/20_model.html#process-model-data",
    "title": "Models",
    "section": "Process model data",
    "text": "Process model data\n\n\nCode\ndef overview(df: pl.DataFrame, bcols, vcols):\n    \"\"\"Overview of the data\"\"\"\n    df_pd = df.to_pandas()\n    df_pd.hvplot(x=\"time\", y=bcols)\n    \n    b_fig = df_pd.hvplot.line(x=\"time\", y=bcols)\n    v_fig = df_pd.hvplot.line(x=\"time\", y=vcols)\n    rho_fig = df_pd.hvplot.line(x=\"time\", y=\"rho\", logy=True)\n    Ti_fig = df_pd.hvplot.line(x=\"time\", y=\"Ti\", logy=True)\n    return (b_fig + v_fig + rho_fig + Ti_fig).cols(1).opts(shared_axes=False)\n\n\n\n\nCode\njno_state_data : pl.DataFrame = catalog.load('JNO.STATE.primary_data_ts_3600s').collect()\njno_state_data.columns\n\n\n                    INFO     Loading data from 'JNO.STATE.primary_data_ts_3600s'                data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n\n\n\n\n[\n    'radial_distance',\n    'plasma_density',\n    'plasma_temperature',\n    'time',\n    'model_b_r',\n    'model_b_t',\n    'model_b_n',\n    'v_x',\n    'v_y',\n    'v_z',\n    'plasma_speed',\n    'B_background_x',\n    'B_background_y',\n    'B_background_z'\n]\n\n\n\n\n\nCode\nfrom datetime import timedelta\n\ntime_column = \"time\"\nts = timedelta(seconds=3600)\nevery = timedelta(days=3)\n###\nwidth = 1600\nheight = 600\n\nlogy = True\nalpha = 0.618\n###\n\nmodel_b_col = \"MSWIM2D\"\njuno_b_col = \"Juno\"\n\ndf = (\n    jno_state_data.with_columns(\n        model_b=pl_norm(\"model_b_r\", \"model_b_t\", \"model_b_n\"),\n        B_background=pl_norm(\"B_background_x\", \"B_background_y\", \"B_background_z\"),\n    )\n    .sort(time_column)\n    .upsample(time_column, every=ts)\n    .rename({\"model_b\": model_b_col, \"B_background\": juno_b_col})\n)\n\ndf_avg = df.group_by_dynamic(time_column, every=every).agg(cs.numeric().mean())\n\npanel01 = df_avg.hvplot(\n    x=time_column,\n    y=[model_b_col, juno_b_col],\n    logy=logy,\n    color=[\"red\", \"black\"],\n    ylabel=\"Magnetic Field (nT)\",\n    width=width,\n    height=height,\n    xaxis=None,\n) * df.hvplot(\n    x=time_column,\n    y=juno_b_col,\n    color=\"gray\",\n    alpha=alpha,\n    label=\"Juno (high res)\",\n    xaxis=None,\n)\n\npanel02 = df.hvplot(\n    x=time_column, y=\"radial_distance\", width=width, ylabel=\"Radial Distance (AU)\"\n)\n\n\n(panel01.opts(legend_position=\"top_right\") + panel02).cols(1)\n\n\n                    WARNING  FutureWarning: Series.__getitem__ treating keys as positions is        warnings.py:109\n                             deprecated. In a future version, integer keys will always be treated                  \n                             as labels (consistent with DataFrame behavior). To access a value by                  \n                             position, use `ser.iloc[pos]`                                                         \n                                                                                                                   \n\n\n\n                    WARNING  FutureWarning: Series.__getitem__ treating keys as positions is        warnings.py:109\n                             deprecated. In a future version, integer keys will always be treated                  \n                             as labels (consistent with DataFrame behavior). To access a value by                  \n                             position, use `ser.iloc[pos]`                                                         \n                                                                                                                   \n\n\n\n                    WARNING  FutureWarning: Series.__getitem__ treating keys as positions is        warnings.py:109\n                             deprecated. In a future version, integer keys will always be treated                  \n                             as labels (consistent with DataFrame behavior). To access a value by                  \n                             position, use `ser.iloc[pos]`                                                         \n                                                                                                                   \n\n\n\n                    WARNING  FutureWarning: Series.__getitem__ treating keys as positions is        warnings.py:109\n                             deprecated. In a future version, integer keys will always be treated                  \n                             as labels (consistent with DataFrame behavior). To access a value by                  \n                             position, use `ser.iloc[pos]`                                                         \n                                                                                                                   \n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\nCode\ntime_column &lt;- \"time\"\nmodel_b_col &lt;- \"MSWIM2D\"\njuno_b_col &lt;- \"Juno\"\n\nalpha &lt;- 0.3\n\nfilename &lt;- \"model/juno_model_validation\"\n\n\nmagnetic_field_plot &lt;- ggplot(data = df_avg, aes(x = .data[[time_column]])) +\n  geom_line(aes(y = .data[[model_b_col]], color = \"MSWIM2D\")) +\n  geom_line(aes(y = .data[[juno_b_col]], color = \"Juno\")) +\n  geom_line(data = df, aes(y = .data[[juno_b_col]], color = \"Juno (high res)\"), alpha = alpha) +\n  scale_y_log10() +\n  labs(y = \"Magnetic Field (nT)\", x=\"Time\", color=NULL) +\n  theme_pubr(base_size = 16, ) +\n  scale_color_okabeito(palette = \"black_first\")\n\nxmin &lt;- as.POSIXct(\"2015-01-01\")\nxmax &lt;- as.POSIXct(\"2015-12-31\")\n\np2 &lt;- magnetic_field_plot + coord_cartesian(xlim = c(xmin, xmax)) + theme_transparent() + theme(legend.position = \"none\")\nmagnetic_field_plot + inset_element(p2, 0.2, 0, 0.8, 0.4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggmagnify)\n\nmagnetic_field_plot &lt;- ggplot(data = df_avg, aes(x = as.numeric(.data[[time_column]]))) +\n  geom_line(aes(y = .data[[model_b_col]], color = \"MSWIM2D\")) +\n  geom_line(aes(y = .data[[juno_b_col]], color = \"Juno\")) +\n  geom_line(data = df, aes(y = .data[[juno_b_col]], color = \"Juno (high res)\"), alpha = alpha) +\n  scale_y_log10() +\n  labs(y = \"Magnetic Field (nT)\", color=NULL) +\n  theme_pubr(base_size = 16) +\n  scale_color_okabeito(palette = \"black_first\")\n\n# Computation failed in `stat_magnify()`                                              \n# ! no applicable method for 'find_bounds' applied to an object of class \"c('POSIXct', 'POSIXt')\"\n\nfrom &lt;- c(xmin = as.numeric(as.POSIXct(\"2013-10-20\")), xmax = as.numeric(as.POSIXct(\"2014-03-15\")), ymin = 0.3, ymax = 10)\nto &lt;- c(xmin = as.numeric(as.POSIXct(\"2013-01-01\")), xmax = as.numeric(as.POSIXct(\"2015-01-01\")), ymin = 0.015, ymax = 0.1)\n\np1 &lt;- magnetic_field_plot + geom_magnify(from = from, to = to, recompute = TRUE, axes=\"y\")  + \n  guides(color = guide_legend(nrow = 1)) +\n  theme(\n    axis.title.x = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank(),\n    legend.position = c(0.5, 1),   # Center top inside the plot\n    legend.text = element_text(size = 16)\n  )\n\n# Create the radial distance plot (similar to panel02)\np2 &lt;- ggplot(df, aes(x = .data[[time_column]], y = radial_distance)) +\n  geom_line(linewidth=1) +\n  labs(x=\"Time\", y = \"Radial Distance (AU)\") +\n  theme_pubr(base_size = 16)\n\np &lt;- p1 + p2 + plot_layout(heights = c(3, 1))\n\nsave_plot(filename)\nprint(p)\n\n\nSaving 16.7 x 6.67 in image\nSaving 16.7 x 6.67 in image\n\n\nWarning: stack imbalance in 'lapply', 112 then 111\nIn addition: There were 33 warnings (use warnings() to see them)",
    "crumbs": [
      "Results",
      "Models"
    ]
  },
  {
    "objectID": "analysis/20_model.html#compare-juno-data-with-model",
    "href": "analysis/20_model.html#compare-juno-data-with-model",
    "title": "Models",
    "section": "Compare JUNO data with model",
    "text": "Compare JUNO data with model\nWe are using juno 1min data to compare with model data",
    "crumbs": [
      "Results",
      "Models"
    ]
  },
  {
    "objectID": "analysis/20_model.html#validation",
    "href": "analysis/20_model.html#validation",
    "title": "Models",
    "section": "Validation",
    "text": "Validation\n\nConnect python with R kernel\n\n\nCode\nfig = px.line(\n    jno_joint_1h_long.sort(\"time\"),\n    x=\"time\",\n    y=\"b\",\n    color=\"type\",\n)\nfig\n\n\n\n\nCompare directly with scatter plot\n\n\nCode\ndef bb_jointplot(data):\n    g = sns.jointplot(\n        x = 'b',\n        y = 'b_model',\n        data = data,\n        kind = 'hist',\n    )\n    return g\n\n\n\n\nCode\ng = bb_jointplot(jno_joint_1h_wide)\ng.ax_marg_x.set_xlim(0, 5)\ng.ax_marg_y.set_ylim(0, 5)\n\n\n\n\nCode\np1 &lt;- ggplot(jno_joint_1h_wide, aes(x=b, y=b_model) ) +\n  geom_bin2d() +\n  geom_density_2d( colour=\"white\" ) +\n  scale_fill_continuous(trans=\"log\", type = \"viridis\") +\n  stat_regline_equation() + \n  xlim(-0.1, 10) +  # Set x-axis limits\n  ylim(-0.1, 10) +  # Set y-axis limits\n  theme_pubr(legend = 'right')\n  # theme(legend.position = c(0.8,0.8))\n  # stat_density_2d(aes(fill = ..level..), geom = \"polygon\", colour=\"white\")\n\np1\n\n\n\nTest: remove outliers\n\n\nCode\nfrom pyod.models.ecod import ECOD\n\n\n\n\nCode\ndata = jno_joint_1h_wide[['b', 'b_model']]\n\nclf = ECOD()\nclf.fit(data)\n\n\n\n\nCode\ny_train_scores = clf.decision_scores_  # raw outlier scores on the train data\ny_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n\n\n\n\nCode\nbb_jointplot(jno_joint_1h_wide.filter(y_train_pred==1))\ndata = jno_joint_1h_wide.filter(y_train_pred==0)\ng = sns.jointplot(\n    x = 'b',\n    y = 'b_model',\n    data = data,\n    kind = 'hist',\n)\n\ng.plot_joint(sns.kdeplot, color=\"r\", zorder=0, levels=6)\n\n\n\n\nCode\n# def create_pipeline(**kwargs) -&gt; Pipeline:\n    # return create_jno_model_pipeline(**kwargs) + create_jno_data_pipeline(**kwargs)\n\n\n\n\nData Porfiling\nResults are showed in the following links\nTimeseries Report Result\nComparison Report Result\n\n\nCode\nfrom ydata_profiling import ProfileReport, compare\n\n\n\n\nCode\nfrom fastcore.utils import threaded\n\n\n\n\nCode\n@threaded\ndef get_report_t(df: pl.DataFrame, output, **kwargs):\n    '''get report and save to file in a thread\n    '''\n    get_report(df, **kwargs).to_file(output)\n    return output\n\ndef get_report(df: pl.DataFrame, **kwargs):\n    return ProfileReport(\n        df.to_pandas().set_index(\"time\"), **kwargs\n    )\n\ndef get_comparison_report(df: pl.DataFrame, compare_col=None, tsmode=False, **kwargs):\n    \n    dfs_dict: Dict[str, pl.DataFrame] = df.partition_by(compare_col, as_dict=True)\n    \n    if tsmode:\n        raise NotImplementedError(\"tsmode for comparison is not implemented yet in `ydata_profiling`\")\n        # UnionMatchError: can not match type \"list\" to any type of \"time_index_analysis.period\" union: typing.Union[float,  \n        \n        # Notes: for `tsmode`, we need to match the time first\n        # select common timestamps\n        from functools import reduce\n        basetimestamps = reduce(np.intersect1d, [df.get_column('time') for df in dfs_dict.values()])\n        dfs_dict = {\n            k: df.filter(pl.col(\"time\").is_in(basetimestamps))\n            for k, df in dfs_dict.items()\n        }\n\n        for k, df in dfs_dict.items():\n            logger.info(f\"{k}: {len(df)}\")\n    \n    comparison_report = compare(\n        [get_report(df, title=k, **kwargs) for k, df in dfs_dict.items()]\n    )\n    \n    # Obtain merged statistics\n    comparison_report.get_description()\n\n    return comparison_report\n\n\n\n\nCode\nget_report_t(\n    jno_joint_1h_wide,\n    output=\"jno_model_ts.html\",\n    tsmode=True,\n    title=\"JUNO Model Timeseries Report\",\n)\n\n\n\n\nCode\nget_comparison_report(jno_joint_1h_long, compare_col=\"type\").to_file(\n    \"jno_model_comparison.html\"\n)",
    "crumbs": [
      "Results",
      "Models"
    ]
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Results",
    "section": "",
    "text": "k Orientaion\n\n\n\n\n\n\n\n\n\n\ndB Orientaion\n\n\n\n\n\nVl Orientaion",
    "crumbs": [
      "Results"
    ]
  },
  {
    "objectID": "analysis/index.html#orientation",
    "href": "analysis/index.html#orientation",
    "title": "Results",
    "section": "",
    "text": "k Orientaion\n\n\n\n\n\n\n\n\n\n\ndB Orientaion\n\n\n\n\n\nVl Orientaion",
    "crumbs": [
      "Results"
    ]
  },
  {
    "objectID": "analysis/index.html#intensity",
    "href": "analysis/index.html#intensity",
    "title": "Results",
    "section": "Intensity",
    "text": "Intensity\n\n\n\n\n\n\n\n\n\n\nNormalized by mean magnitude\n\n\n\n\n\nFigure 1: Insensity with time and radial distance",
    "crumbs": [
      "Results"
    ]
  },
  {
    "objectID": "analysis/index.html#thickness",
    "href": "analysis/index.html#thickness",
    "title": "Results",
    "section": "Thickness",
    "text": "Thickness\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Thickness with time and radial distance\n\n\n\n\n\n\n\n\n\n\nMN Thickness\n\n\n\n\n\nK Thickness\n\n\n\n\n\nMN Normalized Thickness\n\n\n\n\n\nK Normalized Thickness\n\n\n\n\n\nFigure 3: Thickness distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Thickness distribution with radial distance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Thickness with radial distance for different definitions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Thickness distribution with radial distance for different definitions",
    "crumbs": [
      "Results"
    ]
  },
  {
    "objectID": "analysis/index.html#current",
    "href": "analysis/index.html#current",
    "title": "Results",
    "section": "Current",
    "text": "Current\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Current density with time and radial distance\n\n\n\n\n\n\n\n\n\n\nMN current\n\n\n\n\n\nK current\n\n\n\n\n\nMN Normalized current\n\n\n\n\n\nK Normalized current\n\n\n\n\n\nFigure 8: Current distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: JUNO Current distribution with radial distance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Thickness distribution with radial distance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: Current distribution with radial distance",
    "crumbs": [
      "Results"
    ]
  },
  {
    "objectID": "analysis/00_base.html",
    "href": "analysis/00_base.html",
    "title": "Loading all datasets from different sources",
    "section": "",
    "text": "Code\nimport polars as pl\nimport polars.selectors as cs\nimport pandas as pd\nimport numpy as np\n\nfrom loguru import logger\nCode\nfrom ids_finder.utils.basic import load_catalog\n\ncatalog = load_catalog()\nCode\nfrom ids_finder.datasets import cIDsDataset\n\nsta_dataset = cIDsDataset(sat_id=\"STA\", tau=60, ts=1, catalog=catalog)\njno_dataset = cIDsDataset(sat_id=\"JNO\", tau=60, ts=1, catalog=catalog)\nthb_dataset = cIDsDataset(sat_id=\"THB\", tau=60, ts=1, catalog=catalog)\n\n\n16-Nov-23 23:42:22 INFO     16-Nov-23 23:42:22: Loading data from 'events.STA_ts_1s_tau_60s'    data_catalog.py:502\n                            (LazyPolarsDataset)...                                                                 \n\n\n\n                   INFO     16-Nov-23 23:42:22: Loading data from 'STA.MAG.primary_data_ts_1s'  data_catalog.py:502\n                            (PartitionedDataset)...                                                                \n\n\n\n                   INFO     16-Nov-23 23:42:22: Loading data from 'events.JNO_ts_1s_tau_60s'    data_catalog.py:502\n                            (LazyPolarsDataset)...                                                                 \n\n\n\n                   INFO     16-Nov-23 23:42:22: Loading data from 'JNO.MAG.primary_data_ts_1s'  data_catalog.py:502\n                            (PartitionedDataset)...                                                                \n\n\n\n                   INFO     16-Nov-23 23:42:22: Loading data from 'events.THB_ts_1s_tau_60s'    data_catalog.py:502\n                            (LazyPolarsDataset)...                                                                 \n\n\n\n                   INFO     16-Nov-23 23:42:22: Loading data from 'THB.MAG.primary_data_ts_1s'  data_catalog.py:502\n                            (PartitionedDataset)...\nCode\nfrom beforerr.basics import pmap\nfrom ids_finder.utils.analysis import filter_tranges_ds\nCode\nthb_inter_state_sw: pl.LazyFrame = catalog.load('thb.inter_state_sw')\nstart, end = thb_inter_state_sw.select(['start', 'end']).collect()\n\nthb_sw_dataset = filter_tranges_ds(thb_dataset, (start, end))\n\n\n[11/13/23 20:28:03] INFO     Loading data from 'thb.inter_state_sw' (LazyPolarsDataset)...      data_catalog.py:502\nCode\nall_datasets = [sta_dataset, jno_dataset, thb_sw_dataset]\nCode\nall_candidates_l0 : pl.DataFrame = pl.concat(\n    all_datasets | pmap(lambda x: x.candidates),\n    how=\"diagonal\",\n)\nCode\ndef combine_candidates(datasets):\n    return pl.concat(\n        datasets | pmap(lambda x: x.candidates),\n        how=\"diagonal\",\n    )",
    "crumbs": [
      "Results",
      "Loading all datasets from different sources"
    ]
  },
  {
    "objectID": "analysis/00_base.html#processing-datasets",
    "href": "analysis/00_base.html#processing-datasets",
    "title": "Loading all datasets from different sources",
    "section": "Processing datasets",
    "text": "Processing datasets\nSome extreme values are present in the data. We will remove them.\n\n\nCode\nNVARS = ['d_star', 'L_mn', 'L_mn_norm', 'j0', 'j0_norm', 'duration', 'v_mn']\nDISPLAY_VARS = ['time', 'sat'] + NVARS\n\n\ndef check_candidates(df):\n    return df[NVARS].describe()\n\ncheck_candidates(all_candidates_l0)\n\n\n\n\n\n\nshape: (9, 8)\n\n\n\ndescribe\nd_star\nL_mn\nL_mn_norm\nj0\nj0_norm\nduration\nv_mn\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"count\"\n185066.0\n185066.0\n185066.0\n185066.0\n185066.0\n\"185066\"\n185066.0\n\n\n\"null_count\"\n0.0\n4120.0\n4389.0\n4120.0\n4389.0\n\"0\"\n4120.0\n\n\n\"mean\"\n2.611712\n2798.843381\n22.307474\n11.654787\n4.713652\n\"0:00:08.198437…\n343.811034\n\n\n\"std\"\n491.756741\n2179.474212\n20.649185\n2894.040891\n1473.838227\nnull\n99.930132\n\n\n\"min\"\n0.019601\n3.381065\n0.014144\n0.0561\n0.00082\n\"0:00:01.999999…\n0.41411\n\n\n\"25%\"\n0.247087\n1582.102536\n11.284664\n0.601477\n0.028203\n\"0:00:05\"\n286.126017\n\n\n\"50%\"\n0.510951\n2240.279834\n17.513617\n1.239019\n0.051221\n\"0:00:07\"\n343.325961\n\n\n\"75%\"\n0.983944\n3346.020528\n27.236719\n2.34897\n0.091488\n\"0:00:10\"\n402.282733\n\n\n\"max\"\n152023.367594\n103745.212024\n1614.132093\n1.1500e6\n583059.205803\n\"0:03:16\"\n864.604665\n\n\n\n\n\n\n\n\nCode\nfrom datetime import timedelta\ndef process_candidates_l1(raw_df: pl.DataFrame):\n    \"clean data to remove extreme values\"\n\n    df = raw_df.filter(\n        pl.col(\"d_star\") &lt; 100, # exclude JUNO extreme values\n        pl.col('v_mn') &gt; 10,\n        pl.col('duration') &lt; timedelta(seconds=60),\n        # pl.col(\"j0\") &lt; 100\n    ).with_columns(\n        pl.col('radial_distance').fill_null(1) # by default, fill with 1 AU\n    ).with_columns(\n        r_bin = pl.col('radial_distance').round(),\n        j0_norm_log = pl.col('j0_norm').log10(),\n        L_mn_norm_log = pl.col('L_mn_norm').log10(),\n    )\n\n    logger.info(\n        f\"candidates_l1: {len(df)}, with effective ratio: {len(df) / len(raw_df):.2%}\"\n    )\n\n    return df\n\nall_candidates_l1 = process_candidates_l1(all_candidates_l0)\n\ncheck_candidates(all_candidates_l1)\n\n\n2023-11-08 14:11:23.225 | INFO     | __main__:process_candidates_l1:18 - candidates_l1: 180718, with effective ratio: 97.65%\n\n\n\n\n\n\nshape: (9, 8)\n\n\n\ndescribe\nd_star\nL_mn\nL_mn_norm\nj0\nj0_norm\nduration\nv_mn\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"count\"\n180718.0\n180718.0\n180718.0\n180718.0\n180718.0\n\"180718\"\n180718.0\n\n\n\"null_count\"\n0.0\n0.0\n264.0\n0.0\n264.0\n\"0\"\n0.0\n\n\n\"mean\"\n0.745751\n2768.506268\n22.033678\n1.865352\n0.075518\n\"0:00:08.118150…\n343.880697\n\n\n\"std\"\n0.771981\n1909.065522\n17.629565\n2.599027\n0.097857\nnull\n99.846681\n\n\n\"min\"\n0.019601\n48.94197\n0.124168\n0.0561\n0.00082\n\"0:00:01.999999…\n10.240242\n\n\n\"25%\"\n0.243875\n1581.58393\n11.279769\n0.60174\n0.028229\n\"0:00:05\"\n286.190021\n\n\n\"50%\"\n0.50421\n2238.553736\n17.499906\n1.239576\n0.051251\n\"0:00:07\"\n343.360031\n\n\n\"75%\"\n0.97075\n3340.552536\n27.186555\n2.348456\n0.091501\n\"0:00:10\"\n402.301723\n\n\n\"max\"\n13.805873\n35975.767016\n439.323024\n393.479096\n9.634978\n\"0:00:59\"\n864.604665\n\n\n\n\n\n\n\n\nCode\njno_candidates_l1 = all_candidates_l1.filter(pl.col('sat') == 'JNO')\n\n\n\n\nCode\nfrom ids_finder.utils.analysis import filter_before_jupiter\nfrom ids_finder.utils.analysis import link_coord2dim\n\n\n\n\nCode\ndef process_candidates_l2(raw_df: pl.DataFrame, avg_window=\"30d\"):\n    time_col = \"time\"\n\n    candidate = (\n        raw_df.sort(time_col)\n        .group_by_dynamic(time_col, every=avg_window, by=\"sat\")\n        .agg(cs.numeric().mean(), cs.duration().mean(), id_count=pl.count())\n        .filter(pl.col(\"id_count\") &gt; 50)  # filter out JUNO extreme large thickness\n        .sort(time_col)\n        .upsample(time_col, every=avg_window, by=\"sat\", maintain_order=True)\n        .with_columns(pl.col(\"sat\").forward_fill())\n    )\n    return candidate\n\n\n\n\nCode\nall_candidates_l2: pl.DataFrame = (\n    all_candidates_l1.pipe(filter_before_jupiter)\n    .pipe(process_candidates_l2)\n    .pipe(link_coord2dim)\n)\n\n\n\n\nCode\ninspect_df = all_candidates_l2[NVARS]\ninspect_df.describe()\n\n\n\n\n\n\nshape: (9, 8)\n\n\n\ndescribe\nd_star\nL_mn\nL_mn_norm\nj0\nj0_norm\nduration\nv_mn\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"count\"\n172.0\n172.0\n172.0\n172.0\n172.0\n\"172\"\n172.0\n\n\n\"null_count\"\n19.0\n19.0\n19.0\n19.0\n19.0\n\"19\"\n19.0\n\n\n\"mean\"\n0.706261\n2922.959632\n22.028999\n1.937378\n0.090728\n\"0:00:08.719631…\n337.428018\n\n\n\"std\"\n0.358616\n512.032439\n8.140589\n1.077249\n0.051647\nnull\n37.917741\n\n\n\"min\"\n0.108318\n1877.983131\n7.074407\n0.229362\n0.042024\n\"0:00:06.751012…\n256.771354\n\n\n\"25%\"\n0.331532\n2590.280777\n14.498058\n0.795284\n0.060267\n\"0:00:07.707419…\n315.324913\n\n\n\"50%\"\n0.794667\n2786.745403\n22.804505\n2.087583\n0.069789\n\"0:00:08.730158…\n335.332916\n\n\n\"75%\"\n0.931735\n3182.843841\n27.726721\n2.633037\n0.094061\n\"0:00:09.315238…\n359.837854\n\n\n\"max\"\n1.539393\n4458.507484\n41.436617\n4.784021\n0.306938\n\"0:00:12.305699…\n445.849288\n\n\n\n\n\n\n\n\nCode\nfrom ids_finder.utils.analysis import n2_normalize\n\nall_candidates_l2_n2 = n2_normalize(all_candidates_l2, NVARS)",
    "crumbs": [
      "Results",
      "Loading all datasets from different sources"
    ]
  },
  {
    "objectID": "manuscripts/paper.html",
    "href": "manuscripts/paper.html",
    "title": "Discontinuities? Yes!",
    "section": "",
    "text": "‘Discontinuities’ are discontinuous spatial changes in plasma parameters/characteristics and magnetic fields (Colburn and Sonett 1966).\nThey are observed across the heliosphere from inner heliosphere (Liu, Fu, Cao, Yu, et al. 2022) to the heliosheath (Burlaga and Ness 2011).\n\n\n\n\n\n\n\nCurrent sheets in the heliosheath: Voyager 1, 2009\n\n\n\n\n\nA rotational discontinuity (RD) detected at 0.126 AU\n\n\n\n\n\nFigure 1: Discontinuities examples across the heliosphere\n\n\n\nSöding et al. (2001) studied the radial distribution of discontinuities in the solar wind between 0.3 and 19 AU during solar activity minimum. Liu et al. (2021) studied the spatial evolution of the discontinuities from 0.13-0.9 AU, using measurements from the Parker Solar Probe. Typical rate of occurrence is about 50 per day at 1AU\n\n\n\n\n\n\n\nDiscontinuities between 0.3 and 19 AU\n\n\n\n\n\nDiscontinuities between 0.13–0.9 AU\n\n\n\n\n\nFigure 2: Spatial distribution of the discontinuities\n\n\n\n\n\n\nContribution of Strong Discontinuities to the Power Spectrum\n\nThe strong discontinuities produce a power-law spectrum in the ‘inertial subrange’ with a spectral index near the Kolmogorov -5/3 index. The discontinuity spectrum contains about half of the power of the full solar-wind magnetic ﬁeld over this ‘inertial subrange’. (Borovsky 2010)\n\n\n\n\n\nStudying the radial distribution of occurrence rate, as well as the properties of solar wind discontinuities may help answer the following questions:\n\nHow does the discontinuities change with the radial distance from the Sun?\nHow is solar wind discontinuities formed? What is the physical mechanisms?\n\nGenerated at or near the sun?\nLocally generated in the interplanetary space by turbulence?\n\n\nJoint observations of JUNO & ARTEMIS & Other missions really provides a unique opportunity!!!\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Juno Spacecraft Cruise Trajectory\n\n\n\n\nFive-year cruise to Jupiter from 2011 to 2016\nOne earth flyby in 2013\nNearly the same Heliographic latitude as Earth\n\nTo eliminate the effect of the solar wind structure, we use data from other missions (mainly at 1AU) to provide a way of normalization.\n\n\n\nMission\nr [AU]\n\\(\\delta t_B\\)\n\\(\\delta t_{plasma}\\)\nData availability\n\n\n\n\nJUNO\n1-5.5\n1s averaged (64 Hz)\n1h model *\n2011 - 2016 - Today\n\n\nARTEMIS\n1\n1s averaged (8 Hz)\n1h averaged\n2009 - Today (solar wind)\n\n\nSTEREO-A\n1\n1s averaged (8 Hz)\n1h averaged\n2006 - Today\n\n\nSTEREO-B\n1\n1s averaged (8 Hz)\n1h averaged\n2006 - 2016.09\n\n\nWind\n1\n11 Hz\n1h averaged\n1994 - 2004 -2020 - Today\n\n\nSolar Orbiter\n0.28-0.91\n\n\n2020 - Today\n\n\nUlysis",
    "crumbs": [
      "Manuscripts",
      "Discontinuities? Yes!"
    ]
  },
  {
    "objectID": "manuscripts/paper.html#background",
    "href": "manuscripts/paper.html#background",
    "title": "Discontinuities? Yes!",
    "section": "",
    "text": "‘Discontinuities’ are discontinuous spatial changes in plasma parameters/characteristics and magnetic fields (Colburn and Sonett 1966).\nThey are observed across the heliosphere from inner heliosphere (Liu, Fu, Cao, Yu, et al. 2022) to the heliosheath (Burlaga and Ness 2011).\n\n\n\n\n\n\n\nCurrent sheets in the heliosheath: Voyager 1, 2009\n\n\n\n\n\nA rotational discontinuity (RD) detected at 0.126 AU\n\n\n\n\n\nFigure 1: Discontinuities examples across the heliosphere\n\n\n\nSöding et al. (2001) studied the radial distribution of discontinuities in the solar wind between 0.3 and 19 AU during solar activity minimum. Liu et al. (2021) studied the spatial evolution of the discontinuities from 0.13-0.9 AU, using measurements from the Parker Solar Probe. Typical rate of occurrence is about 50 per day at 1AU\n\n\n\n\n\n\n\nDiscontinuities between 0.3 and 19 AU\n\n\n\n\n\nDiscontinuities between 0.13–0.9 AU\n\n\n\n\n\nFigure 2: Spatial distribution of the discontinuities\n\n\n\n\n\n\nContribution of Strong Discontinuities to the Power Spectrum\n\nThe strong discontinuities produce a power-law spectrum in the ‘inertial subrange’ with a spectral index near the Kolmogorov -5/3 index. The discontinuity spectrum contains about half of the power of the full solar-wind magnetic ﬁeld over this ‘inertial subrange’. (Borovsky 2010)\n\n\n\n\n\nStudying the radial distribution of occurrence rate, as well as the properties of solar wind discontinuities may help answer the following questions:\n\nHow does the discontinuities change with the radial distance from the Sun?\nHow is solar wind discontinuities formed? What is the physical mechanisms?\n\nGenerated at or near the sun?\nLocally generated in the interplanetary space by turbulence?\n\n\nJoint observations of JUNO & ARTEMIS & Other missions really provides a unique opportunity!!!\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Juno Spacecraft Cruise Trajectory\n\n\n\n\nFive-year cruise to Jupiter from 2011 to 2016\nOne earth flyby in 2013\nNearly the same Heliographic latitude as Earth\n\nTo eliminate the effect of the solar wind structure, we use data from other missions (mainly at 1AU) to provide a way of normalization.\n\n\n\nMission\nr [AU]\n\\(\\delta t_B\\)\n\\(\\delta t_{plasma}\\)\nData availability\n\n\n\n\nJUNO\n1-5.5\n1s averaged (64 Hz)\n1h model *\n2011 - 2016 - Today\n\n\nARTEMIS\n1\n1s averaged (8 Hz)\n1h averaged\n2009 - Today (solar wind)\n\n\nSTEREO-A\n1\n1s averaged (8 Hz)\n1h averaged\n2006 - Today\n\n\nSTEREO-B\n1\n1s averaged (8 Hz)\n1h averaged\n2006 - 2016.09\n\n\nWind\n1\n11 Hz\n1h averaged\n1994 - 2004 -2020 - Today\n\n\nSolar Orbiter\n0.28-0.91\n\n\n2020 - Today\n\n\nUlysis",
    "crumbs": [
      "Manuscripts",
      "Discontinuities? Yes!"
    ]
  },
  {
    "objectID": "manuscripts/paper.html#methods",
    "href": "manuscripts/paper.html#methods",
    "title": "Discontinuities? Yes!",
    "section": "Methods",
    "text": "Methods\n\nWe use (Liu, Fu, Cao, Wang, et al. 2022) method to identify IDs, which has better compatibility for the IDs with minor field changes.\nThen the minimum variance analysis is applied to each ID event to obtain the boundary normal (LMN) coordinate and extract IDs’ features.\n\n\nID identification (limited feature extraction / anomaly detection)\nTraditional methods for ID identiﬁcation, such as the criteria of\n\nBurlaga & Ness (1969; B-criterion) : a directional change of the magnetic ﬁeld larger than 30° during 60 s\nTsurutani & Smith (1979; TS-criterion) : \\(|ΔB|/|B| \\geq 0.5\\) within 3 minutes\n\nMostly rely on magnetic ﬁeld variations with a certain time lag. B-criterion has, as its main condition.\nIn their methods, the IDs below the thresholds are artiﬁcially abandoned. Therefore, identiﬁcation criteria may affect the statistical results, and there is likely to be a discrepancy between the ﬁndings via B-criterion and TS- criterion.\nLiu’s method : The first two conditions guarantee that the field changes of the IDs identiﬁed are large enough to be distinguished from the stochastic fluctuations on magnetic fields, while the third is a supplementary condition to reduce the uncertainty of recognition.\n\\[ \\textrm{Index}_1 = \\frac{\\sigma(\\vec{B})}{Max(\\sigma(\\vec{B}_-),\\sigma(\\vec{B}_+))} \\]\n\\[ \\textrm{Index}_2 = \\frac{\\sigma(\\vec{B}_- + \\vec{B}_+)} {\\sigma(\\vec{B}_-) + \\sigma(\\vec{B}_+)} \\]\n\\[ \\textrm{Index}_3 = \\frac{| \\Delta \\vec{B} |}{|B_{bg}|} \\]\n\\[ \\textrm{Index}_1 \\ge 2, \\textrm{Index}_2 \\ge 1, \\textrm{Index}_3 \\ge 0.1 \\]\n\n\nSolar Wind Model\nSadly, JUNO does not provide plasma data during the cruise phase, so to estimate the plasma state we will use MHD model.\nWe are using Michigan Solar WInd Model 2D (MSWIM2D), which models the solar wind propagation in 2D using the BATSRUS MHD solver. (Keebler et al. 2022)\nSome key points about the model\n\nRepresenting the solar wind in the ecliptic plane from 1 to 75 AU\n2D MHD model, using the BATSRUS MHD solver\nInclusion of neutral hydrogen (important for the outer heliosphere)\nInner boundary is filled by time-shifting in situ data from multiple spacecraft\n\nFor model validation part, please see JUNO Model Report.",
    "crumbs": [
      "Manuscripts",
      "Discontinuities? Yes!"
    ]
  },
  {
    "objectID": "manuscripts/paper.html#results",
    "href": "manuscripts/paper.html#results",
    "title": "Discontinuities? Yes!",
    "section": "Results",
    "text": "Results\nFor all results, see results page.\n\nOccurrence rates\nFor code, see noteboook.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Occurrence rates with time\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Occurrence rates with radial distance\n\n\n\n\n\nProperties\n\n\n\n\n\n\n\nMN thickness\n\n\n\n\n\nMN current\n\n\n\n\n\n\n\nMN normalized thickness\n\n\n\n\n\nMN normalized current\n\n\n\n\n\nFigure 6: Thickness and current density distribution",
    "crumbs": [
      "Manuscripts",
      "Discontinuities? Yes!"
    ]
  },
  {
    "objectID": "manuscripts/paper.html#conclusion",
    "href": "manuscripts/paper.html#conclusion",
    "title": "Discontinuities? Yes!",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe have collected 5 years of solar wind discontinuities from JUNO, ARTEMIS and STEREO.\nWe have developed a pipeline to identify solar wind discontinuities. (Modular, Performant, Scalable)\nThe normalized occurrence rate of IDs drops with the radial distance from the Sun, following \\(1/r\\) law.\nThe thickness of IDs increases with the radial distance from the Sun, but after normalization to ion inertial length, the thickness of IDs decreases.\nThe current intensity of IDs decrease with the radial distance from the Sun, but after normalization to the Alfven current , the current intensity of IDs increases.",
    "crumbs": [
      "Manuscripts",
      "Discontinuities? Yes!"
    ]
  },
  {
    "objectID": "manuscripts/paper.html#todos",
    "href": "manuscripts/paper.html#todos",
    "title": "Discontinuities? Yes!",
    "section": "TODOs",
    "text": "TODOs\nScience part\n\nAnalysis\n\nCheck STEREO-A and ARTEMIS-B data\nContribution of discontinuities to the power spectrum\nCheck Datagap\nCheck ARTEMIS-B data in different states (solar wind, magnetosheath, magnetotail, moon wake)\nDistribution of |B| over radius\nJUNO from 2012-09~2012-10 lack of IDS and extreme large thickness\nWind data\nAdd error bar\nValidate the effects of calibrate candidate duration\nValidate model density with Voyager\n\nIdentifaction\n\nEnsemble forest?\nSmoothing is important?\nCheck change point algorithm\n\nVisualize data gaps\nFeatures\n\nThickness in N direction\nUse high resolution data for feature extraction\n\nCompare with other methods of identifying IDs\n\nVerify with other methods of identifying IDs\n\nIncorporate solar wind propagation model\n\nVerify with solar wind propagation model\n\nCoordinate transformation\n\n\n\nCode part\n\nOptimization\n\nJAX library for numpy optimization\nshorten import time\n\nRefactor\n\nprocess_candidates to exclude sat_state logics\nrenaming feature layer candidates\n\nKedro\n\nModular pipelines\nIncorporate lineapy\n\nQR codes\n\n\nbugs\n\nJUNO sw_temperature type\nSTEREO B less than zero (after downsampling?)",
    "crumbs": [
      "Manuscripts",
      "Discontinuities? Yes!"
    ]
  },
  {
    "objectID": "manuscripts/AGU23_poster.html#main-findings",
    "href": "manuscripts/AGU23_poster.html#main-findings",
    "title": "Solar wind discontinuities spatial evolution and energetic ion scattering",
    "section": "Main findings",
    "text": "Main findings\n\nSolar wind discontinuities evolve in space with occurrence rate decreasing, thickness increasing and current density decreasing with distance from the Sun. And they are probably generated locally beyond 1 AU.\nBackground sheared magnetic field plays an important role in determining the efficiency of ion pitch angle scattering, and characterize three ion populations: transient, trapped, regular.",
    "crumbs": [
      "Manuscripts",
      "Solar wind discontinuities spatial evolution and energetic ion scattering"
    ]
  },
  {
    "objectID": "manuscripts/AGU23_poster.html#introduction-motivation",
    "href": "manuscripts/AGU23_poster.html#introduction-motivation",
    "title": "Solar wind discontinuities spatial evolution and energetic ion scattering",
    "section": "Introduction & Motivation",
    "text": "Introduction & Motivation\n‘Discontinuities’ are discontinuous spatial changes in plasma parameters/characteristics and magnetic fields (Colburn and Sonett 1966).\n \nSöding et al. (2001) studied the radial distribution of discontinuities in the solar wind.\n \nJoint observations of JUNO & ARTEMIS & Other missions really provides a unique opportunity!!!",
    "crumbs": [
      "Manuscripts",
      "Solar wind discontinuities spatial evolution and energetic ion scattering"
    ]
  },
  {
    "objectID": "manuscripts/AGU23_poster.html#method",
    "href": "manuscripts/AGU23_poster.html#method",
    "title": "Solar wind discontinuities spatial evolution and energetic ion scattering",
    "section": "Method",
    "text": "Method\n\nWe use (Liu et al. 2022) method to identify IDs, which has better compatibility for the IDs with minor field changes.\nThen the minimum variance analysis is applied to each ID event to obtain the boundary normal (LMN) coordinate and extract IDs’ features.\nHamiltonian model is applied for investigation of ion dynamics in the solar wind discontinuity configuration.\n\nThe most generalized form of dimensionless hamiltonian equation for ions in force-free rotational magnetic discontinuities configuration is\n\\[\nH=\\frac{1}{2}\\left(\\frac{1}{6} \\alpha^2 c_2 z^3-c_1 z+p_x\\right)^2+\\frac{p_z^2}{2}+\\frac{1}{2}\\left(\\kappa x-\\frac{\\alpha z^2}{2}\\right)^2\n\\]\nWith \\(B_l^2+B_m^2=\\text { const }\\), we have\n\\[\nH=\\frac{1}{2}\\left(\\frac{\\alpha^2 z^3}{6 \\sqrt{\\kappa_m^2+1}}-z \\sqrt{\\kappa_m^2+1}+p_x\\right)^2+\\frac{p_z^2}{2}+\\frac{1}{2}\\left(\\kappa_n x-\\frac{\\alpha z^2}{2}\\right)^2\n\\]\nThe system has three parameters \\[\n\\kappa_n=\\frac{B_n}{B_{l, \\max }}\n\\quad\n\\kappa_m=\\frac{B_{m, 0}}{B_{l, \\max }}\n\\quad\n\\alpha=\\frac{l_0}{L}=\\frac{\\text { gyro radius }}{\\text { system length }}\n\\]",
    "crumbs": [
      "Manuscripts",
      "Solar wind discontinuities spatial evolution and energetic ion scattering"
    ]
  },
  {
    "objectID": "manuscripts/AGU23_poster.html#results",
    "href": "manuscripts/AGU23_poster.html#results",
    "title": "Solar wind discontinuities spatial evolution and energetic ion scattering",
    "section": "Results",
    "text": "Results\n\nNormalized occurrence rate of IDs drops with the radial distance from the Sun, following 1/r law.\n \n\n\n\n\nHigh energy particle has higher chance to cross uncertainty curve\nHigh shear magnetic field will make separatrix vanishes and the geometrical jumps of the quasiadiabatic invariant disappear",
    "crumbs": [
      "Manuscripts",
      "Solar wind discontinuities spatial evolution and energetic ion scattering"
    ]
  },
  {
    "objectID": "analysis/02_orientation.html",
    "href": "analysis/02_orientation.html",
    "title": "Orientation",
    "section": "",
    "text": "Code\nall_events_l1: pl.DataFrame = catalog.load(\"events.l1.ALL_sw_ts_1s_tau_60s\").collect()\n\ncols = [\"dB_x_norm\", \"dB_y_norm\", \"dB_z_norm\"]\nall_events_l1 = (\n    all_events_l1.with_columns(dB=pl_norm(cols))\n    .with_columns(\n        dB_x_n2=pl.col(\"dB_x_norm\") / pl.col(\"dB\"),\n        dB_y_n2=pl.col(\"dB_y_norm\") / pl.col(\"dB\"),\n        dB_z_n2=pl.col(\"dB_z_norm\") / pl.col(\"dB\"),\n    )\n    .with_columns(\n        theta_dB=pl.col(\"dB_z_n2\").arccos().degrees(),\n        phi_dB=(pl.col(\"dB_y_n2\") / pl.col(\"dB_x_n2\")).arctan().degrees(),\n    )\n)\n\nJNO_events_l1 = all_events_l1.filter(pl.col(\"sat\") == \"JNO\")\nother_events_l1 = all_events_l1.filter(pl.col(\"sat\") != \"JNO\")\nall_events_l2 = all_events_l1.pipe(process_events_l2)\n\n\n[11/30/23 13:28:08] INFO     Loading data from 'events.l1.ALL_sw_ts_1s_tau_60s'                 data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n\n\nCode\nprobs &lt;- c(0.99, 0.9, 0.7, 0.5, 0.3, 0.1)\n\nplot_plot &lt;- function(df, x, y, type = \"hdr\", facets = NULL, color = NULL) {\n  p &lt;- ggplot(df, aes(x = .data[[x]], y = .data[[y]]) ) +\n    scale_color_okabeito(palette = \"black_first\")\n\n\n  if (!is.null(facets)) {\n    p &lt;- p + facet_wrap(vars(.data[[facets]]))\n  }\n\n  if (type == \"hdr\") {\n    p &lt;- p + geom_hdr_lines(\n      aes_string(color = color),\n      probs = probs\n    )\n  } else if (type == \"density\") {\n    p &lt;- p + geom_density_2d_filled()\n  }\n\n  return (p)\n}",
    "crumbs": [
      "Results",
      "Orientation"
    ]
  },
  {
    "objectID": "analysis/02_orientation.html#setup",
    "href": "analysis/02_orientation.html#setup",
    "title": "Orientation",
    "section": "",
    "text": "Code\nall_events_l1: pl.DataFrame = catalog.load(\"events.l1.ALL_sw_ts_1s_tau_60s\").collect()\n\ncols = [\"dB_x_norm\", \"dB_y_norm\", \"dB_z_norm\"]\nall_events_l1 = (\n    all_events_l1.with_columns(dB=pl_norm(cols))\n    .with_columns(\n        dB_x_n2=pl.col(\"dB_x_norm\") / pl.col(\"dB\"),\n        dB_y_n2=pl.col(\"dB_y_norm\") / pl.col(\"dB\"),\n        dB_z_n2=pl.col(\"dB_z_norm\") / pl.col(\"dB\"),\n    )\n    .with_columns(\n        theta_dB=pl.col(\"dB_z_n2\").arccos().degrees(),\n        phi_dB=(pl.col(\"dB_y_n2\") / pl.col(\"dB_x_n2\")).arctan().degrees(),\n    )\n)\n\nJNO_events_l1 = all_events_l1.filter(pl.col(\"sat\") == \"JNO\")\nother_events_l1 = all_events_l1.filter(pl.col(\"sat\") != \"JNO\")\nall_events_l2 = all_events_l1.pipe(process_events_l2)\n\n\n[11/30/23 13:28:08] INFO     Loading data from 'events.l1.ALL_sw_ts_1s_tau_60s'                 data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n\n\nCode\nprobs &lt;- c(0.99, 0.9, 0.7, 0.5, 0.3, 0.1)\n\nplot_plot &lt;- function(df, x, y, type = \"hdr\", facets = NULL, color = NULL) {\n  p &lt;- ggplot(df, aes(x = .data[[x]], y = .data[[y]]) ) +\n    scale_color_okabeito(palette = \"black_first\")\n\n\n  if (!is.null(facets)) {\n    p &lt;- p + facet_wrap(vars(.data[[facets]]))\n  }\n\n  if (type == \"hdr\") {\n    p &lt;- p + geom_hdr_lines(\n      aes_string(color = color),\n      probs = probs\n    )\n  } else if (type == \"density\") {\n    p &lt;- p + geom_density_2d_filled()\n  }\n\n  return (p)\n}",
    "crumbs": [
      "Results",
      "Orientation"
    ]
  },
  {
    "objectID": "analysis/02_orientation.html#theta_nb",
    "href": "analysis/02_orientation.html#theta_nb",
    "title": "Orientation",
    "section": "\\(\\theta_{n,b}\\)",
    "text": "\\(\\theta_{n,b}\\)\n\n\nCode\nall_events_l1['theta_n_b'].describe()\n\n\n\n\n\n\nshape: (9, 2)\n\n\n\nstatistic\nvalue\n\n\nstr\nf64\n\n\n\n\n\"count\"\n280740.0\n\n\n\"null_count\"\n0.0\n\n\n\"mean\"\n89.565227\n\n\n\"std\"\n24.914059\n\n\n\"min\"\n0.046898\n\n\n\"25%\"\n78.403561\n\n\n\"50%\"\n89.87696\n\n\n\"75%\"\n100.96636\n\n\n\"max\"\n179.86788\n\n\n\n\n\n\n\n\nCode\ny &lt;- \"theta_n_b\"\nylab &lt;- expression(theta[\"n,b\"])\ny_lim &lt;- c(60, 120)\np1 &lt;- plot_dist(y=y, ylab=ylab, y_lim = y_lim, y_log=FALSE)\np1",
    "crumbs": [
      "Results",
      "Orientation"
    ]
  },
  {
    "objectID": "analysis/02_orientation.html#change-of-magnetic-field",
    "href": "analysis/02_orientation.html#change-of-magnetic-field",
    "title": "Orientation",
    "section": "Change of magnetic field",
    "text": "Change of magnetic field\n\n\nCode\nx &lt;- \"dB_x_n2\"\ny &lt;- \"dB_y_n2\"\nz &lt;- \"dB_z_n2\"\nfacets &lt;- \"r_bin\"\n\np1_JNO &lt;- plot_plot(JNO_events_l1, x, y, facets = facets) + ggtitle(\"JUNO\") + theme(legend.position = \"none\")\np1_other &lt;- plot_plot(other_events_l1, x, y)  + ggtitle(\"Others\") + theme(legend.position = \"none\")\n\np2_JNO &lt;- plot_plot(JNO_events_l1, x, z, facets = facets) + theme(legend.position = \"none\")\np2_other &lt;- plot_plot(other_events_l1, x, z, facets = facets)\n\np3_JNO &lt;- plot_plot(JNO_events_l1, y, z, facets = facets) + theme(legend.position = \"none\")\np3_other &lt;- plot_plot(other_events_l1, y, z, facets = facets) + theme(legend.position = \"none\")\n\n\np &lt;- (p1_JNO | p1_other) / (p2_JNO | p2_other) / (p3_JNO | p3_other)\nsave_plot(\"orientation/orientation_dB_xyz\")\np\n\n\nSaving 16.7 x 16.7 in image\nSaving 16.7 x 16.7 in image\n\n\nIn addition: Warning message:\n`aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was\ngenerated. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n(JNO_events_l1.hvplot.kde('theta_dB', by=\"r_bin\",subplots=True).cols(1) + JNO_events_l1.hvplot.kde('phi_dB', by=\"r_bin\",subplots=True).cols(1))\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\nCode\n(all_events_l1.hvplot.hist('theta_dB', by=\"sat\",subplots=True).cols(1) + all_events_l1.hvplot.hist('phi_dB', by=\"sat\",subplots=True).cols(1))",
    "crumbs": [
      "Results",
      "Orientation"
    ]
  },
  {
    "objectID": "analysis/02_orientation.html#normal-direction-obtained-from-dot-product",
    "href": "analysis/02_orientation.html#normal-direction-obtained-from-dot-product",
    "title": "Orientation",
    "section": "Normal direction obtained from dot product",
    "text": "Normal direction obtained from dot product\n\n\nCode\ndef dist_plot(df: pl.LazyFrame, var, by=None):\n    return df.hvplot.density(var, by=by, subplots=True, width=300, height=300)\n\n\nMost discontinuities has normal direction with large \\(k_x\\) and small \\(k_y\\) with evenly distributed \\(k_z\\).\n\n\nCode\n# (dist_plot(JNO_events_l1, \"k_x\", by=\"r_bin\") +  dist_plot(other_events_l1, \"k_x\")).cols(3) + (dist_plot(JNO_events_l1, \"k_y\", by=\"r_bin\") +  dist_plot(other_events_l1, \"k_y\")).cols(3)\n\npn.Column(\n    dist_plot(JNO_events_l1, \"k_x\", by=\"r_bin\").cols(5) +  dist_plot(other_events_l1, \"k_x\"),\n    dist_plot(JNO_events_l1, \"k_y\", by=\"r_bin\").cols(5) +  dist_plot(other_events_l1, \"k_y\"),\n    dist_plot(JNO_events_l1, \"k_z\", by=\"r_bin\").cols(5) +  dist_plot(other_events_l1, \"k_z\"),\n)\n\n\n\n\n\n\n\nCode\nx &lt;- \"k_x\"\ny &lt;- \"k_y\"\nz &lt;- \"k_z\"\nfacets &lt;- \"r_bin\"\n\np1_JNO &lt;- plot_plot(JNO_events_l1, x, y, facets = facets) + ggtitle(\"JUNO\") + theme(legend.position = \"none\")\np1_other &lt;- plot_plot(other_events_l1, x, y)  + ggtitle(\"Others\") + theme(legend.position = \"none\")\n\np2_JNO &lt;- plot_plot(JNO_events_l1, x, z, facets = facets) + theme(legend.position = \"none\")\np2_other &lt;- plot_plot(other_events_l1, x, z, facets = facets)\n\np3_JNO &lt;- plot_plot(JNO_events_l1, y, z, facets = facets) + theme(legend.position = \"none\")\np3_other &lt;- plot_plot(other_events_l1, y, z, facets = facets) + theme(legend.position = \"none\")\n\n\np &lt;- (p1_JNO | p1_other) / (p2_JNO | p2_other) / (p3_JNO | p3_other)\nsave_plot(\"orientation/orientation_k_xyz\")\np\n\n\nSaving 16.7 x 16.7 in image\nSaving 16.7 x 16.7 in image\n\n\nIn addition: Warning message:\n`aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was\ngenerated.",
    "crumbs": [
      "Results",
      "Orientation"
    ]
  },
  {
    "objectID": "analysis/02_orientation.html#v_l",
    "href": "analysis/02_orientation.html#v_l",
    "title": "Orientation",
    "section": "\\(V_l\\)",
    "text": "\\(V_l\\)\n\n\nCode\nx &lt;- \"Vl_x\"\ny &lt;- \"Vl_y\"\nz &lt;- \"Vl_z\"\nfacets &lt;- \"r_bin\"\n\np1_JNO &lt;- plot_plot(JNO_events_l1, x, y, facets = facets) + ggtitle(\"JUNO\") + theme(legend.position = \"none\")\np1_other &lt;- plot_plot(other_events_l1, x, y)  + ggtitle(\"Others\") + theme(legend.position = \"none\")\n\np2_JNO &lt;- plot_plot(JNO_events_l1, x, z, facets = facets) + theme(legend.position = \"none\")\np2_other &lt;- plot_plot(other_events_l1, x, z, facets = facets)\n\np3_JNO &lt;- plot_plot(JNO_events_l1, y, z, facets = facets) + theme(legend.position = \"none\")\np3_other &lt;- plot_plot(other_events_l1, y, z, facets = facets) + theme(legend.position = \"none\")\n\n\np &lt;- (p1_JNO | p1_other) / (p2_JNO | p2_other) / (p3_JNO | p3_other)\nsave_plot(\"orientation/orientation_Vl_xyz\")\np\n\n\nSaving 16.7 x 16.7 in image\nSaving 16.7 x 16.7 in image\n\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution\n\n\nCode\nx_var &lt;- \"time\"\ny_vars &lt;- c(\"k_x\", \"k_y\", \"k_z\")\nxlab &lt;- \"Time\"\nylabs &lt;- c(\"Orientation (k_x)\", \"k_y\", \"k_z\")\np &lt;- plot_util(all_events_l2, x_var = x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"orientation/orientation_k_time\")\n\nx_var &lt;- \"ref_radial_distance\"\nxlab &lt;- \"Referred Radial Distance (AU)\"\np &lt;- plot_util(all_events_l2, x_var=x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"orientation/orientation_k_r\")\n\nx_var &lt;- \"ref_radial_distance\"\ny_vars &lt;- c(\"Vl_x\", \"Vl_y\", \"Vl_z\")\nxlab &lt;- \"Referred Radial Distance (AU)\"\nylabs &lt;- c(\"Orientation (l_x)\", \"l_y\", \"l_z\")\np &lt;- plot_util(all_events_l2, x_var=x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"orientation/orientation_l_r\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nIn addition: There were 30 warnings (use warnings() to see them)\n\n\n\n\nCode\nx_col &lt;- \"radial_distance\"\ny_col &lt;- \"k_x\"\ny_lim &lt;- NULL\nx_bins &lt;- 16\ny_bins &lt;- 32\nxlab &lt;- \"Radial Distance (AU)\"\nylab &lt;- \"Orientation (k_x)\"\np &lt;- plot_binned_data(JNO_events_l1, x_col = x_col, y_col = y_col, x_bins = x_bins, y_bins=y_bins, y_lim = y_lim, log_y = FALSE)\np &lt;- p + labs(x = xlab, y= ylab)\nsave_plot(\"orientation/orientation_kx_r_dist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image",
    "crumbs": [
      "Results",
      "Orientation"
    ]
  },
  {
    "objectID": "analysis/11_ts.html",
    "href": "analysis/11_ts.html",
    "title": "Time resolution effect",
    "section": "",
    "text": "Code\nfrom ids_finder.utils.basic import load_params\nfrom ids_finder.utils.basic import load_catalog\n\nimport polars as pl\n\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\n\n\n\nCode\nparams = load_params()\ncatalog = load_catalog()\n\n\n[01/08/24 11:11:19] WARNING  /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ warnings.py:109\n                             kedro_datasets/polars/lazy_polars_dataset.py:14:                                      \n                             KedroDeprecationWarning: 'AbstractVersionedDataSet' has been renamed                  \n                             to 'AbstractVersionedDataset', and the alias will be removed in Kedro                 \n                             0.19.0                                                                                \n                               from kedro.io.core import (                                                         \n                                                                                                                   \n\n\n\n\n\nCode\nwind_ts_low_all: pl.LazyFrame = catalog.load('events.l1.Wind_ts_1s_tau_60s')\nwind_ts_high_all: pl.LazyFrame = catalog.load('events.l1.Wind_ts_0.09s_tau_60s')\nwind_ts_01_all: pl.LazyFrame = catalog.load('events.l1.Wind_ts_0.1s_tau_60s')\nwind_ts_02_all: pl.LazyFrame = catalog.load('events.l1.Wind_ts_0.2s_tau_60s')\nwind_ts_05_all: pl.LazyFrame = catalog.load('events.l1.Wind_ts_0.5s_tau_60s')\njuno_ts_low_all: pl.LazyFrame = catalog.load('events.l1.JNO_ts_1s_tau_60s')\n\ntime_filter = pl.col('time').dt.year()==2016\n\nwind_ts_low = wind_ts_low_all.filter(time_filter).with_columns(\n    ts = pl.lit('1s'),\n    sat = pl.lit('Wind')\n)\nwind_ts_high = wind_ts_high_all.filter(time_filter).with_columns(\n    ts = pl.lit('0.09s'),\n    sat = pl.lit('Wind')\n)\n\nwind_ts_01 = wind_ts_01_all.filter(time_filter).with_columns(\n    ts = pl.lit('0.1s'),\n    sat = pl.lit('Wind')\n)\nwind_ts_02 = wind_ts_02_all.filter(time_filter).with_columns(\n    ts = pl.lit('0.2s'),\n    sat = pl.lit('Wind')\n)\nwind_ts_05 = wind_ts_05_all.filter(time_filter).with_columns(\n    ts = pl.lit('0.5s'),\n    sat = pl.lit('Wind')\n)\n\njuno_ts_low = juno_ts_low_all.filter(time_filter).with_columns(\n    ts = pl.lit('Juno 1s'),\n    sat = pl.lit('JUNO')\n)\n\ndf = pl.concat([juno_ts_low, wind_ts_high, wind_ts_01, wind_ts_02, wind_ts_05, wind_ts_low], how='diagonal').collect()\n\n\n[01/08/24 11:11:20] INFO     Loading data from 'events.l1.Wind_ts_1s_tau_60s'                   data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'events.l1.Wind_ts_0.09s_tau_60s'                data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'events.l1.Wind_ts_0.1s_tau_60s'                 data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'events.l1.Wind_ts_0.2s_tau_60s'                 data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'events.l1.Wind_ts_0.5s_tau_60s'                 data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'events.l1.JNO_ts_1s_tau_60s'                    data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n\n\nCode\ndf.group_by('ts', 'sat').agg(pl.count())\n\n\n\n\n\n\nshape: (5, 3)\n\n\n\nts\nsat\ncount\n\n\nstr\nstr\nu32\n\n\n\n\n\"1s\"\n\"Wind\"\n8711\n\n\n\"Juno 1s\"\n\"JUNO\"\n1223\n\n\n\"0.2s\"\n\"Wind\"\n8698\n\n\n\"0.5s\"\n\"Wind\"\n8664\n\n\n\"0.1s\"\n\"Wind\"\n8731\n\n\n\n\n\n\n\n\nCode\n# sort color with 'JUNO 1s' first\ntemp_df &lt;- df %&gt;%\n  mutate(ts = factor(ts, levels = c(\"Juno 1s\", \"1s\", \"0.5s\", \"0.2s\", \"0.1s\")))\n\ncolor &lt;- \"ts\"\nadd &lt;- \"mean\"\ncommon_custom &lt;- scale_color_okabeito(palette = \"black_first\")\n\nx &lt;- \"L_mn\"\nx_lim &lt;- c(0,7500)\np1 &lt;- ggdensity(temp_df, x = x, color = color, add = add, alpha = 0) + xlim(x_lim) + common_custom\n\nx &lt;- \"L_mn_norm\"\nx_lim &lt;- c(0,60)\nx_lab &lt;- \"Thickness (d_i)\"\np2 &lt;- ggdensity(temp_df, x = x, color = color, add = add, alpha = 0) + xlim(x_lim) + common_custom + labs(x=x_lab)\n\nx &lt;- \"j0\"\nx_lim &lt;- c(0,20)\np3 &lt;- ggdensity(temp_df, x = x, color = color, add = add, alpha = 0) + xlim(x_lim) + common_custom\n\nx &lt;- \"j0_norm\"\nx_lim &lt;- c(0,0.8)\np4 &lt;- ggdensity(temp_df, x = x, color = color, add = add, alpha = 0) + xlim(x_lim) + common_custom\n\n# p1 + p2 + p3 + p4  +\np2 + p4  +\n  plot_layout(guides = 'collect', nrow=2) &\n  theme(legend.position='top')\n\n\n[01/08/24 11:11:30] WARNING  R[write to console]: In addition:                                     callbacks.py:124\n\n\n\n                    WARNING  R[write to console]: Warning messages:                                callbacks.py:124\n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: 1: Removed 334 rows containing non-finite values callbacks.py:124\n                             (`stat_density()`).                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: 2: Removed 502 rows containing non-finite values callbacks.py:124\n                             (`stat_density()`).",
    "crumbs": [
      "Results",
      "Time resolution effect"
    ]
  },
  {
    "objectID": "analysis/01_occurence_rate.html",
    "href": "analysis/01_occurence_rate.html",
    "title": "Occurence Rate",
    "section": "",
    "text": "Code\nfrom fastcore.utils import patch\nfrom fastcore.test import *\n\nfrom ids_finder.utils.basic import load_catalog\nfrom ids_finder.utils.basic import filter_tranges_df\nfrom ids_finder.utils.analysis import filter_before_jupiter\nfrom ids_finder.utils.analysis import link_coord2dim\nfrom ids_finder.datasets import cIDsDataset\n\nfrom beforerr.basics import pmap\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\nfrom datetime import timedelta\n\nimport polars as pl\nimport polars.selectors as cs\nimport pandas as pd\n\nimport warnings\n\n# This will filter out all FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n04-Dec-23 09:02:03 INFO     04-Dec-23 09:02:03: cffi mode is CFFI_MODE.ANY                          situation.py:41\n\n\n\n                   INFO     04-Dec-23 09:02:03: R home found:                                      situation.py:218\n                            /Library/Frameworks/R.framework/Resources                                              \n\n\n\n                   INFO     04-Dec-23 09:02:03: R library path:                                    situation.py:161\n\n\n\n                   INFO     04-Dec-23 09:02:03: LD_LIBRARY_PATH:                                   situation.py:165\n\n\n\n                   INFO     04-Dec-23 09:02:03: Default options to initialize R: rpy2, --quiet,      embedded.py:20\n                            --no-save                                                                              \n\n\n\n                   INFO     04-Dec-23 09:02:03: R is already initialized. No need to initialize.    embedded.py:269\n\n\n\n\n\nCode\ncatalog = load_catalog()\n\n\n[12/04/23 09:02:04] WARNING  /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ke logger.py:205\n                             dro_datasets/polars/lazy_polars_dataset.py:14: KedroDeprecationWarning:               \n                             'AbstractVersionedDataSet' has been renamed to                                        \n                             'AbstractVersionedDataset', and the alias will be removed in Kedro                    \n                             0.19.0                                                                                \n                               from kedro.io.core import (                                                         \n                                                                                                                   \n\n\n\nConnect python with R kernel\n\n\nCode\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\n[12/04/23 09:02:05] WARNING  R[write to console]:                                                  callbacks.py:124\n                             Attaching package: ‘dplyr’                                                            \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:stats’:                                                                      \n                                                                                                                   \n                                 filter, lag                                                                       \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:base’:                                                                       \n                                                                                                                   \n                                 intersect, setdiff, setequal, union                                               \n                                                                                                                   \n                                                                                                                   \n\n\n\n\n\n\n\n\nCode\nSTA_ds = cIDsDataset(sat_id=\"STA\", tau=60, ts=1, catalog=catalog)\nJNO_ds = cIDsDataset(sat_id=\"JNO\", tau=60, ts=1, catalog=catalog)\nTHB_ds = cIDsDataset(sat_id=\"THB\", tau=60, ts=1, catalog=catalog)\nWind_ds = cIDsDataset(sat_id=\"Wind\", tau=60, ts=1, catalog=catalog)\n\n\n                    INFO     Loading data from 'events.STA_ts_1s_tau_60s'                       data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'STA.MAG.primary_data_ts_1s'                     data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\n                    INFO     Loading data from 'events.JNO_ts_1s_tau_60s'                       data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'JNO.MAG.primary_data_ts_1s'                     data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\n                    INFO     Loading data from 'events.THB_ts_1s_tau_60s'                       data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'THB.MAG.primary_data_ts_1s'                     data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\n                    INFO     Loading data from 'events.Wind_ts_1s_tau_60s'                      data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'Wind.MAG.primary_data_ts_1s'                    data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\nARTEMIS missions needs additional care as they are not always in the solar wind.\n\n\nCode\nthb_inter_state_sw: pl.LazyFrame = catalog.load('THB.STATE.inter_data_sw')\nstart, end = thb_inter_state_sw.select(['start', 'end']).collect()\n\nTHB_sw_ds = cIDsDataset(\n    sat_id=\"THB\", tau=60, ts=1, catalog=catalog,\n    candidates = filter_tranges_df(THB_ds.candidates, (start, end)), \n    data = filter_tranges_df(THB_ds.data.collect(), (start, end)).lazy()\n)\n\nTHB_sw_ds = THB_ds.copy(\n    update=dict(\n        candidates = THB_ds.candidates.pipe(filter_tranges_df, (start, end)),\n        data = THB_ds.data.collect().pipe(filter_tranges_df, (start, end)).lazy()\n    )\n)\n\n\n                    INFO     Loading data from 'THB.STATE.inter_data_sw' (LazyPolarsDataset)... data_catalog.py:502\n\n\n\n\n\nCode\nall_ds = [JNO_ds, Wind_ds, STA_ds, THB_sw_ds]",
    "crumbs": [
      "Results",
      "Occurence Rate"
    ]
  },
  {
    "objectID": "analysis/01_occurence_rate.html#setup",
    "href": "analysis/01_occurence_rate.html#setup",
    "title": "Occurence Rate",
    "section": "",
    "text": "Code\nfrom fastcore.utils import patch\nfrom fastcore.test import *\n\nfrom ids_finder.utils.basic import load_catalog\nfrom ids_finder.utils.basic import filter_tranges_df\nfrom ids_finder.utils.analysis import filter_before_jupiter\nfrom ids_finder.utils.analysis import link_coord2dim\nfrom ids_finder.datasets import cIDsDataset\n\nfrom beforerr.basics import pmap\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\nfrom datetime import timedelta\n\nimport polars as pl\nimport polars.selectors as cs\nimport pandas as pd\n\nimport warnings\n\n# This will filter out all FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n04-Dec-23 09:02:03 INFO     04-Dec-23 09:02:03: cffi mode is CFFI_MODE.ANY                          situation.py:41\n\n\n\n                   INFO     04-Dec-23 09:02:03: R home found:                                      situation.py:218\n                            /Library/Frameworks/R.framework/Resources                                              \n\n\n\n                   INFO     04-Dec-23 09:02:03: R library path:                                    situation.py:161\n\n\n\n                   INFO     04-Dec-23 09:02:03: LD_LIBRARY_PATH:                                   situation.py:165\n\n\n\n                   INFO     04-Dec-23 09:02:03: Default options to initialize R: rpy2, --quiet,      embedded.py:20\n                            --no-save                                                                              \n\n\n\n                   INFO     04-Dec-23 09:02:03: R is already initialized. No need to initialize.    embedded.py:269\n\n\n\n\n\nCode\ncatalog = load_catalog()\n\n\n[12/04/23 09:02:04] WARNING  /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ke logger.py:205\n                             dro_datasets/polars/lazy_polars_dataset.py:14: KedroDeprecationWarning:               \n                             'AbstractVersionedDataSet' has been renamed to                                        \n                             'AbstractVersionedDataset', and the alias will be removed in Kedro                    \n                             0.19.0                                                                                \n                               from kedro.io.core import (                                                         \n                                                                                                                   \n\n\n\nConnect python with R kernel\n\n\nCode\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\n[12/04/23 09:02:05] WARNING  R[write to console]:                                                  callbacks.py:124\n                             Attaching package: ‘dplyr’                                                            \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:stats’:                                                                      \n                                                                                                                   \n                                 filter, lag                                                                       \n                                                                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: The following objects are masked from            callbacks.py:124\n                             ‘package:base’:                                                                       \n                                                                                                                   \n                                 intersect, setdiff, setequal, union                                               \n                                                                                                                   \n                                                                                                                   \n\n\n\n\n\n\n\n\nCode\nSTA_ds = cIDsDataset(sat_id=\"STA\", tau=60, ts=1, catalog=catalog)\nJNO_ds = cIDsDataset(sat_id=\"JNO\", tau=60, ts=1, catalog=catalog)\nTHB_ds = cIDsDataset(sat_id=\"THB\", tau=60, ts=1, catalog=catalog)\nWind_ds = cIDsDataset(sat_id=\"Wind\", tau=60, ts=1, catalog=catalog)\n\n\n                    INFO     Loading data from 'events.STA_ts_1s_tau_60s'                       data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'STA.MAG.primary_data_ts_1s'                     data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\n                    INFO     Loading data from 'events.JNO_ts_1s_tau_60s'                       data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'JNO.MAG.primary_data_ts_1s'                     data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\n                    INFO     Loading data from 'events.THB_ts_1s_tau_60s'                       data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'THB.MAG.primary_data_ts_1s'                     data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\n                    INFO     Loading data from 'events.Wind_ts_1s_tau_60s'                      data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'Wind.MAG.primary_data_ts_1s'                    data_catalog.py:502\n                             (PartitionedDataset)...                                                               \n\n\n\nARTEMIS missions needs additional care as they are not always in the solar wind.\n\n\nCode\nthb_inter_state_sw: pl.LazyFrame = catalog.load('THB.STATE.inter_data_sw')\nstart, end = thb_inter_state_sw.select(['start', 'end']).collect()\n\nTHB_sw_ds = cIDsDataset(\n    sat_id=\"THB\", tau=60, ts=1, catalog=catalog,\n    candidates = filter_tranges_df(THB_ds.candidates, (start, end)), \n    data = filter_tranges_df(THB_ds.data.collect(), (start, end)).lazy()\n)\n\nTHB_sw_ds = THB_ds.copy(\n    update=dict(\n        candidates = THB_ds.candidates.pipe(filter_tranges_df, (start, end)),\n        data = THB_ds.data.collect().pipe(filter_tranges_df, (start, end)).lazy()\n    )\n)\n\n\n                    INFO     Loading data from 'THB.STATE.inter_data_sw' (LazyPolarsDataset)... data_catalog.py:502\n\n\n\n\n\nCode\nall_ds = [JNO_ds, Wind_ds, STA_ds, THB_sw_ds]",
    "crumbs": [
      "Results",
      "Occurence Rate"
    ]
  },
  {
    "objectID": "analysis/01_occurence_rate.html#distance-and-occurrence-rates-versus-time-for-juno",
    "href": "analysis/01_occurence_rate.html#distance-and-occurrence-rates-versus-time-for-juno",
    "title": "Occurence Rate",
    "section": "Distance and Occurrence rates versus time for JUNO",
    "text": "Distance and Occurrence rates versus time for JUNO\n\n\nCode\ndef calc_or_df(events: pl.DataFrame, avg_window: timedelta, by=None):\n    \"\"\"Calculate the occurence rate of the candidates with the average window.\n\n    Notes: occurence rate is defined as the number of candidates per day.\n    \"\"\"\n\n    every = avg_window\n    or_factor = every / timedelta(days=1)\n\n    return (\n        events.sort(\"time\")\n        .group_by_dynamic(\"time\", every=every, by=by)\n        .agg(\n            cs.float().mean(),\n            o_rates=pl.count() / or_factor\n        )\n        .upsample(\"time\", every=every)  # upsample to fill the missing time\n    )\n\n\n\n\nCode\ndf = JNO_ds.candidates.pipe(calc_or_df, timedelta(days=5))\n\n\n\n\nCode\np1 &lt;- ggplot(df, aes(x = time, y = radial_distance)) + \n  geom_line() + # Plot distance by date\n  labs(x = \"Date\", y = \"Distance (AU)\") +\n  theme_pubr(base_size = 16) + \n  theme(aspect.ratio=0.25)\n  \np2 &lt;- ggplot(df, aes(x = time, y = o_rates)) + \n  geom_line() + # Plot distance by date\n  labs(x = \"Date\", y = \"Occurrence Rates (#/day)\") +\n  theme_pubr(base_size = 16) + \n  theme(aspect.ratio=0.25)\n\np &lt;- ggarrange(p1, p2, nrow = 2)\n\n# save_plot(\"distance_and_or\")\np",
    "crumbs": [
      "Results",
      "Occurence Rate"
    ]
  },
  {
    "objectID": "analysis/01_occurence_rate.html#occurrence-rates-versus-time-for-all-missions",
    "href": "analysis/01_occurence_rate.html#occurrence-rates-versus-time-for-all-missions",
    "title": "Occurence Rate",
    "section": "Occurrence rates versus time for all missions",
    "text": "Occurrence rates versus time for all missions\n\nData normalization\nDifferent levels of normalization are applied to the data. The normalization is done in the following order:\n\nN1: normalize the data by the the effective time of every duration due to the data gap as we may miss some potential IDs. We assume the data gap is independent of the magnetic discontinuities.\nN2: normalize the data by the mean value of data near 1 AU. This is to remove the effect of the temporal variation of the solar wind.\n\n\n\nCode\ndef calc_n1_factor(\n    data: pl.LazyFrame,\n    s_resolution: timedelta,\n    l_resolution: timedelta,\n):\n    return (\n        data.sort(\"time\")\n        .group_by_dynamic(\"time\", every=s_resolution)\n        .agg(pl.lit(1).alias(\"availablity\"))\n        .group_by_dynamic(\"time\", every=l_resolution)\n        .agg(n1_factor=pl.sum(\"availablity\") * s_resolution / l_resolution)\n    )\n\n\ndef n1_normalize(\n    df: pl.DataFrame,  # the dataframe with count to be normalized\n    data: pl.LazyFrame,  # the data used to calculate the duration ratio\n    s_resolution,  # the smallest resolution to check if the data is available\n    avg_window,\n):\n    duration_df = calc_n1_factor(data, s_resolution, avg_window).with_columns(\n        cs.datetime().dt.cast_time_unit(\"ns\"),\n    )\n\n    return df.lazy().join(duration_df, how=\"left\", on=\"time\").with_columns(\n        o_rates_normalized=pl.col(\"o_rates\") / pl.col(\"n1_factor\")\n    ).collect()\n    \n@patch\ndef calc_or_normalized(\n    self: cIDsDataset, s_resolution: timedelta, avg_window: timedelta\n):\n    count_df = calc_or_df(self.candidates, avg_window)\n    return n1_normalize(count_df, self.data, s_resolution, avg_window)\n\n\n\n\nCode\ndef n2_normalize(df: pl.DataFrame, avg_sats = [\"STA\", \"THB\", \"Wind\"]):\n    avg_df = (\n        df.filter(pl.col(\"sat\").is_in(avg_sats))\n        .group_by(\"time\")\n        .agg(n2_factor=pl.mean(\"o_rates_normalized\"))\n    )\n    return df.join(avg_df, on=\"time\").with_columns(\n        o_rates_normalized=pl.col(\"o_rates_normalized\") / pl.col(\"n2_factor\")\n    )\n\n\n\n\nCode\ns_resolution = timedelta(minutes=1)\navg_window = timedelta(days=30)\n\n\n\n\nCode\nall_events_or_N1: pl.DataFrame = pl.concat(\n    all_ds\n    | pmap(\n        lambda x: x.calc_or_normalized(s_resolution, avg_window).with_columns(\n            sat=pl.lit(x.sat_id)\n        )\n    ),\n    how=\"diagonal\",\n)\n\n\n\n\nCode\nall_events_or_N2 = all_events_or_N1.pipe(n2_normalize)\n\n\n\n\nCode\nplot_or_time &lt;- function(df) {\n  p &lt;- ggline(\n    df, x = \"time\", y = \"o_rates_normalized\", \n    color = \"sat\", linetype = \"sat\") \n  \n  p +   \n    labs(x = \"Date\", y = \"Occurrence Rates (#/day)\", color=\"Satellites\", linetype=\"Satellites\") + \n    theme_pubr(base_size = 16) + \n    theme(legend.text = element_text(size=16)) +\n    scale_color_okabeito(palette = \"black_first\")\n}\n\n\n\n\nCode\np &lt;- plot_or_time(all_events_or_N1)\np\n\n\n[12/04/23 09:02:37] WARNING  R[write to console]: In addition:                                     callbacks.py:124\n\n\n\n                    WARNING  R[write to console]: Warning message:                                 callbacks.py:124\n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: Removed 6 rows containing missing values         callbacks.py:124\n                             (`geom_point()`).                                                                     \n                                                                                                                   \n\n\n\n\n\n\n\n\n\n\n\n\n\nWe noticed some anomalies in the occurrence rates of the magnetic discontinuities for Stereo-A data. Also for Juno, its occurrence rate is much higher when approaching Jupiter.\n\n\nCode\nall_events_or_N1.filter(\n    pl.col('time').is_in(pd.date_range('2014-01-01', '2015-01-01')),\n    sat='STA'\n)[[\"time\", \"o_rates\", \"o_rates_normalized\", 'n1_factor']]\n\n\n\n\n\n\nshape: (12, 4)\n\n\n\ntime\no_rates\no_rates_normalized\nn1_factor\n\n\ndatetime[ns]\nf64\nf64\nf64\n\n\n\n\n2014-01-10 00:00:00\n64.466667\n67.349278\n0.957199\n\n\n2014-02-09 00:00:00\n71.2\n71.232978\n0.999537\n\n\n2014-03-11 00:00:00\n71.666667\n74.137931\n0.966667\n\n\n2014-04-10 00:00:00\n72.2\n72.210029\n0.999861\n\n\n2014-05-10 00:00:00\n74.166667\n74.182121\n0.999792\n\n\n2014-06-09 00:00:00\n65.733333\n71.530265\n0.918958\n\n\n2014-07-09 00:00:00\n68.233333\n76.778496\n0.888704\n\n\n2014-08-08 00:00:00\n4.033333\n9.013968\n0.447454\n\n\n2014-09-07 00:00:00\n11.0\n69.53468\n0.158194\n\n\n2014-10-07 00:00:00\n6.6\n77.163735\n0.085532\n\n\n2014-11-06 00:00:00\n9.866667\n66.548009\n0.148264\n\n\n2014-12-06 00:00:00\n9.766667\n63.686038\n0.153356\n\n\n\n\n\n\nSurprisingly, we found out that the anomaly of STEREO-A data is not mainly due to data gap. We can inspect this data further. See appendix.\nWe remove the intervals which do not have enough data points to calculate the normalized occurrence rates accurately and restrict the time range to exclude Jupiter’s effect.\n\n\nCode\nall_events_or_N1_cleaned = (\n    all_events_or_N1.sort(\"time\")\n    .pipe(filter_before_jupiter)\n    .filter(pl.col(\"o_rates\") &gt; 5)\n    .upsample(\"time\", every=avg_window, by=\"sat\", maintain_order=True)\n    .with_columns(pl.col(\"sat\").forward_fill())\n)\n\nall_events_or_N2_cleaned = n2_normalize(all_events_or_N1_cleaned)\n\n\n\n\nCode\np &lt;- plot_or_time(all_events_or_N1_cleaned)\n# save_plot(\"ocr/ocr_time_cleaned\")\np\n\n\n[12/04/23 09:07:11] WARNING  R[write to console]: In addition:                                     callbacks.py:124\n\n\n\n                    WARNING  R[write to console]: Warning message:                                 callbacks.py:124\n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: Removed 9 rows containing missing values         callbacks.py:124\n                             (`geom_point()`).                                                                     \n                                                                                                                   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\np &lt;- plot_or_time(all_events_or_N2_cleaned)\np &lt;- p + labs(y = \"Normalized Occurrence Rates\")\nsave_plot(\"ocr/ocr_time_N2_cleaned\")\np\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\n[12/04/23 09:16:01] WARNING  R[write to console]: In addition:                                     callbacks.py:124\n\n\n\n                    WARNING  R[write to console]: Warning message:                                 callbacks.py:124\n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: Removed 9 rows containing missing values         callbacks.py:124\n                             (`geom_point()`).                                                                     \n                                                                                                                   \n\n\n\nIn addition: Warning messages:\n1: Removed 9 rows containing missing values (`geom_point()`). \n2: Removed 9 rows containing missing values (`geom_point()`). \n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot the occurrence rates with radial distance\n\n\nCode\nplot_or_r &lt;- function(df, target_sat = \"JNO\") {\n  \"plot normalized occurrence rate over radial distance\"\n  \n  # Filter data for target_sat\n  df_target &lt;- df[df$sat == target_sat,]\n  \n  # Compute the linear model\n  fit &lt;- lm(o_rates_normalized ~ I(1/ref_radial_distance), data = df_target)\n  \n  # Extract coefficients\n  intercept &lt;- coef(fit)[1]\n  slope &lt;- coef(fit)[2]\n  \n  # Format equation\n  equation &lt;- sprintf(\"y ~ %.2f / x\", slope)\n  \n  p &lt;- ggscatter(df, x = \"ref_radial_distance\", y = \"o_rates_normalized\", color = \"sat\") +\n    geom_smooth(data = df_target, formula = y ~ I(1/x), method = \"lm\", color=\"gray\", linetype=\"dashed\")\n    \n  p &lt;- p +\n    labs(x = \"Referred Radial Distance (AU)\", y = \"Occurrence Rate  (#/day)\", color=\"Satellites\") +\n    annotate(\"text\", label = equation, x = Inf, y = Inf, hjust = 1.1, vjust = 1.5, size = 7) +\n    theme_pubr(base_size = 16) + \n    theme(legend.text = element_text(size=16)) +\n    scale_color_okabeito(palette = \"black_first\")\n  \n  return(p)\n}\n\n\n\n\nCode\ndf = link_coord2dim(all_events_or_N1_cleaned).sort(\"ref_radial_distance\")\n\n\n\n\nCode\np &lt;- plot_or_r(df)\nprint(p)\n# save_plot(\"ocr_r_cleaned\")\n\n\nIn addition: Warning messages:\n1: Removed 4 rows containing non-finite values (`stat_smooth()`). \n2: Removed 21 rows containing missing values (`geom_point()`). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf = link_coord2dim(all_events_or_N2_cleaned).sort(\"ref_radial_distance\")\n\n\n\n\nR Code\np &lt;- plot_or_r(df) + labs(y = \"Normalized Occurrence Rates\")\nprint(p)\n# save_plot(\"ocr/ocr_r_N2_cleaned\")\n\n\nIn addition: Warning messages:\n1: Removed 4 rows containing non-finite values (`stat_smooth()`). \n2: Removed 21 rows containing missing values (`geom_point()`).",
    "crumbs": [
      "Results",
      "Occurence Rate"
    ]
  },
  {
    "objectID": "analysis/01_occurence_rate.html#appendix",
    "href": "analysis/01_occurence_rate.html#appendix",
    "title": "Occurence Rate",
    "section": "Appendix",
    "text": "Appendix\n\nObsolete codes\nNotes: seaborn.lineplot drops nans from the DataFrame before plotting, this is not desired…\n\n\nCode\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport scienceplots\nfrom ids_finder.utils.plot import savefig\n\n\n\n\nCode\nplt.style.use(['science', 'nature', 'notebook'])\n\n\n\n\nCode\ndef plot_or_r(df: pl.DataFrame):\n    \"plot normalized occurence rate over radial distance\"\n\n    sns.lineplot(x=\"ref_radial_distance\", y=\"o_rates_normalized\", hue=\"sat\", data=df)\n\n    ax = plt.gca()  # Get current axis\n    ax.set_yscale(\"log\")\n    ax.set_xlabel(\"Referred Radial Distance (AU)\")\n    ax.set_ylabel(\"Normalized Occurrence Rate\")\n    # savefig('occurrence_rate_ratio')\n\n    return ax.figure\n\n\n\n\nCode\ndef plot_or_time(df: pl.DataFrame):\n    \"\"\"Plot the occurence rate of the candidates with time.    \n    \"\"\"\n    # Create a unique list of all satellites and sort them to let JNO' be plotted first\n    all_sats = df[\"sat\"].unique().to_list()\n    all_sats.sort(key=lambda x: x != \"JNO\")\n\n    # Plot each satellite separately\n    for sat in all_sats:\n        sat_df = df.filter(sat=sat)\n        if sat == \"JNO\":\n            sns.lineplot(sat_df, x=\"time\", y=\"o_rates_normalized\", label=sat)\n        else:\n            # Making the other satellites more distinct with linestyle and alpha\n            sns.lineplot(\n                sat_df,\n                x=\"time\",\n                y=\"o_rates_normalized\",\n                linestyle=\"--\",  # dashed line style\n                alpha=0.5,  # keep the order of the legend\n                label=sat,\n            )\n\n    ax = plt.gca()  # Get current axis\n    # Set the y-axis and x-axis labels\n    ax.set_ylabel(\"Occurrence Rates (#/day)\")\n    ax.set_xlabel(\"Date\")\n    ax.legend(title=\"Satellites\")\n\n    # savefig(\"occurrence_rates\")\n    return ax.figure\n\n\n\n\nCode\nfig = plot_or_time(\n    all_events_or_N1\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSTEREO-A anomaly during 2014-08 period\n\n\nCode\nmag: pl.DataFrame = catalog.load('sta.inter_mag_1s').filter(pl.col('B')&gt;0).collect()\n\n\n\n\nCode\nimport plotly.graph_objects as go;\nfrom plotly_resampler import FigureResampler\nimport plotly.express as px\n\n\n\n\nCode\n# px.line(mag, x='time', y='B') # This is extremely slow for large datasets\n\nfig = FigureResampler(go.Figure())\nfig.add_trace({\"x\": mag[\"time\"], \"y\": mag['B']})\nfig",
    "crumbs": [
      "Results",
      "Occurence Rate"
    ]
  },
  {
    "objectID": "analysis/02_properties.html",
    "href": "analysis/02_properties.html",
    "title": "SWD Properties",
    "section": "",
    "text": "Code\ncatalog = load_catalog()\n\n\n[01/11/24 16:43:56] WARNING  /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ke logger.py:205\n                             dro_datasets/polars/lazy_polars_dataset.py:14: KedroDeprecationWarning:               \n                             'AbstractVersionedDataSet' has been renamed to                                        \n                             'AbstractVersionedDataset', and the alias will be removed in Kedro                    \n                             0.19.0                                                                                \n                               from kedro.io.core import (                                                         \n                                                                                                                   \n\n\n\nConnect python with R kernel\n\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\n\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython",
    "crumbs": [
      "Results",
      "SWD Properties"
    ]
  },
  {
    "objectID": "analysis/02_properties.html#setup",
    "href": "analysis/02_properties.html#setup",
    "title": "SWD Properties",
    "section": "",
    "text": "Code\ncatalog = load_catalog()\n\n\n[01/11/24 16:43:56] WARNING  /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ke logger.py:205\n                             dro_datasets/polars/lazy_polars_dataset.py:14: KedroDeprecationWarning:               \n                             'AbstractVersionedDataSet' has been renamed to                                        \n                             'AbstractVersionedDataset', and the alias will be removed in Kedro                    \n                             0.19.0                                                                                \n                               from kedro.io.core import (                                                         \n                                                                                                                   \n\n\n\nConnect python with R kernel\n\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\n\n\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython",
    "crumbs": [
      "Results",
      "SWD Properties"
    ]
  },
  {
    "objectID": "analysis/02_properties.html#processing-datasets",
    "href": "analysis/02_properties.html#processing-datasets",
    "title": "SWD Properties",
    "section": "Processing datasets",
    "text": "Processing datasets\n\n\nCode\nall_events_l1 : pl.DataFrame = catalog.load('events.l1.ALL_sw_ts_1s_tau_60s').collect()\nJNO_events_l1 = all_events_l1.filter(pl.col('sat') == 'JNO')\nother_events_l1 = all_events_l1.filter(pl.col('sat') != 'JNO')\nall_events_l2 = all_events_l1.pipe(process_events_l2)\n\n\n                    INFO     Loading data from 'events.l1.ALL_sw_ts_1s_tau_60s'                 data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\nSome extreme values are present in the data. We will remove them.\n\n\nCode\nNVARS = ['d_star', 'L_mn', 'L_mn_norm', 'j0', 'j0_norm', 'duration', 'v_mn']\nDISPLAY_VARS = ['time', 'sat'] + NVARS\n\n\ndef check_candidates(df):\n    if isinstance(df, pl.LazyFrame):\n        df = df.collect()\n    return df.select(NVARS).describe()\n\ncheck_candidates(all_events_l1)\n\n\n\n\n\n\nshape: (9, 8)\n\n\n\ndescribe\nd_star\nL_mn\nL_mn_norm\nj0\nj0_norm\nduration\nv_mn\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"count\"\n280740.0\n280740.0\n280740.0\n280740.0\n280740.0\n\"280740\"\n280740.0\n\n\n\"null_count\"\n0.0\n0.0\n524.0\n0.0\n524.0\n\"0\"\n0.0\n\n\n\"mean\"\n0.919975\n2368.171241\n20.579939\n2.584397\n0.079965\n\"0:00:07.200559…\n329.576546\n\n\n\"std\"\n0.917593\n1597.426008\n15.017044\n3.960893\n0.105744\nnull\n107.559275\n\n\n\"min\"\n0.025615\n40.088585\n0.227812\n0.048355\n0.001512\n\"0:00:02\"\n10.022146\n\n\n\"25%\"\n0.32078\n1347.811045\n11.000497\n0.820266\n0.030138\n\"0:00:04\"\n267.769087\n\n\n\"50%\"\n0.643556\n1946.797109\n16.804255\n1.640207\n0.054373\n\"0:00:06\"\n332.027701\n\n\n\"75%\"\n1.200596\n2916.708805\n25.68928\n3.093725\n0.095973\n\"0:00:09\"\n393.696592\n\n\n\"max\"\n17.98526\n32939.36824\n358.253288\n309.842292\n9.634978\n\"0:00:56\"\n864.808497\n\n\n\n\n\n\n\n\nCode\ninspect_df = all_events_l2[NVARS]\ninspect_df.describe()\n\n\n\n\n\n\nshape: (9, 8)\n\n\n\ndescribe\nd_star\nL_mn\nL_mn_norm\nj0\nj0_norm\nduration\nv_mn\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"count\"\n230.0\n230.0\n230.0\n230.0\n230.0\n\"230\"\n230.0\n\n\n\"null_count\"\n19.0\n19.0\n19.0\n19.0\n19.0\n\"19\"\n19.0\n\n\n\"mean\"\n0.877702\n2532.137047\n20.405935\n2.524317\n0.092205\n\"0:00:07.666787…\n330.599885\n\n\n\"std\"\n0.403323\n511.717175\n5.956418\n1.246758\n0.042522\nnull\n37.204692\n\n\n\"min\"\n0.110787\n1817.802972\n6.912295\n0.234345\n0.048246\n\"0:00:06.390404…\n255.313631\n\n\n\"25%\"\n0.726833\n2184.78267\n18.368551\n1.988046\n0.069698\n\"0:00:06.831121…\n304.836154\n\n\n\"50%\"\n0.941395\n2398.748205\n21.023224\n2.779766\n0.078657\n\"0:00:07.195599…\n329.466847\n\n\n\"75%\"\n1.129473\n2751.132373\n24.169994\n3.241188\n0.092763\n\"0:00:08.172248…\n353.839774\n\n\n\"max\"\n1.775508\n4249.960369\n37.964232\n5.746615\n0.298918\n\"0:00:11.522727…\n444.819813",
    "crumbs": [
      "Results",
      "SWD Properties"
    ]
  },
  {
    "objectID": "analysis/02_properties.html#plotting-function",
    "href": "analysis/02_properties.html#plotting-function",
    "title": "SWD Properties",
    "section": "Plotting function",
    "text": "Plotting function\nPlotting function for Level 1 data.\nSimilar to the geom_bin2d function, but with added functionality\n\nNormalize the data to every x-axis value\nAdd peak values\nAdd mean values with error bars\n\n\nPlotting function for Level 2 averaged data.\n\n\nCode\n# Utility function for plotting\nplot_util &lt;- function(df, x_var, y_vars, xlab, ylabs, color=\"sat\", linetype=\"sat\") {\n  # Initialize an empty list for storing plots\n  plots &lt;- list()\n  \n  # Iterate over each y_var and its corresponding label\n  for (i in seq_along(y_vars)) {\n    y_var &lt;- y_vars[i]\n    y_lab &lt;- ylabs[i]\n    \n    # Create plot for the current y variable\n    p &lt;- ggplot(df, aes(x = .data[[x_var]], y = .data[[y_var]], color = .data[[color]], linetype = .data[[linetype]])) +\n      geom_line() + geom_point() +\n      labs(y = y_lab, x = x_var, color = \"Satellites\", linetype = \"Satellites\") +\n      theme_pubr(base_size = 16) +\n      theme(legend.text = element_text(size = 16)) +\n      scale_color_okabeito(palette = \"black_first\")\n\n    # Apply modifications specific to the position of the plot\n    p &lt;- ggpar(p, legend = \"none\")\n    if (i != length(y_vars)) {\n      p &lt;- ggpar(p, xlab = FALSE)\n    }\n    if (i == length(y_vars)) {\n      p &lt;- ggpar(p, xlab = xlab)\n    }\n    if (i == 1) {\n      p &lt;- ggpar(p, legend = \"top\")\n    }\n    \n    # Add the plot to the list\n    plots[[i]] &lt;- p\n  }\n\n  # Combine all the plots vertically\n  combined_plot &lt;- wrap_plots(plots) + plot_layout(ncol = 1)\n\n  return(combined_plot)\n}\n\n\n\n\nHistogram\n\n\nCode\nplot_limited_histogram &lt;- function(df, x, x_lim, bins, facet_var) {\n    data &lt;- df %&gt;%\n        filter(!!sym(x) &gt;= x_lim[1], !!sym(x) &lt;= x_lim[2])\n\n    p &lt;- gghistogram(data, x = x, y=\"density\", color = \"sat\", bins = bins, facet.by = facet_var)\n    \n    return(p)\n}",
    "crumbs": [
      "Results",
      "SWD Properties"
    ]
  },
  {
    "objectID": "analysis/02_properties.html#delta-b_l",
    "href": "analysis/02_properties.html#delta-b_l",
    "title": "SWD Properties",
    "section": "\\(\\Delta B_l\\)",
    "text": "\\(\\Delta B_l\\)\n\n\nCode\nJNO_events_l1 &lt;- JNO_events_l1 %&gt;%\n    mutate(\n        dB_lmn_x = abs(dB_lmn_x),\n        dB_lmn_x_norm = abs(dB_lmn_x_norm)\n    )\n  \nother_events_l1 &lt;- other_events_l1 %&gt;%\n    mutate(\n        dB_lmn_x = abs(dB_lmn_x),\n        dB_lmn_x_norm = abs(dB_lmn_x_norm)\n    )  \n\ny &lt;- \"dB_lmn_x\"\nylab &lt;- \"Log dB_l (nT)\"\ny_lim &lt;- c(0.1,30)\ny_log &lt;- TRUE\n\n    \np1 &lt;- plot_dist(y=y, ylab=ylab, y_lim = y_lim, y_log = y_log)\n# save_plot(\"dB_l_dist\")\n\ny &lt;- \"dB_lmn_x_norm\"\ny_lim &lt;- c(0,0.618)\nylab &lt;- \"dB_l (&lt;B&gt;)\"\ny_log &lt;- FALSE\np2 &lt;- plot_dist(y=y, ylab=ylab, y_lim = y_lim, y_log = y_log)\n# save_plot(\"dB_l_N1_dist\")\n\np1 | p2",
    "crumbs": [
      "Results",
      "SWD Properties"
    ]
  },
  {
    "objectID": "analysis/02_properties.html#overview",
    "href": "analysis/02_properties.html#overview",
    "title": "SWD Properties",
    "section": "Overview",
    "text": "Overview\n\n\nCode\nthresold = 0.2\n\ndef temp_plot(l1_df, y=\"j0_norm\"):\n    return l1_df.pipe(process_events_l2).hvplot(\n        x=\"time\", y=y, by=\"sat\", hover_cols=[\"id_count\"], width=300, height=300, legend=False\n    )\n\ndirection_col = pl.col('k_x').abs()\n\n((\n    all_events_l1.pipe(temp_plot, y=\"j0_norm\")\n    + all_events_l1.filter(direction_col &gt; thresold).pipe(temp_plot, y=\"j0_norm\")\n    + all_events_l1.filter(direction_col &lt; thresold).pipe(temp_plot, y=\"j0_norm\")\n) + (\n    all_events_l1.pipe(temp_plot, y=\"L_mn_norm\")\n    + all_events_l1.filter(direction_col &gt; thresold).pipe(temp_plot, y=\"L_mn_norm\")\n    + all_events_l1.filter(direction_col &lt; thresold).pipe(temp_plot, y=\"L_mn_norm\")\n)).cols(3)",
    "crumbs": [
      "Results",
      "SWD Properties"
    ]
  },
  {
    "objectID": "analysis/02_properties.html#thickness",
    "href": "analysis/02_properties.html#thickness",
    "title": "SWD Properties",
    "section": "Thickness",
    "text": "Thickness\nNote since want different y-axis titles (labels) for each facet, not different facet titles, it is not clear how to do this with facet_wrap after pivot_longer. Also these are different units, so it is better to plot them separately and combine them together.\n\nEvolution\n\n\nCode\nx_var &lt;- \"time\"\ny_vars &lt;- c(\"L_mn\", \"L_mn_norm\")\nxlab &lt;- \"Time\"\nylabs &lt;- c(\"Thickness (km)\", \"Normalized thickness (d_i)\")\np &lt;- plot_util(all_events_l2, x_var = x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"thickness_time\")\n\nx_var &lt;- \"ref_radial_distance\"\nxlab &lt;- \"Referred Radial Distance (AU)\"\np &lt;- plot_util(all_events_l2, x_var=x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"thickness_r\")\n\ny_vars &lt;- c(\"L_mn_n2\", \"L_mn_norm_n2\")\nylabs &lt;- c(\"Relative thickness\", \"Normalized relative thickness\")\np &lt;- plot_util(all_events_l2, x_var=x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"thickness_r_N2\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nIn addition: There were 20 warnings (use warnings() to see them)\n\n\nResults\n  \n\n\nDifferent thickness standard\n\n\nCode\nx_var &lt;- \"ref_radial_distance\"\ny_vars &lt;- c(\"L_mn\", \"L_n\", \"L_k\")\nylabs &lt;- c(\"MN thickness\", \"N thickness\", \"k thickness\")\np &lt;- plot_util(all_events_l2, x_var=x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"thickness/thickness_r_diff\")\n\ny_vars &lt;- c(\"L_mn_norm\", \"L_n_norm\", \"L_k_norm\")\nylabs &lt;- c(\"MN thickness\", \"N thickness\", \"k thickness\")\np &lt;- plot_util(all_events_l2, x_var=x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"thickness/thickness_norm_r_diff\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nIn addition: There were 24 warnings (use warnings() to see them)\n\n\nResults\n \n\n\nMap\n\n\nCode\nx_col &lt;- \"radial_distance\"\ny_col &lt;- \"L_mn\"\ny_lim &lt;- NULL\nxlab &lt;- \"Radial Distance (AU)\"\nylab &lt;- \"Log Thickness (km)\"\np &lt;- plot_binned_data(JNO_events_l1, x_col = x_col, y_col = y_col, x_bins = 8, y_bins = 32, y_lim = y_lim, log_y = TRUE)\np &lt;- p + labs(x = xlab, y= ylab)\nsave_plot(\"thickness/thickness_r_dist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\n\n\nCode\nplot_threshold &lt;- function(df, x, y, cols, k_threshold = 0.75, x_bins = 8, y_bins = 16, y_lim = NULL, xlab = NULL, ylab = NULL, larger_only = FALSE) {\n  # Initialize an empty list to store plots\n  plots &lt;- list()\n\n  for (col in cols) {\n    # Create dynamic filter expression for greater than\n    filter_expr_gt &lt;- expr(abs(!!ensym(col)) &gt; k_threshold)\n\n    # Apply the filter and create plot for greater than\n    data_gt &lt;- df %&gt;% filter(!!filter_expr_gt)\n    plot_gt &lt;- plot_binned_data(data_gt, x_col = x, y_col = y, x_bins = x_bins, y_bins = y_bins, y_lim = y_lim, log_y = TRUE) +\n      labs(x = xlab, y = ylab, title = paste0(col, \" &gt; \", k_threshold, \" (N = \", nrow(data_gt), \")\"))\n\n    # Add the plot to the list\n    plots[[length(plots) + 1]] &lt;- plot_gt\n\n    # Check if plots for less than or equal are also needed\n    if (!larger_only) {\n      filter_expr_le &lt;- expr(abs(!!ensym(col)) &lt;= k_threshold)\n      data_le &lt;- df %&gt;% filter(!!filter_expr_le)\n      plot_le &lt;- plot_binned_data(data_le, x_col = x, y_col = y, x_bins = x_bins, y_bins = y_bins, y_lim = y_lim, log_y = TRUE) +\n        labs(x = xlab, y = ylab, title = paste0(col, \" &lt;= \", k_threshold, \" (N = \", nrow(data_le), \")\"))\n\n      # Add the plot to the list\n      plots[[length(plots) + 1]] &lt;- plot_le\n    }\n  }\n\n  # Combine all plots\n  do.call(patchwork::wrap_plots, c(plots, list(ncol = ifelse(larger_only, 1, 2))))\n}\n\nplot_threshold &lt;- function(df, x, y, cols, k_threshold = 0.75, x_bins = 8, y_bins = 16, y_lim = NULL, xlab = NULL, ylab = NULL, operators = c(\"&gt;\", \"&lt;\")) {\n  # Initialize an empty list to store plots\n  plots &lt;- list()\n\n  for (col in cols) {\n    for (op in operators) {\n      # Create dynamic filter expression based on the operator\n      if (op == \"&gt;\") {\n        filter_expr &lt;- expr(abs(!!ensym(col)) &gt; k_threshold)\n      } else if (op == \"&lt;\") {\n        filter_expr &lt;- expr(abs(!!ensym(col)) &lt; k_threshold)\n      } else {\n        stop(\"Invalid operator: choose '&gt;' or '&lt;'\")\n      }\n\n      # Apply the filter and create plot\n      data_filtered &lt;- df %&gt;% filter(!!filter_expr)\n      plot_filtered &lt;- plot_binned_data(data_filtered, x_col = x, y_col = y, x_bins = x_bins, y_bins = y_bins, y_lim = y_lim, log_y = TRUE) +\n        labs(x = xlab, y = ylab, title = paste0(col, \" \", op, \" \", k_threshold, \" (N = \", nrow(data_filtered), \")\"))\n\n      # Add the plot to the list\n      plots[[length(plots) + 1]] &lt;- plot_filtered\n    }\n  }\n\n  # Combine all plots\n  n_cols &lt;- ifelse(length(operators) == 1, 1, 2)\n  do.call(patchwork::wrap_plots, c(plots, list(ncol = n_cols)))\n}\n\nplot_effect &lt;- function(df, cols, y_vars, x = \"radial_distance\", xlab = \"Radial Distance (AU)\", k_threshold = 0.75, operators = \"&gt;\") {\n  # Initialize an empty list to store plots\n  plots &lt;- list()\n\n  for (y_var in y_vars) {\n    y &lt;- y_var$y\n    ylab &lt;- y_var$ylab\n    y_lim &lt;- y_var$y_lim\n\n    # Generate plot for each y variable\n    plot &lt;- plot_threshold(df, x, y, cols, k_threshold = k_threshold, xlab = xlab, ylab = ylab, y_lim = y_lim, operators = operators)\n    plots[[length(plots) + 1]] &lt;- plot\n  }\n\n  # Combine all plots\n  do.call(patchwork::wrap_plots, c(plots, list(nrow = 1)))\n}\n\n\nWe examine the effects of orientation \\(k\\) on the evolution of the thickness.\n\n\nCode\ny_vars_l &lt;- list(\n  list(y = \"L_k\", ylab = \"Log Thickness (km)\", y_lim = c(500, 10000)),\n  list(y = \"L_k_norm\", ylab = \"Log Thickness (d_i)\", y_lim = c(1, 100))\n)\n\ny_vars_j &lt;- list(\n  list(y = \"j0\", ylab = \"Log J (nA/m^2)\", y_lim = c(0.05, 10)),\n  list(y = \"j0_norm\", ylab = \"Log J (J_A)\", y_lim = NULL)\n)\n\n# combine two groups\ny_vars &lt;- c(y_vars_l, y_vars_j)\n\n\n\n\nCode\ncols &lt;- c(\"k_x\", \"k_y\", \"k_z\")\nplot_effect(JNO_events_l1, cols, y_vars)\n\n\n\n\n\n\n\n\n\n\n\n\nWe examine the effects of max variation orientation \\(V_l\\) on the evolution of the thickness.\n\n\nCode\ncols &lt;- c(\"Vl_x\", \"Vl_y\", \"Vl_z\")\nplot_effect(JNO_events_l1, cols, y_vars)\n\n\n\n\n\n\n\n\n\n\n\n\nWe examine the effects of max variation orientation \\(V_l\\) on the evolution of the thickness, now dividing into two groups depending on whether or not out of eclipic plane.\n\n\nCode\ncols &lt;- c(\"Vl_z\")\n\nplot_effect(JNO_events_l1, cols, y_vars_l, operators = \"&gt;\") / plot_effect(JNO_events_l1, cols, y_vars_l, k_threshold = 0.25, operators = \"&lt;\") |\nplot_effect(JNO_events_l1, cols, y_vars_j, operators = \"&gt;\") / plot_effect(JNO_events_l1, cols, y_vars_j, k_threshold = 0.25, operators = \"&lt;\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\n\n\nCode\ny &lt;- \"L_mn\"\nylab &lt;- \"Log Thickness (km)\"\nim &lt;- c(500,13000)\np1 &lt;- plot_dist(y=y, ylab=ylab, y_lim = y_lim)\nsave_plot(\"thickness/thickness_mn_dist\")\n\ny &lt;- \"L_k\"\np2 &lt;- plot_dist(y=y, ylab=ylab, y_lim = y_lim)\nsave_plot(\"thickness/thickness_k_dist\")\n\n\ny &lt;- \"L_mn_norm\"\nylab &lt;- expression(Log~Thickness~(d[i]))\ny_lim &lt;- c(1,100)\np3 &lt;- plot_dist(y=y, ylab=ylab, y_lim = y_lim)\nsave_plot(\"thickness/thickness_mn_N1_dist\")\n\ny &lt;- \"L_k_norm\"\nylab &lt;- expression(Log~Thickness~(d[i]))\ny_lim &lt;- c(1,100)\np4 &lt;- plot_dist(y=y, ylab=ylab, y_lim = y_lim)\nsave_plot(\"thickness/thickness_k_N1_dist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\n\n\nHistogram\n\n\nCode\ndf &lt;- JNO_events_l1\ndf$r_bin &lt;- as.factor(df$r_bin)\n\ncolor &lt;- \"r_bin\"\nx &lt;- \"L_mn\"\nx_lim &lt;- c(500,10000)\np1 &lt;- ggdensity(df, x = x, color = color, add = \"mean\") + xlim(x_lim) + scale_color_okabeito(palette = \"black_first\")\n\nx &lt;- \"L_mn_norm\"\nx_lim &lt;- c(0,60)\np2 &lt;- ggdensity(df, x = x, color = color, add = \"mean\") + xlim(x_lim) + scale_color_okabeito(palette = \"black_first\")\n\n\nx &lt;- \"j0\"\nx_lim &lt;- c(0,5)\np3 &lt;- ggdensity(df, x = x, color = color, add = \"mean\") + xlim(x_lim) + scale_color_okabeito(palette = \"black_first\")\n\nx &lt;- \"j0_norm\"\nx_lim &lt;- c(0,1)\np4 &lt;- ggdensity(df, x = x, color = color, add = \"mean\") + xlim(x_lim) + scale_color_okabeito(palette = \"black_first\")\n\n(p1 / p2) | (p3 / p4)\n\n\n[12/31/23 10:56:47] WARNING  R[write to console]: In addition:                                     callbacks.py:124\n\n\n\n                    WARNING  R[write to console]: Warning messages:                                callbacks.py:124\n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: 1: Removed 623 rows containing non-finite values callbacks.py:124\n                             (`stat_density()`).                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: 2: Removed 263 rows containing non-finite values callbacks.py:124\n                             (`stat_density()`).                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: 3: Removed 638 rows containing non-finite values callbacks.py:124\n                             (`stat_density()`).                                                                   \n                                                                                                                   \n\n\n\n                    WARNING  R[write to console]: 4: Removed 96 rows containing non-finite values  callbacks.py:124\n                             (`stat_density()`).                                                                   \n                                                                                                                   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- \"L_mn_norm\"\nx_lim &lt;- c(0,60)\nbins &lt;- 16\nfacet_var &lt;- \"r_bin\"\np &lt;- plot_limited_histogram(all_events_l1, x = x, x_lim = x_lim , bins = bins, facet_var=facet_var)\nsave_plot(\"thickness_N1_r_hist\")\n\nx &lt;- \"L_mn_norm_log\"\nx_lim &lt;- c(0, 2)\nfacet_var &lt;- \"r_bin\"\np &lt;- plot_limited_histogram(all_events_l1, x = x, x_lim = x_lim , bins = bins, facet_var=facet_var)\nsave_plot(\"thickness_N1_log_r_hist\")\n\n\nSaving 16.7 x 11.1 in image\nSaving 16.7 x 11.1 in image\nSaving 16.7 x 11.1 in image\nSaving 16.7 x 11.1 in image\n\n\nResults",
    "crumbs": [
      "Results",
      "SWD Properties"
    ]
  },
  {
    "objectID": "analysis/02_properties.html#current-intensity",
    "href": "analysis/02_properties.html#current-intensity",
    "title": "SWD Properties",
    "section": "Current intensity",
    "text": "Current intensity\n\nMean value\n\n\nCode\nx_var &lt;- \"time\"\ny_vars &lt;- c(\"j0\", \"j0_norm\")\nxlab &lt;- \"Time\"\nylabs &lt;- c(\"J (nA/m^2)\",  \"Normalized J (J_A)\")\np &lt;- plot_util(all_events_l2, x_var = x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"current_time\")\n\nx_var &lt;- \"ref_radial_distance\"\nxlab &lt;- \"Referred Radial Distance (AU)\"\np &lt;- plot_util(all_events_l2, x_var=x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"current_r\")\n\ny_vars &lt;- c(\"j0_n2\", \"j0_norm_n2\")\nylabs &lt;- c(\"Relative J\", \"Relative normalized J\")\np &lt;- plot_util(all_events_l2, x_var=x_var, y_vars = y_vars, xlab=xlab, ylabs=ylabs)\nsave_plot(\"current_r_N2\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nIn addition: There were 20 warnings (use warnings() to see them)\n\n\nResults\n  \n\n\nMap\n\n\nCode\nx_col &lt;- \"radial_distance\"\ny_col &lt;- \"j0\"\ny_lim &lt;- c(0, 15)\nxlab &lt;- \"Radial Distance (AU)\"\nylab &lt;- expression(Log~J~(nA~m^-2))\np &lt;- plot_binned_data(JNO_events_l1, x_col = x_col, y_col = y_col, x_bins = 8, y_bins = 32, y_lim = y_lim, log_y = TRUE)\np &lt;- p + labs(x = xlab, y= ylab)\nsave_plot(\"current_r_dist\")\n\ny_col &lt;- \"j0_norm\"\ny_lim &lt;- c(0.01, 1)\nylab &lt;- expression(Log~Normalized~J~(J[A]))\np &lt;- plot_binned_data(JNO_events_l1, x_col = x_col, y_col = y_col, x_bins = 8, y_bins = 32, y_lim = y_lim, log_y = TRUE)\np &lt;- p + labs(x = xlab, y= ylab)\nsave_plot(\"current_N1_r_dist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\n\n\nDistribution\n\n\nCode\ny &lt;- \"j0\"\nylab &lt;- expression(Log~J~(nA~m^-2))\ny_lim &lt;- c(0.01, 15)\np &lt;- plot_dist(y=y, ylab=ylab, y_lim = y_lim)\nsave_plot(\"current/current_mn_dist\")\n\ny &lt;- \"j0_k\"\np &lt;- plot_dist(y=y, ylab=ylab, y_lim = y_lim)\nsave_plot(\"current/current_k_dist\")\n\n\ny &lt;- \"j0_norm\"\nylab &lt;- expression(Log~J~(J[A]))\ny_lim &lt;- c(0.01, 1)\np &lt;- plot_dist(y=y, ylab=ylab, y_lim = y_lim)\nsave_plot(\"current/current_mn_N1_dist\")\n\ny &lt;- \"j0_k_norm\"\np &lt;- plot_dist(y=y, ylab=ylab, y_lim = y_lim)\nsave_plot(\"current/current_k_N1_dist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\n\n\nHistogram\n\n\nCode\nx &lt;- \"j0_norm\"\nx_lim &lt;- c(0, 1)\nfacet_var &lt;- \"r_bin\"\np &lt;- plot_limited_histogram(all_events_l1, x = x, x_lim = x_lim, bins = 8, facet_var=facet_var)\nsave_plot(\"current_N1_r_hist\")\n\nx &lt;- \"j0_norm_log\"\nx_lim &lt;- c(-2, 0)\np &lt;- plot_limited_histogram(all_events_l1, x = x, x_lim = x_lim, bins = 8, facet_var=facet_var)\nsave_plot(\"current_N1_log_r_hist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nResults",
    "crumbs": [
      "Results",
      "SWD Properties"
    ]
  },
  {
    "objectID": "analysis/03_map.html",
    "href": "analysis/03_map.html",
    "title": "Map of thickness and current intensity",
    "section": "",
    "text": "Bin the data and fit the shape does not work. Using 2d gaussian kernel density estimation instead.\n\n\nCode\ndef bin_df(df: pl.DataFrame, col_to_bin, bins=10):\n    binned_col = f\"{col_to_bin}_bin\"\n    \n    return (\n        df.with_columns(\n            pl.col(col_to_bin).qcut(bins).alias(binned_col),\n        )\n        .group_by(binned_col)\n        .agg(cs.numeric().median(), pl.count().alias(\"bin_count\"))\n        .drop(binned_col)\n    )\n\ncol_to_bin=\"L_mn_norm_log\"\n# col_to_bin=\"j0_norm_log\"\n\nall_events_l1_L_binned = pl.concat(\n    [\n        data.pipe(bin_df, col_to_bin=col_to_bin, bins=64).with_columns(sat= pl.lit(name))\n        for name, data in all_events_l1.group_by(\"sat\")\n    ]\n)\n\njno_events_l1_L_binned = pl.concat(\n    [\n        data.pipe(bin_df, col_to_bin=col_to_bin, bins=64).with_columns(sat= pl.lit(name))\n        for name, data in jno_candidates_l1.group_by(\"r_bin\")\n    ]\n)\n\n\n\n\nCode\ndf = all_events_l1.filter(pl.col('L_mn_norm_log').is_not_nan())\n# df = jno_candidates_l1.filter(pl.col('L_mn_norm_log').is_not_nan())\n\n\n\n\nCode\nimport lmfit\nimport numpy as np\n\ndef gaussian2d(x, y, amplitude=1., centerx=0., centery=0., sigmax=1., sigmay=1.,\n                 rotation=0, A0=0.):\n    \"\"\"Return a two dimensional lorentzian.\n\n    The maximum of the peak occurs at ``centerx`` and ``centery``\n    with widths ``sigmax`` and ``sigmay`` in the x and y directions\n    respectively. The peak can be rotated by choosing the value of ``rotation``\n    in radians.\n    \"\"\"\n    xp = (x - centerx)*np.cos(rotation) - (y - centery)*np.sin(rotation)\n    yp = (x - centerx)*np.sin(rotation) + (y - centery)*np.cos(rotation)\n    R = (xp/sigmax)**2 + (yp/sigmay)**2\n\n    return A0 + amplitude * np.exp(-R/2)\n\nmodel = lmfit.Model(gaussian2d, independent_vars=['x', 'y'])\n# params = model.make_params(amplitude=10, centerx=x[np.argmax(z)], centery=y[np.argmax(z)])\n\n\n\n\nCode\nfit_gaussian_2D_pdf(df)\n\n\n\n\nCode\nlibrary(purrr)\nlibrary(tidyr)\nfit_gaussian_2D_pdf &lt;- function(data) {\n  kde_result &lt;- MASS::kde2d(data$L_mn_norm_log, data$j0_norm_log)\n  x_values &lt;- rep(kde_result$x, each = length(kde_result$y))\n  y_values &lt;- rep(kde_result$y, length(kde_result$x))\n  response &lt;- as.vector(kde_result$z)\n\n  density &lt;- data.frame(X_values = x_values, Y_values = y_values, response = response)\n  model &lt;- fit_gaussian_2D(density)\n  return(model)\n}\n\nresults &lt;- df %&gt;% \n  # group_by(r_bin) %&gt;% \n  nest() %&gt;% \n  mutate(fitted = map(data, fit_gaussian_2D_pdf))\n\n\n\n\nCode\n# Creating a list of layers for the binned data\n# model &lt;- lm(j0_norm_log ~ L_mn_norm_log, data = all_events_l1_L_binned)\n# slope &lt;- coef(model)[2]\n\nbinned_layer &lt;- list(\n  geom_line(data = all_events_l1_L_binned, color = 'blue'),\n  geom_point(data = all_events_l1_L_binned, color = 'blue'), \n  geom_smooth(data = all_events_l1_L_binned, method = \"glm\", color = 'red')\n)\n\n\n# Plot creation\np &lt;- ggplot(mapping = aes(x = L_mn_norm_log, y = j0_norm_log)) +\n  geom_density_2d(data = all_events_l1) +\n  # stat_density_2d(data = all_events_l1, aes(fill = after_stat(density)), geom = \"raster\", contour = FALSE) +\n  binned_layer +\n  facet_wrap(~ sat, scales = \"free\")\n\n  \n# Print the plot\nprint(p)\n\n\n\n\nCode\np &lt;- ggplot() +\n  geom_point(data = all_events_l1, aes(x = L_mn_norm_log, y = j0_norm_log)) +\n  binned_layer +\n  facet_wrap(~ sat, scales = \"free\")\n\nprint(p)\n\n\n\n\nCode\n# Fit a linear model to the log-transformed data\nlm_fit &lt;- lm(j0_norm_log ~ L_mn_norm_log, data = all_events_l1)\n\n# Extract the coefficients\nintercept &lt;- coef(lm_fit)[1]\nslope &lt;- coef(lm_fit)[2]\n\n# Create a scatter plot with the log-log transformation\np &lt;- ggplot(all_events_l1, aes(x = L_mn_norm_log, y = j0_norm_log)) +\n  geom_point() + # Add the scatter points\n  geom_abline(intercept = intercept, slope = slope, color = 'blue', size = 1) + # Add the fitted line\n  facet_wrap(~ sat, scales = \"free\") + # Facet by 'sat'\n  labs(x = \"Log10(L_mn_norm)\", y = \"Log10(j0_norm)\") # Label axes\n\nprint(p)\n\n\n\n\nCode\n# Plot creation\np &lt;- ggplot(all_events_l1_L_binned, aes(x = L_mn_norm_log, y = j0_norm_log)) +\n    geom_line(color = 'blue') +\n    geom_point(color = 'blue') +\n    geom_smooth(method = \"glm\", color = 'red') +\n    facet_wrap(~ sat, scales = \"free\") +\n    stat_regline_equation()\n\n  \n# Print the plot\nprint(p)\n\n\n\n\nCode\n# Plot creation\np &lt;- ggplot(jno_events_l1_L_binned, aes(x = L_mn_norm_log, y = j0_norm_log)) +\n    geom_line(color = 'blue') +\n    geom_point(color = 'blue') +\n    geom_smooth(method = \"glm\", color = 'red') +\n    facet_wrap(~ r_bin, scales = \"free\") +\n    stat_regline_equation()\n\n  \n# Print the plot\nprint(p)\n\n\n\n\nCode\np &lt;- ggplot(jno_candidates_l1, aes(x = L_mn_norm, y = j0_norm)) +\n  stat_density_2d(aes(fill = ..density..), geom = \"raster\", contour = FALSE) +\n  facet_wrap(~ r_bin, nrow = length(unique(jno_candidates_l1$r_bin))) +\n  scale_x_log10() + \n  scale_y_log10() +\n  labs(fill = \"Density\")\n\n\nprint(p)",
    "crumbs": [
      "Results",
      "Map of thickness and current intensity"
    ]
  },
  {
    "objectID": "missions/stereo/mag.html",
    "href": "missions/stereo/mag.html",
    "title": "STEREO Magnetic field data pipeline",
    "section": "",
    "text": "STEREO magnetic field is already in RTN coordinates, so no need to transform it.\nDownload data using pyspedas, but load it using pycdfpp (using pyspedas to load the data directly into xarray is very slow)",
    "crumbs": [
      "Missions",
      "IDs from STEREO",
      "STEREO Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/stereo/mag.html#loading-data",
    "href": "missions/stereo/mag.html#loading-data",
    "title": "STEREO Magnetic field data pipeline",
    "section": "Loading data",
    "text": "Loading data\n\nsource\n\nload_data\n\n load_data (start, end, datatype='8hz', probe:str='a')\n\n\nsource\n\n\ndownload_data\n\n download_data (start, end, probe:str='a', datatype='8hz')\n\nList of CDF files",
    "crumbs": [
      "Missions",
      "IDs from STEREO",
      "STEREO Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/stereo/mag.html#preprocessing-data",
    "href": "missions/stereo/mag.html#preprocessing-data",
    "title": "STEREO Magnetic field data pipeline",
    "section": "Preprocessing data",
    "text": "Preprocessing data\n\nsource\n\npreprocess_data\n\n preprocess_data (raw_data, var_names='BFIELD')\n\nPreprocess the raw dataset (only minor transformations) - Applying naming conventions for columns",
    "crumbs": [
      "Missions",
      "IDs from STEREO",
      "STEREO Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/stereo/mag.html#processing-data",
    "href": "missions/stereo/mag.html#processing-data",
    "title": "STEREO Magnetic field data pipeline",
    "section": "Processing data",
    "text": "Processing data\n\nsource\n\nprocess_data\n\n process_data\n               (raw_data:Dict[str,Callable[...,polars.lazyframe.frame.Lazy\n               Frame]], ts)\n\n\n\n\n\nType\nDetails\n\n\n\n\nraw_data\nDict\n\n\n\nts\n\ntime resolution, in seconds",
    "crumbs": [
      "Missions",
      "IDs from STEREO",
      "STEREO Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/stereo/mag.html#pipeline",
    "href": "missions/stereo/mag.html#pipeline",
    "title": "STEREO Magnetic field data pipeline",
    "section": "Pipeline",
    "text": "Pipeline\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='STA', source='MAG')",
    "crumbs": [
      "Missions",
      "IDs from STEREO",
      "STEREO Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/wind/state.html",
    "href": "missions/wind/state.html",
    "title": "Wind State data pipeline",
    "section": "",
    "text": "We use low resolution OMNI data for plasma state data, as we did in the OMNI notebook",
    "crumbs": [
      "Missions",
      "IDs from Wind",
      "Wind State data pipeline"
    ]
  },
  {
    "objectID": "missions/wind/state.html#loading-data",
    "href": "missions/wind/state.html#loading-data",
    "title": "Wind State data pipeline",
    "section": "Loading data",
    "text": "Loading data",
    "crumbs": [
      "Missions",
      "IDs from Wind",
      "Wind State data pipeline"
    ]
  },
  {
    "objectID": "missions/wind/state.html#pipelines",
    "href": "missions/wind/state.html#pipelines",
    "title": "Wind State data pipeline",
    "section": "Pipelines",
    "text": "Pipelines\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='Wind', source='STATE')",
    "crumbs": [
      "Missions",
      "IDs from Wind",
      "Wind State data pipeline"
    ]
  },
  {
    "objectID": "missions/wind/index.html",
    "href": "missions/wind/index.html",
    "title": "IDs from Wind",
    "section": "",
    "text": "See following notebooks for details:\nReferences:\nNotes:",
    "crumbs": [
      "Missions",
      "IDs from Wind"
    ]
  },
  {
    "objectID": "missions/wind/index.html#setup",
    "href": "missions/wind/index.html#setup",
    "title": "IDs from Wind",
    "section": "Setup",
    "text": "Setup\nNeed to run command in shell first as pipeline is project-specific command\nkedro pipeline create wind\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='Wind', params:Optional[dict]=None)",
    "crumbs": [
      "Missions",
      "IDs from Wind"
    ]
  },
  {
    "objectID": "missions/themis/state.html",
    "href": "missions/themis/state.html",
    "title": "THEMIS State data pipeline",
    "section": "",
    "text": "We use low resolution OMNI data for plasma state data, as we did in the OMNI notebook",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS State data pipeline"
    ]
  },
  {
    "objectID": "missions/themis/state.html#solar-wind-state",
    "href": "missions/themis/state.html#solar-wind-state",
    "title": "THEMIS State data pipeline",
    "section": "Solar wind state",
    "text": "Solar wind state\nAlso we have additional data file that indicate if THEMIS is in solar wind or not.\n\nsource\n\nload_sw_data\n\n load_sw_data (raw_data:pandas.core.frame.DataFrame)\n\n\nsource\n\n\npreprocess_sw_data\n\n preprocess_sw_data (raw_data:polars.lazyframe.frame.LazyFrame)\n\n\nApplying naming conventions for columns\nParsing and typing data (like from string to datetime for time columns)",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS State data pipeline"
    ]
  },
  {
    "objectID": "missions/themis/state.html#pipelines",
    "href": "missions/themis/state.html#pipelines",
    "title": "THEMIS State data pipeline",
    "section": "Pipelines",
    "text": "Pipelines\n\nsource\n\ncreate_sw_pipeline\n\n create_sw_pipeline (sat_id='THB', source='STATE')\n\n\nsource\n\n\ncreate_pipeline\n\n create_pipeline (sat_id='THB', source='STATE')",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS",
      "THEMIS State data pipeline"
    ]
  },
  {
    "objectID": "missions/themis/index.html",
    "href": "missions/themis/index.html",
    "title": "IDs from ARTHEMIS",
    "section": "",
    "text": "See following notebooks for details:",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS"
    ]
  },
  {
    "objectID": "missions/themis/index.html#background",
    "href": "missions/themis/index.html#background",
    "title": "IDs from ARTHEMIS",
    "section": "Background",
    "text": "Background\nARTEMIS spacecrafts will be exposed in the solar wind at 1 AU during its orbits around the Moon. So it’s very interesting to look into its data.\n\nFor time inteval for THEMIS-B in solar wind, see Link\nFor time inteval for THEMIS-C in solar wind, see Link",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS"
    ]
  },
  {
    "objectID": "missions/themis/index.html#setup",
    "href": "missions/themis/index.html#setup",
    "title": "IDs from ARTHEMIS",
    "section": "Setup",
    "text": "Setup\nNeed to run command in shell first as pipeline is project-specific command\nkedro pipeline create themis\nTo get candidates data, run kedro run --from-inputs=jno.feature_1s --to-outputs=candidates.jno_1s",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS"
    ]
  },
  {
    "objectID": "missions/themis/index.html#solar-wind-pipeline",
    "href": "missions/themis/index.html#solar-wind-pipeline",
    "title": "IDs from ARTHEMIS",
    "section": "Solar wind pipeline",
    "text": "Solar wind pipeline\n\nsource\n\ncreate_sw_events_pipeline\n\n create_sw_events_pipeline (sat_id, tau:int=60, ts_mag:int=1)\n\n\nsource\n\n\nfilter_sw_events\n\n filter_sw_events (events:polars.lazyframe.frame.LazyFrame,\n                   sw_state:polars.lazyframe.frame.LazyFrame)",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS"
    ]
  },
  {
    "objectID": "missions/themis/index.html#pipelines",
    "href": "missions/themis/index.html#pipelines",
    "title": "IDs from ARTHEMIS",
    "section": "Pipelines",
    "text": "Pipelines\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='THB', params:Optional[dict]=None)",
    "crumbs": [
      "Missions",
      "IDs from ARTHEMIS"
    ]
  },
  {
    "objectID": "missions/juno/mag.html",
    "href": "missions/juno/mag.html",
    "title": "JUNO Magnetic field data pipeline",
    "section": "",
    "text": "JUNO Magnetic field data can be downloaded from PDS website.",
    "crumbs": [
      "Missions",
      "IDs from Juno",
      "JUNO Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/juno/mag.html#downloading-data",
    "href": "missions/juno/mag.html#downloading-data",
    "title": "JUNO Magnetic field data pipeline",
    "section": "Downloading data",
    "text": "Downloading data\n\nsource\n\ndownload_data\n\n download_data (start=None, end=None,\n                phase:Literal['CRUISE','JUPITER']='CRUISE',\n                coord:Literal['SE','SS','PL']='SE',\n                datatype:Literal['1SEC','1MIN']='1SEC')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart\nNoneType\nNone\n\n\n\nend\nNoneType\nNone\n\n\n\nphase\nLiteral\nCRUISE\n\n\n\ncoord\nLiteral\nSE\n\n\n\ndatatype\nLiteral\n1SEC\ntime resolution\n\n\nReturns\nlist\n\n\n\n\n\n\nsource\n\n\nload_data\n\n load_data (start, end, datatype:str='1SEC')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart\n\n\n\n\n\nend\n\n\n\n\n\ndatatype\nstr\n1SEC\ntime resolution\n\n\nReturns\nDataFrame\n\n\n\n\n\n\nsource\n\n\nparse_fp\n\n parse_fp (fp)",
    "crumbs": [
      "Missions",
      "IDs from Juno",
      "JUNO Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/juno/mag.html#preprocessing-data",
    "href": "missions/juno/mag.html#preprocessing-data",
    "title": "JUNO Magnetic field data pipeline",
    "section": "Preprocessing data",
    "text": "Preprocessing data\nConvert all files from lbl format to parquet format for faster processing\n\nsource\n\npreprocess_data\n\n preprocess_data (raw_data:polars.dataframe.frame.DataFrame)\n\nPreprocess the raw dataset (only minor transformations)\n\nApplying naming conventions for columns\nParsing and typing data\nChanging storing format (from lbl to parquet)\nDropping useless columns",
    "crumbs": [
      "Missions",
      "IDs from Juno",
      "JUNO Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/juno/mag.html#processing-data",
    "href": "missions/juno/mag.html#processing-data",
    "title": "JUNO Magnetic field data pipeline",
    "section": "Processing data",
    "text": "Processing data\n\nsource\n\nprocess_data\n\n process_data (raw_data:polars.dataframe.frame.DataFrame, ts:str=None)\n\nPartitioning data, for the sake of memory\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nDataFrame\n\n\n\n\nts\nstr\nNone\ntime resolution\n\n\nReturns\npolars.dataframe.frame.DataFrame | dict[str, polars.dataframe.frame.DataFrame]",
    "crumbs": [
      "Missions",
      "IDs from Juno",
      "JUNO Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/juno/mag.html#pipeline",
    "href": "missions/juno/mag.html#pipeline",
    "title": "JUNO Magnetic field data pipeline",
    "section": "Pipeline",
    "text": "Pipeline\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='JNO', source='MAG')",
    "crumbs": [
      "Missions",
      "IDs from Juno",
      "JUNO Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/juno/mag.html#dataset-overview",
    "href": "missions/juno/mag.html#dataset-overview",
    "title": "JUNO Magnetic field data pipeline",
    "section": "Dataset Overview",
    "text": "Dataset Overview\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\n\n\nIndex\n\n\nCode\npds_dir = \"https://pds-ppi.igpp.ucla.edu/data\"\n\npossible_coords = [\"se\", \"ser\", \"pc\", \"ss\", \"pl\"]\npossible_exts = [\"sts\", \"lbl\"]\npossible_data_rates = [\"1s\", \"1min\", \"1h\"]\n\njuno_ss_config = {\n    \"DATA_SET_ID\": \"JNO-SS-3-FGM-CAL-V1.0\",\n    \"FILE_SPECIFICATION_NAME\": \"INDEX/INDEX.LBL\",\n}\n\njuno_j_config = {\n    \"DATA_SET_ID\": \"JNO-J-3-FGM-CAL-V1.0\",\n    \"FILE_SPECIFICATION_NAME\": \"INDEX/INDEX.LBL\",\n}\n\n\n\nProcess index\n\nsource\n\n\n\nprocess_jno_index\n\n process_jno_index (df:pandas.core.frame.DataFrame)\n\n\nPipleline\n\nsource\n\n\n\ncreate_jno_index_pipeline\n\n create_jno_index_pipeline ()\n\n\n\nCode\ncatalog = load_catalog()\n\nraw_JNO_SS_index = catalog.load(\"raw_JNO_SS_index\")\nraw_JNO_J_index = catalog.load(\"raw_JNO_J_index\")\njno_index = catalog.load(\"JNO_index\")\n\njno_ss_index = jno_index[lambda df: df[\"DATA_SET_ID\"] == \"JNO-SS-3-FGM-CAL-V1.0\"]\njno_j_index = jno_index[lambda df: df[\"DATA_SET_ID\"] == \"JNO-J-3-FGM-CAL-V1.0\"]\n\n\n\nCheck the data\n\n\nCode\nfrom fastcore.utils import coll_repr",
    "crumbs": [
      "Missions",
      "IDs from Juno",
      "JUNO Magnetic field data pipeline"
    ]
  },
  {
    "objectID": "missions/omni/index.html",
    "href": "missions/omni/index.html",
    "title": "OMNI data",
    "section": "",
    "text": "Reference:\nNotes:",
    "crumbs": [
      "Missions",
      "OMNI data"
    ]
  },
  {
    "objectID": "missions/omni/index.html#setup",
    "href": "missions/omni/index.html#setup",
    "title": "OMNI data",
    "section": "Setup",
    "text": "Setup\nNeed to run command in shell first as pipeline is project-specific command\nkedro pipeline create omni",
    "crumbs": [
      "Missions",
      "OMNI data"
    ]
  },
  {
    "objectID": "missions/omni/index.html#downloading-data",
    "href": "missions/omni/index.html#downloading-data",
    "title": "OMNI data",
    "section": "Downloading data",
    "text": "Downloading data\n\nsource\n\nload_data\n\n load_data (start, end, datatype='hourly', vars:dict={'N': {'COLNAME':\n            'plasma_density', 'FIELDNAM': 'Ion density', 'UNITS': 'Per\n            cc'}, 'T': {'COLNAME': 'plasma_temperature', 'FIELDNAM':\n            'Plasma temperature', 'UNITS': 'K'}, 'V': {'COLNAME':\n            'plasma_speed', 'FIELDNAM': 'Flow speed', 'UNITS': 'km/s'},\n            'THETA-V': {'COLNAME': 'sw_vel_theta', 'FIELDNAM': 'Flow\n            latitude', 'UNITS': 'Deg'}, 'PHI-V': {'COLNAME': 'sw_vel_phi',\n            'FIELDNAM': 'Flow longitude', 'UNITS': 'Deg'}, 'BX_GSE':\n            {'COLNAME': 'B_background_x'}, 'BY_GSE': {'COLNAME':\n            'B_background_y'}, 'BZ_GSE': {'COLNAME': 'B_background_z'}})\n\n\nsource\n\n\ndownload_data\n\n download_data (start, end, datatype)",
    "crumbs": [
      "Missions",
      "OMNI data"
    ]
  },
  {
    "objectID": "missions/omni/index.html#preprocessing-data",
    "href": "missions/omni/index.html#preprocessing-data",
    "title": "OMNI data",
    "section": "Preprocessing data",
    "text": "Preprocessing data\n\nsource\n\npreprocess_data\n\n preprocess_data (raw_data:polars.lazyframe.frame.LazyFrame,\n                  vars:dict={'N': {'COLNAME': 'plasma_density',\n                  'FIELDNAM': 'Ion density', 'UNITS': 'Per cc'}, 'T':\n                  {'COLNAME': 'plasma_temperature', 'FIELDNAM': 'Plasma\n                  temperature', 'UNITS': 'K'}, 'V': {'COLNAME':\n                  'plasma_speed', 'FIELDNAM': 'Flow speed', 'UNITS':\n                  'km/s'}, 'THETA-V': {'COLNAME': 'sw_vel_theta',\n                  'FIELDNAM': 'Flow latitude', 'UNITS': 'Deg'}, 'PHI-V':\n                  {'COLNAME': 'sw_vel_phi', 'FIELDNAM': 'Flow longitude',\n                  'UNITS': 'Deg'}, 'BX_GSE': {'COLNAME':\n                  'B_background_x'}, 'BY_GSE': {'COLNAME':\n                  'B_background_y'}, 'BZ_GSE': {'COLNAME':\n                  'B_background_z'}})\n\nPreprocess the raw dataset (only minor transformations)\n\nApplying naming conventions for columns\nExtracting variables from CDF files, and convert them to DataFrame",
    "crumbs": [
      "Missions",
      "OMNI data"
    ]
  },
  {
    "objectID": "missions/omni/index.html#processing-data",
    "href": "missions/omni/index.html#processing-data",
    "title": "OMNI data",
    "section": "Processing data",
    "text": "Processing data\n\nsource\n\nprocess_data\n\n process_data (raw_data:polars.lazyframe.frame.LazyFrame, ts=None)\n\n\nTransforming data to GSE coordinate system\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nLazyFrame\n\n\n\n\nts\nNoneType\nNone\ntime resolution\n\n\nReturns\nLazyFrame\n\n\n\n\n\n\nsource\n\n\nflow2gse\n\n flow2gse (df:polars.lazyframe.frame.LazyFrame)\n\n\nTransforming solar wind data from Quasi-GSE coordinate to GSE coordinate system",
    "crumbs": [
      "Missions",
      "OMNI data"
    ]
  },
  {
    "objectID": "missions/omni/index.html#pipelines",
    "href": "missions/omni/index.html#pipelines",
    "title": "OMNI data",
    "section": "Pipelines",
    "text": "Pipelines\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='OMNI', source='LowRes')",
    "crumbs": [
      "Missions",
      "OMNI data"
    ]
  },
  {
    "objectID": "pipelines/100_project.html",
    "href": "pipelines/100_project.html",
    "title": "Project Pipeline",
    "section": "",
    "text": "Generally, it includes the following steps:\nL1 level solar wind dataset\nCode\nfrom ids_finder.utils.basic import load_catalog\n\ncatalog = load_catalog()\nsource",
    "crumbs": [
      "Pipelines",
      "Project Pipeline"
    ]
  },
  {
    "objectID": "pipelines/100_project.html#level-2-datasets",
    "href": "pipelines/100_project.html#level-2-datasets",
    "title": "Project Pipeline",
    "section": "Level 2 datasets",
    "text": "Level 2 datasets\nTime averaged solar wind dataset\n\nsource\n\nprocess_events_l2\n\n process_events_l2 (raw_df:polars.dataframe.frame.DataFrame,\n                    avg_window=datetime.timedelta(days=30),\n                    avg_sats=['STA', 'THB', 'Wind'])\n\nL2 level datasets - Time average - Link time and radial distance\n\nsource\n\n\ntime_average\n\n time_average (raw_df:polars.dataframe.frame.DataFrame,\n               avg_window=datetime.timedelta(days=30))\n\nTime average\n\n\nCode\ndef combine_candidates(dict):\n    pass\n\n# node_thm_extract_features = node(\n#     extract_features,\n#     inputs=[\"primary_thm_rtn_1s\", \"params:tau\", \"params:thm_1s_params\"],\n#     outputs=\"candidates_thm_rtn_1s\",\n#     name=\"extract_ARTEMIS_features\",\n# )\n\n# node_combine_candidates = node(\n#     combine_candidates,\n#     inputs=dict(\n#         sta_candidates=\"candidates_sta_rtn_1s\",\n#         jno_candidates=\"candidates_jno_ss_se_1s\",\n#         thm_candidates=\"candidates_thm_rtn_1s\",\n#     ),\n#     outputs=\"candidates_all_1s\",\n#     name=\"combine_candidates\",\n# )",
    "crumbs": [
      "Pipelines",
      "Project Pipeline"
    ]
  },
  {
    "objectID": "pipelines/index.html",
    "href": "pipelines/index.html",
    "title": "Pipelines",
    "section": "",
    "text": "We have three layers of pipeline1:\nNote:",
    "crumbs": [
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/index.html#footnotes",
    "href": "pipelines/index.html#footnotes",
    "title": "Pipelines",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA pipeline is a collection of nodes that are connected to each other. Each node is a function that takes inputs and produces outputs. The inputs and outputs are data sets of different layer/level.↩︎",
    "crumbs": [
      "Pipelines"
    ]
  },
  {
    "objectID": "pipelines/3_data_state.html",
    "href": "pipelines/3_data_state.html",
    "title": "State data pipeline",
    "section": "",
    "text": "Here we define state data to be the complementary data of the magnetic field data, useful for the analysis of the magnetic field data. It may include:\n\nPlasma data\nSatellite location data",
    "crumbs": [
      "Pipelines",
      "State data pipeline"
    ]
  },
  {
    "objectID": "utils/30_cdf.html",
    "href": "utils/30_cdf.html",
    "title": "Utils for CDF file format",
    "section": "",
    "text": "source\n\ncdf2pl\n\n cdf2pl (file_path:str, var_names:str|list[str])\n\nConvert a CDF file to Polars Dataframe.\nParameters: file_path (str): The path to the CDF file. var_names (Union[str, List[str]]): The name(s) of the variable(s) to retrieve from the CDF file.\nReturns: pl.LazyFrame: A lazy dataframe containing the requested data.",
    "crumbs": [
      "Utils",
      "Utils for CDF file format"
    ]
  },
  {
    "objectID": "utils/31_lbl.html",
    "href": "utils/31_lbl.html",
    "title": "Utils for lbl file format",
    "section": "",
    "text": "Read lbl file\n\nsource\n\n\nLblDataset\n\n LblDataset (filepath:str, load_type:str='table',\n             metadata:Dict[str,Any]=None)\n\nAbstractDataset is the base class for all data set implementations. All data set implementations should extend this abstract class and implement the methods marked as abstract. If a specific dataset implementation cannot be used in conjunction with the ParallelRunner, such user-defined dataset should have the attribute _SINGLE_PROCESS = True. Example: ::\n&gt;&gt;&gt; from pathlib import Path, PurePosixPath\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from kedro.io import AbstractDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; class MyOwnDataset(AbstractDataset[pd.DataFrame, pd.DataFrame]):\n&gt;&gt;&gt;     def __init__(self, filepath, param1, param2=True):\n&gt;&gt;&gt;         self._filepath = PurePosixPath(filepath)\n&gt;&gt;&gt;         self._param1 = param1\n&gt;&gt;&gt;         self._param2 = param2\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _load(self) -&gt; pd.DataFrame:\n&gt;&gt;&gt;         return pd.read_csv(self._filepath)\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _save(self, df: pd.DataFrame) -&gt; None:\n&gt;&gt;&gt;         df.to_csv(str(self._filepath))\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _exists(self) -&gt; bool:\n&gt;&gt;&gt;         return Path(self._filepath.as_posix()).exists()\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _describe(self):\n&gt;&gt;&gt;         return dict(param1=self._param1, param2=self._param2)\nExample catalog.yml specification: ::\nmy_dataset:\n    type: &lt;path-to-my-own-dataset&gt;.MyOwnDataset\n    filepath: data/01_raw/my_data.csv\n    param1: &lt;param1-value&gt; # param1 is a required argument\n    # param2 will be True by default\n\nsource\n\n\nload_lbl\n\n load_lbl (filepath:str, type:str='table')\n\nLoad LBL data.\nArgs: filepath: File path to load the data from. type: Type of data to load. Options are ‘table’ and ‘index’.\nReturns: A pandas DataFrame containing the loaded data.",
    "crumbs": [
      "Utils",
      "Utils for lbl file format"
    ]
  },
  {
    "objectID": "utils/10_polars.html",
    "href": "utils/10_polars.html",
    "title": "Utils for Polars",
    "section": "",
    "text": "source",
    "crumbs": [
      "Utils",
      "Utils for Polars"
    ]
  },
  {
    "objectID": "utils/10_polars.html#io",
    "href": "utils/10_polars.html#io",
    "title": "Utils for Polars",
    "section": "IO",
    "text": "IO\n\nsource\n\nconvert_to_pd_dataframe\n\n convert_to_pd_dataframe\n                          (df:polars.dataframe.frame.DataFrame|polars.lazy\n                          frame.frame.LazyFrame, modin:bool=False)\n\nConvert a Polars DataFrame or LazyFrame into a pandas-like DataFrame. If modin=True, returns a Modin DataFrame.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npolars.dataframe.frame.DataFrame | polars.lazyframe.frame.LazyFrame\n\noriginal DataFrame or LazyFrame\n\n\nmodin\nbool\nFalse\nwhether to use modin or not",
    "crumbs": [
      "Utils",
      "Utils for Polars"
    ]
  },
  {
    "objectID": "utils/10_polars.html#functions",
    "href": "utils/10_polars.html#functions",
    "title": "Utils for Polars",
    "section": "Functions",
    "text": "Functions\n\nsource\n\nsort\n\n sort (df:polars.dataframe.frame.DataFrame, col='time')\n\n\nsource\n\n\npl_norm\n\n pl_norm (columns, *more_columns)\n\nComputes the square root of the sum of squares for the given columns.\nArgs: *columns (str): Names of the columns.\nReturns: pl.Expr: Expression representing the square root of the sum of squares.\n\nsource\n\n\ndecompose_vector\n\n decompose_vector (df:polars.lazyframe.frame.LazyFrame, vector_col,\n                   name=None)",
    "crumbs": [
      "Utils",
      "Utils for Polars"
    ]
  },
  {
    "objectID": "utils/15_kedro.html",
    "href": "utils/15_kedro.html",
    "title": "Kedro",
    "section": "",
    "text": "Standard import\nfrom kedro.pipeline import Pipeline, node\nfrom kedro.pipeline.modular_pipeline import pipeline\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([\n        ...\n    ])\n\n\nCode\ndef load_catalog(conf_source: str, catalog_source: str = \"catalog\"):\n    # Initialise a ConfigLoader\n    conf_loader = OmegaConfigLoader(conf_source)\n\n    # Load the data catalog configuration from catalog.yml\n    conf_catalog = conf_loader.get(catalog_source)\n\n    # Create the DataCatalog instance from the configuration\n    catalog = DataCatalog.from_config(conf_catalog)\n    \n    return catalog\n\n\nThe following load_catalog provides project-aware access to the catalog. The preceding load_catalog only works when notebook is run from the project root.\n\nsource\n\nload_context\n\n load_context (project_path:str, params_only:bool=False,\n               catalog_only:bool=False)",
    "crumbs": [
      "Utils",
      "`Kedro`"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Discontinuities? Yes!",
    "section": "",
    "text": "For how to use this project as a python library, please see this page."
  },
  {
    "objectID": "index.html#todos",
    "href": "index.html#todos",
    "title": "Discontinuities? Yes!",
    "section": "TODOs",
    "text": "TODOs\nScience part\n\nAnalysis\n\nCheck STEREO-A and ARTEMIS-B data\nContribution of discontinuities to the power spectrum\nCheck Datagap\nCheck ARTEMIS-B data in different states (solar wind, magnetosheath, magnetotail, moon wake)\nDistribution of |B| over radius\nJUNO from 2012-09~2012-10 lack of IDS and extreme large thickness\nWind data\nAdd error bar\nValidate the effects of calibrate candidate duration\nValidate model density with Voyager\n\nIdentifaction\n\nEnsemble forest?\nSmoothing is important?\nCheck change point algorithm\n\nVisualize data gaps\nFeatures\n\nThickness in N direction\nUse high resolution data for feature extraction\n\nCompare with other methods of identifying IDs\n\nVerify with other methods of identifying IDs\n\nIncorporate solar wind propagation model\n\nVerify with solar wind propagation model\n\nCoordinate transformation\n\n\n\nCode part\n\nOptimization\n\nJAX library for numpy optimization\nshorten import time\n\nRefactor\n\nprocess_candidates to exclude sat_state logics\nrenaming feature layer candidates\n\nKedro\n\nModular pipelines\nIncorporate lineapy\n\nQR codes\n\n\nbugs\n\nJUNO sw_temperature type\nSTEREO B less than zero (after downsampling?)"
  },
  {
    "objectID": "02_ids_properties.html",
    "href": "02_ids_properties.html",
    "title": "ID properties",
    "section": "",
    "text": "source",
    "crumbs": [
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#duration",
    "href": "02_ids_properties.html#duration",
    "title": "ID properties",
    "section": "Duration",
    "text": "Duration\nDefinitions of duration - Define $d^* = ( | dB / dt | ) $, and then define time interval where \\(| dB/dt |\\) decreases to \\(d^*/4\\)\n\nsource\n\nget_time_from_condition\n\n get_time_from_condition (vec:xarray.core.dataarray.DataArray, threshold,\n                          condition_type)\n\n\nsource\n\n\nfind_start_end_times\n\n find_start_end_times (vec_diff_mag:xarray.core.dataarray.DataArray,\n                       d_time, threshold)\n\n\nsource\n\n\ncalc_d_duration\n\n calc_d_duration (vec:xarray.core.dataarray.DataArray, d_time, threshold)\n\n\nsource\n\n\ncalc_duration\n\n calc_duration (vec:xarray.core.dataarray.DataArray, threshold_ratio=0.25)\n\n\nsource\n\n\ncalc_candidate_duration\n\n calc_candidate_duration (candidate:modin.pandas.series.Series, data)",
    "crumbs": [
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#minimum-variance-analysis-mva-features",
    "href": "02_ids_properties.html#minimum-variance-analysis-mva-features",
    "title": "ID properties",
    "section": "Minimum variance analysis (MVA) features",
    "text": "Minimum variance analysis (MVA) features\nTo ensure the accuracy of MVA, only when the ratio of the middle to the minimum eigenvalue (labeled QMVA for simplicity) is larger than 3 are the results used for further analysis.\n\n\nCode\ndef minvar(data):\n    \"\"\"\n    see `pyspedas.cotrans.minvar`\n    This program computes the principal variance directions and variances of a\n    vector quantity as well as the associated eigenvalues.\n\n    Parameters\n    -----------\n    data:\n        Vxyz, an (npoints, ndim) array of data(ie Nx3)\n\n    Returns\n    -------\n    vrot:\n        an array of (npoints, ndim) containing the rotated data in the new coordinate system, ijk.\n        Vi(maximum direction)=vrot[0,:]\n        Vj(intermediate direction)=vrot[1,:]\n        Vk(minimum variance direction)=Vrot[2,:]\n    v:\n        an (ndim,ndim) array containing the principal axes vectors\n        Maximum variance direction eigenvector, Vi=v[*,0]\n        Intermediate variance direction, Vj=v[*,1] (descending order)\n    w:\n        the eigenvalues of the computation\n    \"\"\"\n\n    #  Min var starts here\n    # data must be Nx3\n    vecavg = np.nanmean(np.nan_to_num(data, nan=0.0), axis=0)\n\n    mvamat = np.zeros((3, 3))\n    for i in range(3):\n        for j in range(3):\n            mvamat[i, j] = np.nanmean(np.nan_to_num(data[:, i] * data[:, j], nan=0.0)) - vecavg[i] * vecavg[j]\n\n    # Calculate eigenvalues and eigenvectors\n    w, v = np.linalg.eigh(mvamat, UPLO='U')\n\n    # Sorting to ensure descending order\n    w = np.abs(w)\n    idx = np.flip(np.argsort(w))\n\n    # IDL compatability\n    if True:\n        if np.sum(w) == 0.0:\n            idx = [0, 2, 1]\n\n    w = w[idx]\n    v = v[:, idx]\n\n    # Rotate intermediate var direction if system is not Right Handed\n    YcrossZdotX = v[0, 0] * (v[1, 1] * v[2, 2] - v[2, 1] * v[1, 2])\n    if YcrossZdotX &lt; 0:\n        v[:, 1] = -v[:, 1]\n        # v[:, 2] = -v[:, 2] # Should not it is being flipped at Z-axis?\n\n    # Ensure minvar direction is along +Z (for FAC system)\n    if v[2, 2] &lt; 0:\n        v[:, 2] = -v[:, 2]\n        v[:, 1] = -v[:, 1]\n\n    vrot = np.array([np.dot(row, v) for row in data])\n\n    return vrot, v, w\n\n\n                WARNING  UserWarning: potentially wrong underline length...                                                                                                     warnings.py:109\n                         Parameters                                                                                                                                                            \n                         ----------- in                                                                                                                                                        \n                         see `pyspedas.cotrans.minvar`                                                                                                                                         \n                         This program computes the principal variance directions and variances of a...                                                                                         \n                                                                                                                                                                                               \n\nsource\n\nminvar\n\n minvar (data)\n\nsee pyspedas.cotrans.minvar This program computes the principal variance directions and variances of a vector quantity as well as the associated eigenvalues.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\n\n\n\n\nReturns\nvrot:\nan array of (npoints, ndim) containing the rotated data in the new coordinate system, ijk.Vi(maximum direction)=vrot[0,:]Vj(intermediate direction)=vrot[1,:]Vk(minimum variance direction)=Vrot[2,:]\n\n\n\n\nsource\n\n\nmva_features\n\n mva_features (data:numpy.ndarray)\n\nCompute MVA features based on the given data array.\nParameters: - data (np.ndarray): Input data\nReturns: - List: Computed features\n\nsource\n\n\ncalc_candidate_mva_features\n\n calc_candidate_mva_features (candidate,\n                              data:xarray.core.dataarray.DataArray)\n\n\nTest\n\n\nCode\nfrom fastcore.test import *\n\n# Generate synthetic data\nnp.random.seed(42)  # for reproducibility\ndata = np.random.rand(100, 3)  # 100 time points, 3-dimensional data\n# Call the mva_features function\nfeatures = mva_features(data)\n_features = [0.3631060892452051, 0.8978455426527485, -0.24905290500542857, 0.09753158579102299, 0.086943767300213, 0.07393142040422575, 1.1760056390752571, 0.9609421690770317, 0.6152039820297959, -0.5922397773398479, 0.6402091632847049, 0.61631157045453, 1.2956351134759623, 0.19091785005728523, 0.5182488424049534, 0.4957624347593598]\ntest_eq(features, _features)",
    "crumbs": [
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#field-rotation-angles",
    "href": "02_ids_properties.html#field-rotation-angles",
    "title": "ID properties",
    "section": "Field rotation angles",
    "text": "Field rotation angles\nThe PDF of the field rotation angles across the solar-wind IDs is well fitted by the exponential function exp(−θ/)…\n\nsource\n\nget_data_at_times\n\n get_data_at_times (data:xarray.core.dataarray.DataArray, times)\n\nSelect data at specified times.\n\nsource\n\n\ncalc_rotation_angle\n\n calc_rotation_angle (v1, v2)\n\nComputes the rotation angle between two vectors.\nParameters: - v1: The first vector(s). - v2: The second vector(s).\n\nsource\n\n\ncalc_events_rotation_angle\n\n calc_events_rotation_angle (events, data:xarray.core.dataarray.DataArray)\n\nComputes the rotation angle(s) at two different time steps.",
    "crumbs": [
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#normal-direction",
    "href": "02_ids_properties.html#normal-direction",
    "title": "ID properties",
    "section": "Normal direction",
    "text": "Normal direction\n\nsource\n\ncalc_normal_direction\n\n calc_normal_direction (v1, v2, normalize=True)\n\nComputes the normal direction of two vectors.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nv1\narray_like\n\nThe first vector(s).\n\n\nv2\narray_like\n\nThe second vector(s).\n\n\nnormalize\nbool\nTrue\n\n\n\nReturns\nndarray\n\n\n\n\n\n\nsource\n\n\ncalc_events_normal_direction\n\n calc_events_normal_direction (events,\n                               data:xarray.core.dataarray.DataArray)\n\nComputes the normal directions(s) at two different time steps.\n\nsource\n\n\ncalc_events_vec_change\n\n calc_events_vec_change (events, data:xarray.core.dataarray.DataArray)\n\nUtils function to calculate features related to the change of the magnetic field",
    "crumbs": [
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#pipelines",
    "href": "02_ids_properties.html#pipelines",
    "title": "ID properties",
    "section": "Pipelines",
    "text": "Pipelines\npatch pdp.ApplyToRows to work with modin and xorbits DataFrames\nPipelines Class for processing IDs\n\nsource\n\nIDsPipeline\n\n IDsPipeline ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nprocess_events\n\n process_events (candidates_pl:polars.dataframe.frame.DataFrame,\n                 sat_fgm:xarray.core.dataarray.DataArray,\n                 data_resolution:datetime.timedelta, modin=True)\n\nProcess candidates DataFrame\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncandidates_pl\nDataFrame\n\npotential candidates DataFrame\n\n\nsat_fgm\nDataArray\n\nsatellite FGM data\n\n\ndata_resolution\ntimedelta\n\ntime resolution of the data\n\n\nmodin\nbool\nTrue\n\n\n\nReturns\nDataFrame",
    "crumbs": [
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#obsolete-codes",
    "href": "02_ids_properties.html#obsolete-codes",
    "title": "ID properties",
    "section": "Obsolete codes",
    "text": "Obsolete codes\nThis is obsolete codes because the timewindow now is overlapping. No need to consider where magnetic discontinuities happens in the boundary of one timewindow.\n\n\nCode\ndef calc_candidate_d_duration(candidate, data) -&gt; pd.Series:\n    try:\n        if pd.isnull(candidate['d_tstart']) or pd.isnull(candidate['d_tstop']):\n            candidate_data = get_candidate_data(candidate, data, neighbor=1)\n            d_time = candidate['d_time']\n            threshold = candidate['threshold']\n            return calc_d_duration(candidate_data, d_time, threshold)\n        else:\n            return pandas.Series({\n                'd_tstart': candidate['d_tstart'],\n                'd_tstop': candidate['d_tstop'],\n            })\n    except Exception as e:\n        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n        raise e\n\n\n\n\nCode\n# pdp.ApplyToRows(\n#     lambda candidate: calc_candidate_d_duration(candidate, sat_fgm),\n#     func_desc=\"calculating duration parameters if needed\"\n# )\n\n\n\nCalibrates candidate duration\nThis calibration is based on the assumption that the magnetic discontinuity is symmetric around the center of time, which is not always true.\nSo instead of calibrating the duration, we drop the events. - Cons: Might influence the statistics of occurrence rate, but - Pros: More robust results about the properties of the magnetic discontinuity.\n\n\nCode\ndef calibrate_candidate_duration(\n    candidate: pd.Series, data:xr.DataArray, data_resolution, ratio = 3/4\n):\n    \"\"\"\n    Calibrates the candidate duration. \n    - If only one of 'd_tstart' or 'd_tstop' is provided, calculates the missing one based on the provided one and 'd_time'.\n    - Then if this is not enough points between 'd_tstart' and 'd_tstop', returns None for both.\n    \n    \n    Parameters\n    ----------\n    - candidate (pd.Series): The input candidate with potential missing 'd_tstart' or 'd_tstop'.\n    \n    Returns\n    -------\n    - pd.Series: The calibrated candidate.\n    \"\"\"\n    \n    start_notnull = pd.notnull(candidate['d_tstart'])\n    stop_notnull = pd.notnull(candidate['d_tstop']) \n    \n    match start_notnull, stop_notnull:\n        case (True, True):\n            d_tstart = candidate['d_tstart']\n            d_tstop = candidate['d_tstop']\n        case (True, False):\n            d_tstart = candidate['d_tstart']\n            d_tstop = candidate['d_time'] -  candidate['d_tstart'] + candidate['d_time']\n        case (False, True):\n            d_tstart = candidate['d_time'] -  candidate['d_tstop'] + candidate['d_time']\n            d_tstop = candidate['d_tstop']\n        case (False, False):\n            return pandas.Series({\n                'd_tstart': None,\n                'd_tstop': None,\n            })\n    \n    duration = d_tstop - d_tstart\n    num_of_points_between = data.time.sel(time=slice(d_tstart, d_tstop)).count().item()\n    \n    if num_of_points_between &lt;= (duration/data_resolution) * ratio:\n        d_tstart = None\n        d_tstop = None\n    \n    return pandas.Series({\n        'd_tstart': d_tstart,\n        'd_tstop': d_tstop,\n    })\n\n\n\n\nCode\ndef calibrate_candidates_duration(candidates, sat_fgm, data_resolution):\n    # calibrate duration\n\n    calibrate_duration = pdp.ApplyToRows(\n        lambda candidate: calibrate_candidate_duration(\n            candidate, sat_fgm, data_resolution\n        ),\n        func_desc=\"calibrating duration parameters if needed\",\n    )\n\n    temp_candidates = candidates.loc[\n        lambda df: df[\"d_tstart\"].isnull() | df[\"d_tstop\"].isnull()\n    ]  # temp_candidates = candidates.query('d_tstart.isnull() | d_tstop.isnull()') # not implemented in `modin`\n\n    if not temp_candidates.empty:\n        temp_candidates_updated = calibrate_duration(sat_fgm, data_resolution).apply(\n            temp_candidates\n        )\n        candidates.update(temp_candidates_updated)\n    return candidates",
    "crumbs": [
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#test-1",
    "href": "02_ids_properties.html#test-1",
    "title": "ID properties",
    "section": "Test",
    "text": "Test\n\nTest parallelization\nGenerally mapply and modin are the fastest. xorbits is expected to be the fastest but it is not and it is the slowest one.\n#| notest\nsat = 'jno'\ncoord = 'se'\ncols = [\"BX\", \"BY\", \"BZ\"]\ntau = timedelta(seconds=60)\ndata_resolution = timedelta(seconds=1)\n\nif True:\n    year = 2012\n    files = f'../data/{sat}_data_{year}.parquet'\n    output = f'../data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n\n    data = pl.scan_parquet(files).set_sorted('time').collect()\n\n    indices = compute_indices(data, tau)\n    # filter condition\n    sparse_num = tau / data_resolution // 3\n    filter_condition = filter_indices(sparse_num = sparse_num)\n\n    candidates = indices.filter(filter_condition).with_columns(pl_format_time(tau)).sort('time')\n    \n    data_c = compress_data_by_cands(data, candidates, tau)\n    sat_fgm = df2ts(data_c, cols, attrs={\"units\": \"nT\"})\n\n\nCode\ncandidates_pd = candidates.to_pandas()\ncandidates_modin = mpd.DataFrame(candidates_pd)\n# candidates_x = xpd.DataFrame(candidates_pd)\n\n\n\n\nTest different libraries to parallelize the computation\nif True:\n    pdp_test = pdp.ApplyToRows(\n        lambda candidate: calc_candidate_duration(candidate, sat_fgm),  # fast a little bit\n        # lambda candidate: calc_duration(get_candidate_data_xr(candidate, sat_fgm)),\n        # lambda candidate: calc_duration(sat_fgm.sel(time=slice(candidate['tstart'], candidate['tstop']))),\n        func_desc=\"calculating duration parameters\",\n    )\n    \n    # process_events(candidates_modin, sat_fgm, sat_state, data_resolution)\n    \n    # ---\n    # successful cases\n    # ---\n    # candidates_pd.mapply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works, 4.2 secs\n    # candidates_pd.mapply(calc_candidate_duration, axis=1, data=sat_fgm) # this works, but a little bit slower, 6.7 secs\n    \n    # candidates_pd.apply(calc_candidate_duration, axis=1, data=sat_fgm) # Standard case: 24+s secs\n    # candidates_pd.swifter.apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 80 secs\n    # candidates_pd.swifter.set_dask_scheduler(scheduler=\"threads\").apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 60 secs\n    # candidates_modin.apply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works with ray, 6 secs # NOTE: can not work with dask\n    # candidates_x.apply(calc_candidate_duration, axis=1, data=sat_fgm) # 30 seconds\n    # pdp_test(candidates_modin) # this works, 8 secs\n    \n    # ---\n    # failed cases\n    # ---\n    # candidates_modin.apply(calc_candidate_duration, axis=1, data=sat_fgm) # AttributeError: 'DataFrame' object has no attribute 'sel'\n\n\n\n\nCode\nimport timeit\nfrom functools import partial\n\n\n\n\nCode\ndef benchmark(task_dict, number=1):\n    results = {}\n    for name, (data, task) in task_dict.items():\n        try:\n            time_taken = timeit.timeit(\n                lambda: task(data),\n                number=number\n            )\n            results[name] = time_taken / number\n        except Exception as e:\n            results[name] = str(e)\n    return results\n\n\n\n\nCode\ndef benchmark_results(results, sat_fgm):\n    func = partial(calc_candidate_duration, data=sat_fgm)\n    task_dict = {\n        'pandas': (candidates_pd, lambda _: _.apply(func, axis=1)),\n        'pandas-mapply': (candidates_pd, lambda _: _.mapply(func, axis=1)),\n        'modin': (candidates_modin, lambda _: _.apply(func, axis=1)),\n        # 'xorbits': (candidates_x, lambda _: _.apply(func, axis=1)),\n    }\n\n    results = benchmark(task_dict)\n    return results",
    "crumbs": [
      "ID properties"
    ]
  }
]