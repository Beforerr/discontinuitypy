[
  {
    "objectID": "20_datasets.html",
    "href": "20_datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Foundational Dataset Class\n\nsource\n\n\n\n IDsDataset (data:polars.lazyframe.frame.LazyFrame,\n             candidates:polars.dataframe.frame.DataFrame=None,\n             plasma_data:polars.lazyframe.frame.LazyFrame=None,\n             bcols:list[str]=None, vec_cols:list[str]=None,\n             **extra_data:Any)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.",
    "crumbs": [
      "Home",
      "Datasets"
    ]
  },
  {
    "objectID": "20_datasets.html#datasets",
    "href": "20_datasets.html#datasets",
    "title": "Datasets",
    "section": "",
    "text": "Foundational Dataset Class\n\nsource\n\n\n\n IDsDataset (data:polars.lazyframe.frame.LazyFrame,\n             candidates:polars.dataframe.frame.DataFrame=None,\n             plasma_data:polars.lazyframe.frame.LazyFrame=None,\n             bcols:list[str]=None, vec_cols:list[str]=None,\n             **extra_data:Any)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.",
    "crumbs": [
      "Home",
      "Datasets"
    ]
  },
  {
    "objectID": "properties/00_mva.html",
    "href": "properties/00_mva.html",
    "title": "Minimum variance analysis (MVA)",
    "section": "",
    "text": "Notes:\nThe following method implicitly assumes the data is evenly sampled, otherwise, data resampling is needed.\nCode\ndef minvar(data):\n    \"\"\"\n    see `pyspedas.cotrans.minvar`\n\n    This program computes the principal variance directions and variances of a\n    vector quantity as well as the associated eigenvalues.\n\n    Parameters\n    -----------\n    data:\n        Vxyz, an (npoints, ndim) array of data(ie Nx3)\n\n    Returns\n    -------\n    vrot:\n        an array of (npoints, ndim) containing the rotated data in the new coordinate system, ijk.\n        Vi(maximum direction)=vrot[0,:]\n        Vj(intermediate direction)=vrot[1,:]\n        Vk(minimum variance direction)=Vrot[2,:]\n    v:\n        an (ndim,ndim) array containing the principal axes vectors\n        Maximum variance direction eigenvector, Vi=v[*,0]\n        Intermediate variance direction, Vj=v[*,1] (descending order)\n    w:\n        the eigenvalues of the computation\n    \"\"\"\n\n    #  Min var starts here\n    # data must be Nx3\n    vecavg = np.nanmean(np.nan_to_num(data, nan=0.0), axis=0)\n\n    mvamat = np.zeros((3, 3))\n    for i in range(3):\n        for j in range(3):\n            mvamat[i, j] = (\n                np.nanmean(np.nan_to_num(data[:, i] * data[:, j], nan=0.0))\n                - vecavg[i] * vecavg[j]\n            )\n\n    # Calculate eigenvalues and eigenvectors\n    w, v = np.linalg.eigh(mvamat, UPLO=\"U\")\n\n    # Sorting to ensure descending order\n    w = np.abs(w)\n    idx = np.flip(np.argsort(w))\n\n    # IDL compatability\n    if True:\n        if np.sum(w) == 0.0:\n            idx = [0, 2, 1]\n\n    w = w[idx]\n    v = v[:, idx]\n\n    # Rotate intermediate var direction if system is not Right Handed\n    YcrossZdotX = v[0, 0] * (v[1, 1] * v[2, 2] - v[2, 1] * v[1, 2])\n    if YcrossZdotX &lt; 0:\n        v[:, 1] = -v[:, 1]\n        # v[:, 2] = -v[:, 2] # Should not it is being flipped at Z-axis?\n\n    # Ensure minvar direction is along +Z (for FAC system)\n    if v[2, 2] &lt; 0:\n        v[:, 2] = -v[:, 2]\n        v[:, 1] = -v[:, 1]\n\n    vrot = np.array([np.dot(row, v) for row in data])\n\n    return vrot, v, w\nsource",
    "crumbs": [
      "Home",
      "Properties",
      "Minimum variance analysis (MVA)"
    ]
  },
  {
    "objectID": "properties/00_mva.html#mva-related-features",
    "href": "properties/00_mva.html#mva-related-features",
    "title": "Minimum variance analysis (MVA)",
    "section": "MVA related features",
    "text": "MVA related features\n\nsource\n\ncalc_mva_features\n\n calc_mva_features (data:numpy.ndarray)\n\nCompute MVA features based on the given data array.\nParameters: - data (np.ndarray): Input data\nReturns: - List: Computed features",
    "crumbs": [
      "Home",
      "Properties",
      "Minimum variance analysis (MVA)"
    ]
  },
  {
    "objectID": "properties/00_mva.html#fit-maximum-variance-direction",
    "href": "properties/00_mva.html#fit-maximum-variance-direction",
    "title": "Minimum variance analysis (MVA)",
    "section": "Fit maximum variance direction",
    "text": "Fit maximum variance direction\n\\[\nf(x; A, \\mu, \\sigma, {\\mathrm{form={}'logistic{}'}}) = A \\left[1 - \\frac{1}{1 + e^{\\alpha}} \\right]\n\\]\nwhere \\(\\alpha = (x - \\mu)/{\\sigma}\\). And the derivative is\n\\[\n\\frac{df}{dx} = \\frac{A}{\\sigma} \\frac{e^{\\alpha}}{(1 + e^{\\alpha})^2}\n\\]\nat center \\(x = \\mu\\), the derivative is\n\\[\n\\frac{df}{dx} = \\frac{A}{4 \\sigma}\n\\]\n\nsource\n\nfit_maxiumum_variance_direction\n\n fit_maxiumum_variance_direction (ts:xarray.core.dataarray.DataArray,\n                                  datetime_unit='s',\n                                  return_best_fit:bool=True, **kwargs)\n\nFit maximum variance direction data by model\nNote: - see datetime_to_numeric in xarray.core.duck_array_ops for more details about converting datetime to numeric - Xarray uses the numpy dtypes datetime64[ns] and timedelta64[ns] to represent datetime data.\n\nsource\n\n\ncalc_candidate_mva_features\n\n calc_candidate_mva_features (event, data:xarray.core.dataarray.DataArray,\n                              **kwargs)",
    "crumbs": [
      "Home",
      "Properties",
      "Minimum variance analysis (MVA)"
    ]
  },
  {
    "objectID": "properties/00_mva.html#test",
    "href": "properties/00_mva.html#test",
    "title": "Minimum variance analysis (MVA)",
    "section": "Test",
    "text": "Test\n\n\ntest for ts_max_distance function\ntime = pd.date_range(\"2000-01-01\", periods=10)\nx = np.linspace(0, np.pi, 10)\n# generate data circular in three dimensions, so the biggest distance is between the first and the last point\ndata = np.zeros((10, 3))\ndata[:, 0] = np.cos(x)\ndata[:, 1] = np.sin(x)\nts = xr.DataArray(data, coords={\"time\": time}, dims=[\"time\", \"space\"])\nfit_maxiumum_variance_direction(ts[:, 0]).best_fit\n\n\narray([ 1.01476962,  0.92001526,  0.75955714,  0.5124786 ,  0.18267077,\n       -0.18267075, -0.51247858, -0.75955714, -0.92001527, -1.01476964])\n\n\n\n\nCode\nmva_features, vrot = calc_mva_features(data)\nvrot.shape\n\n\n(10, 3)\n\n\n\n\nCode\nfrom fastcore.test import test_eq\n\n# Generate synthetic data\nnp.random.seed(42)  # for reproducibility\ndata = np.random.rand(100, 3)  # 100 time points, 3-dimensional data\n# Call the mva_features function\nfeatures, vrot = calc_mva_features(data)\n_features = [\n    0.3631060892452051,\n    0.8978455426527485,\n    -0.24905290500542857,\n    0.09753158579102299,\n    0.086943767300213,\n    0.07393142040422575,\n    1.1760056390752571,\n    0.9609421690770317,\n    0.6152039820297959,\n    -0.5922397773398479,\n    0.6402091632847049,\n    0.61631157045453,\n    1.2956351134759623,\n    0.19091785005728523,\n    0.5182488424049534,\n    0.4957624347593598,\n]\ntest_eq(features, _features)",
    "crumbs": [
      "Home",
      "Properties",
      "Minimum variance analysis (MVA)"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This python package is still in beta phrase, and we are actively working on it. If you have any questions or suggestions, please feel free to contact us.",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DiscontinuityPy",
    "section": "",
    "text": "This package is designed to identify and analyze discontinuities in time series data.\n\nFinding the discontinuities, see this notebook\n\nCorresponding to limited feature extraction / anomaly detection\n\nCalculating the properties of the discontinuities, see this notebook\n\nOne can use higher time resolution data\n\n\nFor how to use this project as a python library, please see this page.\n\n\npip install discontinuitypy\n\n\n\nImport the package\nfrom discontinuitypy.utils.basic import *\nfrom discontinuitypy.core import *"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "DiscontinuityPy",
    "section": "",
    "text": "pip install discontinuitypy"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "DiscontinuityPy",
    "section": "",
    "text": "Import the package\nfrom discontinuitypy.utils.basic import *\nfrom discontinuitypy.core import *"
  },
  {
    "objectID": "utils/01_plotting.html",
    "href": "utils/01_plotting.html",
    "title": "discontinuitypy",
    "section": "",
    "text": "source\n\nsavefig\n\n savefig (name, **kwargs)\n\n\n\nMVA plotting\n\nsource\n\n\nsetup_mva_plot\n\n setup_mva_plot (data:xarray.core.dataarray.DataArray,\n                 tstart:datetime.datetime, tstop:datetime.datetime,\n                 mva_tstart:datetime.datetime=None,\n                 mva_tstop:datetime.datetime=None)\n\n\nsource\n\n\ntime_stamp\n\n time_stamp (ts)\n\nReturn POSIX timestamp as float.\n\n\nCode\ndef format_candidate_title(candidate: dict):\n    def format_float(x):\n        return rf\"$\\bf {x:.2f} $\" if isinstance(x, (float, int)) else rf\"$\\bf {x} $\"\n\n    base_line = rf'$\\bf {candidate.get(\"type\", \"N/A\")} $ candidate (time: {candidate.get(\"time\", \"N/A\")}) with index '\n    index_line = rf'i1: {format_float(candidate.get(\"index_std\", \"N/A\"))}, i2: {format_float(candidate.get(\"index_fluctuation\", \"N/A\"))}, i3: {format_float(candidate.get(\"index_diff\", \"N/A\"))}'\n    info_line = rf'$B_n/B$: {format_float(candidate.get(\"BnOverB\", \"N/A\"))}, $dB/B$: {format_float(candidate.get(\"dBOverB\", \"N/A\"))}, $(dB/B)_{{max}}$: {format_float(candidate.get(\"dBOverB_max\", \"N/A\"))},  $Q_{{mva}}$: {format_float(candidate.get(\"Q_mva\", \"N/A\"))}'\n    title = rf\"\"\"{base_line}\n    {index_line}\n    {info_line}\"\"\"\n    return title\n\n\n\nsource\n\n\nplot_candidate\n\n plot_candidate (candidate:dict, data:xarray.core.dataarray.DataArray,\n                 add_timebars=True, add_plasma_params=False,\n                 plot_fit_data=False, **kwargs)",
    "crumbs": [
      "Home",
      "Utils",
      "MVA plotting"
    ]
  },
  {
    "objectID": "utils/basic.html",
    "href": "utils/basic.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n pmap (func, *args, **kwargs)\n\nmap with partial",
    "crumbs": [
      "Home",
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "utils/basic.html#utilities-functions",
    "href": "utils/basic.html#utilities-functions",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n pmap (func, *args, **kwargs)\n\nmap with partial",
    "crumbs": [
      "Home",
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "utils/basic.html#utils",
    "href": "utils/basic.html#utils",
    "title": "Utils",
    "section": "Utils",
    "text": "Utils",
    "crumbs": [
      "Home",
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "utils/basic.html#configurations",
    "href": "utils/basic.html#configurations",
    "title": "Utils",
    "section": "Configurations",
    "text": "Configurations\n\nsource\n\nDataConfig\n\n DataConfig (sat_id:str=None, start:datetime.datetime=None,\n             end:datetime.datetime=None, ts:datetime.timedelta=None,\n             coord:str=None)\n\nUsage docs: https://docs.pydantic.dev/2.6/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.",
    "crumbs": [
      "Home",
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "utils/basic.html#polars",
    "href": "utils/basic.html#polars",
    "title": "Utils",
    "section": "Polars",
    "text": "Polars\n\nsource\n\nfilter_tranges_df\n\n filter_tranges_df (df:polars.dataframe.frame.DataFrame,\n                    tranges:Tuple[list,list], time_col:str='time')\n\n\nFilter data by time ranges\n\n\nsource\n\n\nfilter_tranges\n\n filter_tranges (time:polars.series.series.Series,\n                 tranges:Tuple[list,list])\n\n\nFilter data by time ranges, return the indices of the time that are in the time ranges (left inclusive, right exclusive)\n\n\n\n\nDataFrame.plot\n\n DataFrame.plot (*args, **kwargs)\n\n\nsource\n\n\npl_norm\n\n pl_norm (columns, *more_columns)\n\nComputes the square root of the sum of squares for the given columns.\nArgs: *columns (str): Names of the columns.\nReturns: pl.Expr: Expression representing the square root of the sum of squares.\n\nPartition the dataset by time\n\nsource\n\n\n\npartition_data_by_time\n\n partition_data_by_time\n                         (df:polars.lazyframe.frame.LazyFrame|polars.dataf\n                         rame.frame.DataFrame, method)\n\nPartition the dataset by time\nArgs: df: Input DataFrame. method: The method to partition the data.\nReturns: Partitioned DataFrame.\n\nsource\n\n\npartition_data_by_year_month\n\n partition_data_by_year_month (df:polars.dataframe.frame.DataFrame)\n\nPartition the dataset by year\nArgs: df: Input DataFrame.\nReturns: Partitioned DataFrame.\n\nsource\n\n\npartition_data_by_year\n\n partition_data_by_year (df:polars.dataframe.frame.DataFrame)\n\nPartition the dataset by year\nArgs: df: Input DataFrame.\nReturns: Partitioned DataFrame.\n\nsource\n\n\npartition_data_by_ts\n\n partition_data_by_ts (df:polars.dataframe.frame.DataFrame,\n                       ts:datetime.timedelta)\n\nPartition the dataset by time\nArgs: df: Input DataFrame. ts: Time interval.\nReturns: Partitioned DataFrame.\n\nsource\n\n\nconcat_partitions\n\n concat_partitions (partitioned_input:Dict[str,Callable])\n\nConcatenate input partitions into one DataFrame.\nArgs: partitioned_input: A dictionary with partition ids as keys and load functions as values.\n\nsource\n\n\nconcat_df\n\n concat_df (dfs:list[typing.Union[polars.dataframe.frame.DataFrame,polars.\n            lazyframe.frame.LazyFrame,pandas.core.frame.DataFrame]])\n\nConcatenate a list of DataFrames into one DataFrame.\n\nResample data\n\nsource\n\n\n\nformat_timedelta\n\n format_timedelta (time)\n\nFormat timedelta to timedelta\n\nsource\n\n\nresample\n\n resample\n           (df:polars.lazyframe.frame.LazyFrame|polars.dataframe.frame.Dat\n           aFrame, every:datetime.timedelta,\n           period:datetime.timedelta=None, offset:datetime.timedelta=None,\n           shift:datetime.timedelta=None, time_column='time')\n\nResample the DataFrame\n\nsource\n\n\nresample\n\n resample\n           (df:polars.lazyframe.frame.LazyFrame|polars.dataframe.frame.Dat\n           aFrame, every:datetime.timedelta,\n           period:datetime.timedelta=None, offset:datetime.timedelta=None,\n           shift:datetime.timedelta=None, time_column='time')\n\nResample the DataFrame\n\nsource\n\n\nresample\n\n resample\n           (df:polars.lazyframe.frame.LazyFrame|polars.dataframe.frame.Dat\n           aFrame, every:datetime.timedelta,\n           period:datetime.timedelta=None, offset:datetime.timedelta=None,\n           shift:datetime.timedelta=None, time_column='time')\n\nResample the DataFrame\n\nsource\n\n\ncalc_vec_mag\n\n calc_vec_mag (vec)\n\n\nsource\n\n\ndf2ts\n\n df2ts (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFr\n        ame,polars.lazyframe.frame.LazyFrame], cols=None, time_col='time',\n        attrs=None, name=None)\n\nConvert DataFrame to TimeSeries\n\nsource\n\n\ncheck_fgm\n\n check_fgm (vec:xarray.core.dataarray.DataArray)",
    "crumbs": [
      "Home",
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "03_mag_plasma.html",
    "href": "03_mag_plasma.html",
    "title": "Combine magnetic field data and plasma data",
    "section": "",
    "text": "combine features from different sources/instruments (magnetic field, state data, etc.)\ngenerate new features\nsource",
    "crumbs": [
      "Home",
      "Combine magnetic field data and plasma data"
    ]
  },
  {
    "objectID": "03_mag_plasma.html#additional-features-after-combining",
    "href": "03_mag_plasma.html#additional-features-after-combining",
    "title": "Combine magnetic field data and plasma data",
    "section": "Additional features after combining",
    "text": "Additional features after combining\nWith combined dataset, we calculate additional features for each candidate.\nLength\nthe length along the n direction of LMN coordinate system.\n\\[L_{n} = v_{n}  T_{duration}\\]\nHowever this may not be accurate due to the MVA method.\n\\[L_{mn} = v_{mn}  T_{duration}\\]\nIf we have the normal vector of the current sheet, we can calculate the length along the normal direction.\n\\[L_{normal} = L_{k} = v_{normal}  T_{duration}\\]\nAdditionally, we can calculate the length projected into RTN coordinate system.\n\\[L_{R} = L_{k} \\cos \\theta\\]\n\\[ j_0 = (\\frac{d B}{d t})_{max} \\frac{1}{v_{mn}}\\]\n\nsource\n\nvector_project_pl\n\n vector_project_pl (df:polars.dataframe.frame.DataFrame, v1_cols, v2_cols,\n                    name=None)\n\n\nsource\n\n\nvector_project\n\n vector_project (v1, v2, dim='v_dim')\n\n\nsource\n\n\ncalc_rotation_angle_pl\n\n calc_rotation_angle_pl (df:polars.dataframe.frame.DataFrame, v1_cols,\n                         v2_cols, name)\n\n\n\nInertial length\n\nsource\n\n\ncompute_inertial_length\n\n compute_inertial_length (df:polars.dataframe.frame.DataFrame,\n                          density_col='plasma_density')\n\n\n\nAlfven current\n\nsource\n\n\ncompute_Alfven_current\n\n compute_Alfven_current (df:polars.dataframe.frame.DataFrame,\n                         density_col='plasma_density')",
    "crumbs": [
      "Home",
      "Combine magnetic field data and plasma data"
    ]
  },
  {
    "objectID": "03_mag_plasma.html#all-features",
    "href": "03_mag_plasma.html#all-features",
    "title": "Combine magnetic field data and plasma data",
    "section": "All features",
    "text": "All features\n\nsource\n\ncalc_plasma_parameter_change\n\n calc_plasma_parameter_change (df:polars.dataframe.frame.DataFrame)\n\n\nsource\n\n\ncalc_combined_features\n\n calc_combined_features (df:polars.dataframe.frame.DataFrame,\n                         vec_cols:list[str]=['v_x', 'v_y', 'v_z'],\n                         normal_cols:list[str]=['k_x', 'k_y', 'k_z'],\n                         b_cols:list[str]=None,\n                         density_col='plasma_density', detail:bool=True)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\n\n\n\nvec_cols\nlist\n[‘v_x’, ‘v_y’, ‘v_z’]\nplasma velocity vector in any fixed coordinate system,\n\n\nnormal_cols\nlist\n[‘k_x’, ‘k_y’, ‘k_z’]\nnormal vector of the discontinuity plane\n\n\nb_cols\nlist\nNone\n[“B_background_x”, “B_background_y”, “B_background_z”]\n\n\ndensity_col\nstr\nplasma_density\n\n\n\ndetail\nbool\nTrue",
    "crumbs": [
      "Home",
      "Combine magnetic field data and plasma data"
    ]
  },
  {
    "objectID": "01_ids_detection.html",
    "href": "01_ids_detection.html",
    "title": "ID identification",
    "section": "",
    "text": "There are couple of ways to identify the ID.",
    "crumbs": [
      "Home",
      "ID identification"
    ]
  },
  {
    "objectID": "01_ids_detection.html#lius-variance-method",
    "href": "01_ids_detection.html#lius-variance-method",
    "title": "ID identification",
    "section": "Liu’s variance method",
    "text": "Liu’s variance method\nFor each sampling instant \\(t\\), we define three intervals: the pre-interval \\([-1,-1/2]\\cdot T+t\\), the middle interval \\([-1/,1/2]\\cdot T+t\\), and the post-interval \\([1/2,1]\\cdot T+t\\), in which \\(T\\) are time lags. Let time series of the magnetic field data in these three intervals are labeled \\({\\mathbf B}_-\\), \\({\\mathbf B}_0\\), \\({\\mathbf B}_+\\), respectively. Compute the following indices:\n\\[\nI_1 = \\frac{\\sigma(B_0)}{Max(\\sigma(B_-),\\sigma(B_+))}\n\\] \\[\nI_2 = \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)}\n\\] \\[\nI_3 = \\frac{| \\Delta \\vec{B} |}{|B_{bg}|}\n\\]\nBy selecting a large and reasonable threshold for the ﬁrst two indices (\\(I_1&gt;2, I_2&gt;1\\)) , we could guarantee that the ﬁeld changes of the IDs identiﬁed are large enough to be distinguished from the stochastic ﬂuctuations on magnetic ﬁelds, while the third is a supplementary condition to reduce the uncertainty of recognition. While the third index (relative field jump) is a supplementary condition to reduce the uncertainty of recognition.\n\nsource\n\npl_format_time\n\n pl_format_time\n                 (df:polars.lazyframe.frame.LazyFrame|polars.dataframe.fra\n                 me.DataFrame, tau:datetime.timedelta)\n\n\nsource\n\n\ncompute_combinded_std\n\n compute_combinded_std\n                        (df:polars.dataframe.frame.DataFrame|polars.lazyfr\n                        ame.frame.LazyFrame, tau, cols)\n\n\nsource\n\n\ncompute_std\n\n compute_std\n              (df:polars.dataframe.frame.DataFrame|polars.lazyframe.frame.\n              LazyFrame, period:datetime.timedelta, b_cols=['BX', 'BY',\n              'BZ'], every:datetime.timedelta=None)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npolars.dataframe.frame.DataFrame | polars.lazyframe.frame.LazyFrame\n\n\n\n\nperiod\ntimedelta\n\nperiod to group by\n\n\nb_cols\nlist\n[‘BX’, ‘BY’, ‘BZ’]\n\n\n\nevery\ntimedelta\nNone\nevery to group by (default: period / 2)",
    "crumbs": [
      "Home",
      "ID identification"
    ]
  },
  {
    "objectID": "01_ids_detection.html#index-of-the-standard-deviation",
    "href": "01_ids_detection.html#index-of-the-standard-deviation",
    "title": "ID identification",
    "section": "Index of the standard deviation",
    "text": "Index of the standard deviation\nUserWarning: Unknown section Notes\n\nsource\n\nadd_neighbor_std\n\n add_neighbor_std (df:polars.lazyframe.frame.LazyFrame,\n                   tau:datetime.timedelta, join_strategy='inner')\n\nGet the neighbor standard deviations\nUserWarning: Unknown section Examples\n\nsource\n\n\ncompute_index_std\n\n compute_index_std (df:polars.lazyframe.frame.LazyFrame)\n\nCompute the standard deviation index based on the given DataFrame\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndf\nLazyFrame\nnoqa: F811\n\n\nReturns\n- pl.LazyFrame: DataFrame with calculated ‘index_std’ column.",
    "crumbs": [
      "Home",
      "ID identification"
    ]
  },
  {
    "objectID": "01_ids_detection.html#index-of-fluctuation",
    "href": "01_ids_detection.html#index-of-fluctuation",
    "title": "ID identification",
    "section": "Index of fluctuation",
    "text": "Index of fluctuation\n\nsource\n\ncompute_index_fluctuation\n\n compute_index_fluctuation (df:polars.lazyframe.frame.LazyFrame,\n                            base_col='B_std')",
    "crumbs": [
      "Home",
      "ID identification"
    ]
  },
  {
    "objectID": "01_ids_detection.html#index-of-the-relative-field-jump",
    "href": "01_ids_detection.html#index-of-the-relative-field-jump",
    "title": "ID identification",
    "section": "Index of the relative field jump",
    "text": "Index of the relative field jump\n\nsource\n\npl_dvec\n\n pl_dvec (columns, *more_columns)\n\n\nsource\n\n\ncompute_index_diff\n\n compute_index_diff (df:polars.lazyframe.frame.LazyFrame,\n                     period:datetime.timedelta, cols)\n\n\nsource\n\n\ncompute_indices\n\n compute_indices (df:polars.dataframe.frame.DataFrame,\n                  tau:datetime.timedelta, bcols:list[str]=['BX', 'BY',\n                  'BZ'])\n\nCompute all index based on the given DataFrame and tau value.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nInput DataFrame.\n\n\ntau\ntimedelta\n\nTime interval value.\n\n\nbcols\nlist\n[‘BX’, ‘BY’, ‘BZ’]\n\n\n\nReturns\nDataFrame\n\nTuple containing DataFrame results for fluctuation index,standard deviation index, and ‘index_num’.",
    "crumbs": [
      "Home",
      "ID identification"
    ]
  },
  {
    "objectID": "01_ids_detection.html#pipelines",
    "href": "01_ids_detection.html#pipelines",
    "title": "ID identification",
    "section": "Pipelines",
    "text": "Pipelines\n\nsource\n\nfilter_indices\n\n filter_indices\n                 (df:polars.dataframe.frame.DataFrame|polars.lazyframe.fra\n                 me.LazyFrame, index_std_threshold:float=2,\n                 index_fluc_threshold:float=1,\n                 index_diff_threshold:float=0.1, sparse_num:int=15)\n\n\nsource\n\n\ndetect_events\n\n detect_events (data:polars.dataframe.frame.DataFrame,\n                tau:datetime.timedelta, ts:datetime.timedelta, bcols,\n                method='liu')",
    "crumbs": [
      "Home",
      "ID identification"
    ]
  },
  {
    "objectID": "10_datasets.html",
    "href": "10_datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "Foundational Dataset Class\n\nsource\n\n\n\n IdsEvents (name:str=None, events:polars.dataframe.frame.DataFrame=None,\n            data:polars.lazyframe.frame.LazyFrame=None,\n            ts:datetime.timedelta=None, tau:datetime.timedelta=None,\n            **extra_data:Any)\n\nUsage docs: https://docs.pydantic.dev/2.6/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\n\nsource\n\n\n\n\n IDsDataset (name:str=None, events:polars.dataframe.frame.DataFrame=None,\n             mag_data:polars.lazyframe.frame.LazyFrame=None,\n             ts:datetime.timedelta=None, tau:datetime.timedelta=None,\n             plasma_data:polars.lazyframe.frame.LazyFrame=None,\n             bcols:list[str]=None, vec_cols:list[str]=None,\n             density_col:str='n', speed_col:str='v',\n             temperature_col:str='T', plasma_meta:dict=None,\n             **extra_data:Any)\n\nUsage docs: https://docs.pydantic.dev/2.6/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\nCode\nlen({'a':1})\n\n\n1",
    "crumbs": [
      "Home",
      "Datasets"
    ]
  },
  {
    "objectID": "10_datasets.html#datasets",
    "href": "10_datasets.html#datasets",
    "title": "Datasets",
    "section": "",
    "text": "Foundational Dataset Class\n\nsource\n\n\n\n IdsEvents (name:str=None, events:polars.dataframe.frame.DataFrame=None,\n            data:polars.lazyframe.frame.LazyFrame=None,\n            ts:datetime.timedelta=None, tau:datetime.timedelta=None,\n            **extra_data:Any)\n\nUsage docs: https://docs.pydantic.dev/2.6/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nAny\n\n\n\nReturns\nNone\ntype: ignore\n\n\n\n\nsource\n\n\n\n\n IDsDataset (name:str=None, events:polars.dataframe.frame.DataFrame=None,\n             mag_data:polars.lazyframe.frame.LazyFrame=None,\n             ts:datetime.timedelta=None, tau:datetime.timedelta=None,\n             plasma_data:polars.lazyframe.frame.LazyFrame=None,\n             bcols:list[str]=None, vec_cols:list[str]=None,\n             density_col:str='n', speed_col:str='v',\n             temperature_col:str='T', plasma_meta:dict=None,\n             **extra_data:Any)\n\nUsage docs: https://docs.pydantic.dev/2.6/concepts/models/\nA base class for creating Pydantic models.\nAttributes: class_vars: The names of classvars defined on the model. private_attributes: Metadata about the private attributes of the model. signature: The signature for instantiating the model.\n__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.\n__pydantic_core_schema__: The pydantic-core schema used to build the SchemaValidator and SchemaSerializer.\n__pydantic_custom_init__: Whether the model has a custom `__init__` function.\n__pydantic_decorators__: Metadata containing the decorators defined on the model.\n    This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.\n__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to\n    __args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.\n__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.\n__pydantic_post_init__: The name of the post-init method for the model, if defined.\n__pydantic_root_model__: Whether the model is a `RootModel`.\n__pydantic_serializer__: The pydantic-core SchemaSerializer used to dump instances of the model.\n__pydantic_validator__: The pydantic-core SchemaValidator used to validate instances of the model.\n\n__pydantic_extra__: An instance attribute with the values of extra fields from validation when\n    `model_config['extra'] == 'allow'`.\n__pydantic_fields_set__: An instance attribute with the names of fields explicitly set.\n__pydantic_private__: Instance attribute with the values of private attributes set on the model instance.\n\n\nCode\nlen({'a':1})\n\n\n1",
    "crumbs": [
      "Home",
      "Datasets"
    ]
  },
  {
    "objectID": "21_datasets_kedro.html",
    "href": "21_datasets_kedro.html",
    "title": "Datasets",
    "section": "",
    "text": "╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:3                                                                                    │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'E1IDsDataset' is not defined\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:3                                                                                    │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'cIDsDataset' is not defined",
    "crumbs": [
      "Home",
      "Datasets"
    ]
  },
  {
    "objectID": "21_event.html",
    "href": "21_event.html",
    "title": "Event",
    "section": "",
    "text": "Code\nclass Event():\n    pass\n\n\n\n\nget_event_data\n\n get_event_data (event:dict, data:xarray.core.dataarray.DataArray,\n                 neighbor:int=0)",
    "crumbs": [
      "Home",
      "Event"
    ]
  },
  {
    "objectID": "utils/02_analysis_utils.html",
    "href": "utils/02_analysis_utils.html",
    "title": "Utils",
    "section": "",
    "text": "source",
    "crumbs": [
      "Home",
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "utils/02_analysis_utils.html#common-codes-used-across-notebooks",
    "href": "utils/02_analysis_utils.html#common-codes-used-across-notebooks",
    "title": "Utils",
    "section": "Common codes used across notebooks",
    "text": "Common codes used across notebooks",
    "crumbs": [
      "Home",
      "Utils",
      "Utils"
    ]
  },
  {
    "objectID": "utils/10_polars.html",
    "href": "utils/10_polars.html",
    "title": "Utils for Polars",
    "section": "",
    "text": "source",
    "crumbs": [
      "Home",
      "Utils",
      "Utils for Polars"
    ]
  },
  {
    "objectID": "utils/10_polars.html#io",
    "href": "utils/10_polars.html#io",
    "title": "Utils for Polars",
    "section": "IO",
    "text": "IO\n\nsource\n\nconvert_to_pd_dataframe\n\n convert_to_pd_dataframe\n                          (df:polars.dataframe.frame.DataFrame|polars.lazy\n                          frame.frame.LazyFrame, modin:bool=False)\n\nConvert a Polars DataFrame or LazyFrame into a pandas-like DataFrame. If modin=True, returns a Modin DataFrame.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\npolars.dataframe.frame.DataFrame | polars.lazyframe.frame.LazyFrame\n\noriginal DataFrame or LazyFrame\n\n\nmodin\nbool\nFalse\nwhether to use modin or not",
    "crumbs": [
      "Home",
      "Utils",
      "Utils for Polars"
    ]
  },
  {
    "objectID": "utils/10_polars.html#functions",
    "href": "utils/10_polars.html#functions",
    "title": "Utils for Polars",
    "section": "Functions",
    "text": "Functions\n\nsource\n\nsort\n\n sort (df:polars.dataframe.frame.DataFrame, col='time')\n\n\nsource\n\n\npl_norm\n\n pl_norm (columns, *more_columns)\n\nComputes the square root of the sum of squares for the given columns.\nArgs: *columns (str): Names of the columns.\nReturns: pl.Expr: Expression representing the square root of the sum of squares.\n\nsource\n\n\ndecompose_vector\n\n decompose_vector (df:polars.dataframe.frame.DataFrame, vector_col,\n                   name=None, suffixes:list=['_x', '_y', '_z'])\n\nDecompose a vector column in a DataFrame into separate columns for each component with custom suffixes.\nParameters: - df (pl.DataFrame): The input DataFrame. - vector_col (str): The name of the vector column to decompose. - name (str, optional): Base name for the decomposed columns. If None, uses vector_col as the base name. - suffixes (list, optional): A list of suffixes to use for the decomposed columns. If None or not enough suffixes are provided, defaults to ’_0’, ’_1’, etc.\nReturns: - pl.DataFrame: A DataFrame with the original vector column decomposed into separate columns.",
    "crumbs": [
      "Home",
      "Utils",
      "Utils for Polars"
    ]
  },
  {
    "objectID": "utils/15_kedro.html",
    "href": "utils/15_kedro.html",
    "title": "discontinuitypy",
    "section": "",
    "text": "Standard import\nfrom kedro.pipeline import Pipeline, node\nfrom kedro.pipeline.modular_pipeline import pipeline\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([\n        ...\n    ])\n\n\nCode\ndef load_catalog(conf_source: str, catalog_source: str = \"catalog\"):\n    # Initialise a ConfigLoader\n    conf_loader = OmegaConfigLoader(conf_source)\n\n    # Load the data catalog configuration from catalog.yml\n    conf_catalog = conf_loader.get(catalog_source)\n\n    # Create the DataCatalog instance from the configuration\n    catalog = DataCatalog.from_config(conf_catalog)\n    \n    return catalog\n\n\nThe following load_catalog provides project-aware access to the catalog. The preceding load_catalog only works when notebook is run from the project root.\n\nsource\n\n\n\n load_context (project_path:str, params_only:bool=False,\n               catalog_only:bool=False)",
    "crumbs": [
      "Home",
      "Utils",
      "`Kedro`"
    ]
  },
  {
    "objectID": "utils/15_kedro.html#kedro",
    "href": "utils/15_kedro.html#kedro",
    "title": "discontinuitypy",
    "section": "",
    "text": "Standard import\nfrom kedro.pipeline import Pipeline, node\nfrom kedro.pipeline.modular_pipeline import pipeline\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([\n        ...\n    ])\n\n\nCode\ndef load_catalog(conf_source: str, catalog_source: str = \"catalog\"):\n    # Initialise a ConfigLoader\n    conf_loader = OmegaConfigLoader(conf_source)\n\n    # Load the data catalog configuration from catalog.yml\n    conf_catalog = conf_loader.get(catalog_source)\n\n    # Create the DataCatalog instance from the configuration\n    catalog = DataCatalog.from_config(conf_catalog)\n    \n    return catalog\n\n\nThe following load_catalog provides project-aware access to the catalog. The preceding load_catalog only works when notebook is run from the project root.\n\nsource\n\n\n\n load_context (project_path:str, params_only:bool=False,\n               catalog_only:bool=False)",
    "crumbs": [
      "Home",
      "Utils",
      "`Kedro`"
    ]
  },
  {
    "objectID": "00_ids_finder.html",
    "href": "00_ids_finder.html",
    "title": "Finding magnetic discontinuities",
    "section": "",
    "text": "It can be divided into two parts:",
    "crumbs": [
      "Home",
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#processing-the-whole-dataset",
    "href": "00_ids_finder.html#processing-the-whole-dataset",
    "title": "Finding magnetic discontinuities",
    "section": "Processing the whole dataset",
    "text": "Processing the whole dataset\nNotes that the candidates only require a small portion of the data so we can compress the data to speed up the processing.\n\nsource\n\ncompress_data_by_cands\n\n compress_data_by_cands (data:polars.dataframe.frame.DataFrame,\n                         candidates:polars.dataframe.frame.DataFrame)\n\nCompress the data for parallel processing\n\nsource\n\n\nids_finder\n\n ids_finder (detection_df:polars.lazyframe.frame.LazyFrame,\n             tau:datetime.timedelta, ts:datetime.timedelta, bcols=None,\n             extract_df:polars.lazyframe.frame.LazyFrame=None, **kwargs)\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndetection_df\nLazyFrame\n\ndata used for anomaly dectection (typically low cadence data)\n\n\ntau\ntimedelta\n\n\n\n\nts\ntimedelta\n\n\n\n\nbcols\nNoneType\nNone\n\n\n\nextract_df\nLazyFrame\nNone\ndata used for feature extraction (typically high cadence data),\n\n\nkwargs\n\n\n\n\n\n\nwrapper function for partitioned input used in Kedro\n\nsource\n\n\nextract_features\n\n extract_features (partitioned_input:dict[str,typing.Callable[...,polars.l\n                   azyframe.frame.LazyFrame]], tau:float, ts:float,\n                   **kwargs)\n\nwrapper function for partitioned input\n\n\n\n\nType\nDetails\n\n\n\n\npartitioned_input\ndict\n\n\n\ntau\nfloat\nin seconds, yaml input\n\n\nts\nfloat\nin seconds, yaml input\n\n\nkwargs\n\n\n\n\nReturns\nDataFrame",
    "crumbs": [
      "Home",
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#conventions",
    "href": "00_ids_finder.html#conventions",
    "title": "Finding magnetic discontinuities",
    "section": "Conventions",
    "text": "Conventions\nAs we are dealing with multiple spacecraft, we need to be careful about naming conventions. Here are the conventions we use in this project.\n\nsat_id: name of the spacecraft. We also use abbreviation, for example\n\nsta for STEREO-A\nthb for ARTEMIS-B\n\nsat_state: state data of the spacecraft\nb_vl: maximum variance vector of the magnetic field, (major eigenvector)\n\nData Level\n\nl0: unprocessed\nl1: cleaned data, fill null value, add useful columns\nl2: time-averaged data\n\n\nColumns naming conventions\n\nradial_distance: radial distance of the spacecraft, in units of \\(AU\\)\nplasma_speed: solar wind plasma speed, in units of \\(km/s\\)\nsw_elevation: solar wind elevation angle, in units of \\(\\degree\\)\nsw_azimuth: solar wind azimuth angle, in units of \\(\\degree\\)\nv_{x,y,z} or sw_vel_{X,Y,Z}: solar wind plasma speed in the ANY coordinate system, in units of \\(km/s\\)\n\nsw_vel_{r,t,n}: solar wind plasma speed in the RTN coordinate system, in units of \\(km/s\\)\nsw_vel_gse_{x,y,z}: solar wind plasma speed in the GSE coordinate system, in units of \\(km/s\\)\nsw_vel_lmn_{x,y,z}: solar wind plasma speed in the LMN coordinate system, in units of \\(km/s\\)\n\nv_l or sw_vel_l: abbreviation for sw_vel_lmn_1\nv_mn or sw_vel_mn (deprecated)\n\n\nplasma_density: plasma density, in units of \\(1/cm^{3}\\)\nplasma_temperature: plasma temperature, in units of \\(K\\)\nB_{x,y,z}: magnetic field in ANY coordinate system\n\nb_rtn_{x,y,z} or b_{r,t,n}: magnetic field in the RTN coordinate system\nb_gse_{x,y,z}: magnetic field in the GSE coordinate system\n\nB_mag: magnetic field magnitude\nVl_{x,y,z} or b_vecL_{X,Y,Z}: maxium variance vector of the magnetic field in ANY coordinate system\n\nb_vecL_{r,t,n}: maxium variance vector of the magnetic field in the RTN coordinate system\n\nmodel_b_{r,t,n}: modelled magnetic field in the RTN coordinate system\nstate : 1 for solar wind, 0 for non-solar wind\nL_mn{_norm}: thickness of the current sheet in MN direction, in units of \\(km\\)\nj0{_norm}: current density, in units of \\(nA/m^2\\)\n\nNotes: we recommend use unique names for each variable, for example, plasma_speed instead of speed. Because it is easier to search and replace the variable names in the code whenever necessary.\nFor the unit, by default we use\n\nlength : \\(km\\)\ntime : \\(s\\)\nmagnetic field : \\(nT\\)\ncurrent : \\(nA/m^2\\)",
    "crumbs": [
      "Home",
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#test",
    "href": "00_ids_finder.html#test",
    "title": "Finding magnetic discontinuities",
    "section": "Test",
    "text": "Test\n\nTest feature engineering\n\n\nCode\n# from tsflex.features import MultipleFeatureDescriptors, FeatureCollection\n\n# from tsflex.features.integrations import catch22_wrapper\n# from pycatch22 import catch22_all\n\n\n\n\nCode\n# tau_pd = pd.Timedelta(tau)\n\n# catch22_feats = MultipleFeatureDescriptors(\n#     functions=catch22_wrapper(catch22_all),\n#     series_names=bcols,  # list of signal names\n#     windows = tau_pd, strides=tau_pd/2,\n# )\n\n# fc = FeatureCollection(catch22_feats)\n# features = fc.calculate(data, return_df=True)  # calculate the features on your data\n\n\n\n\nCode\n# features_pl = pl.DataFrame(features.reset_index()).sort('time')\n# df = candidates_pl.join_asof(features_pl, on='time').to_pandas()\n\n\n\n\nCode\n# profile = ProfileReport(df, title=\"JUNO Candidates Report\")\n# profile.to_file(\"jno.html\")\n\n\n\n\nBenchmark",
    "crumbs": [
      "Home",
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#notes",
    "href": "00_ids_finder.html#notes",
    "title": "Finding magnetic discontinuities",
    "section": "Notes",
    "text": "Notes\n\nTODOs\n\nFeature engineering\nFeature selection",
    "crumbs": [
      "Home",
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "00_ids_finder.html#obsolete-codes",
    "href": "00_ids_finder.html#obsolete-codes",
    "title": "Finding magnetic discontinuities",
    "section": "Obsolete codes",
    "text": "Obsolete codes",
    "crumbs": [
      "Home",
      "Finding magnetic discontinuities"
    ]
  },
  {
    "objectID": "02_ids_properties.html",
    "href": "02_ids_properties.html",
    "title": "ID properties",
    "section": "",
    "text": "source",
    "crumbs": [
      "Home",
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#duration",
    "href": "02_ids_properties.html#duration",
    "title": "ID properties",
    "section": "Duration",
    "text": "Duration\n\nsource\n\ncalc_candidate_duration\n\n calc_candidate_duration (candidate:modin.pandas.series.Series, data,\n                          method='distance', **kwargs)",
    "crumbs": [
      "Home",
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#minimum-variance-analysis-mva-features",
    "href": "02_ids_properties.html#minimum-variance-analysis-mva-features",
    "title": "ID properties",
    "section": "Minimum variance analysis (MVA) features",
    "text": "Minimum variance analysis (MVA) features",
    "crumbs": [
      "Home",
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#field-rotation-angles",
    "href": "02_ids_properties.html#field-rotation-angles",
    "title": "ID properties",
    "section": "Field rotation angles",
    "text": "Field rotation angles\nThe PDF of the field rotation angles across the solar-wind IDs is well fitted by the exponential function exp(−θ/)…\n\nsource\n\nget_data_at_times\n\n get_data_at_times (data:xarray.core.dataarray.DataArray, times)\n\nSelect data at specified times.\n\nsource\n\n\ncalc_rotation_angle\n\n calc_rotation_angle (v1, v2)\n\nComputes the rotation angle between two vectors.\nParameters: - v1: The first vector(s). - v2: The second vector(s).\n\nsource\n\n\ncalc_events_rotation_angle\n\n calc_events_rotation_angle (events, data:xarray.core.dataarray.DataArray)\n\nComputes the rotation angle(s) at two different time steps.",
    "crumbs": [
      "Home",
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#normal-direction",
    "href": "02_ids_properties.html#normal-direction",
    "title": "ID properties",
    "section": "Normal direction",
    "text": "Normal direction\n\nsource\n\ncalc_normal_direction\n\n calc_normal_direction (v1, v2, normalize=True)\n\nComputes the normal direction of two vectors.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nv1\narray_like\n\nThe first vector(s).\n\n\nv2\narray_like\n\nThe second vector(s).\n\n\nnormalize\nbool\nTrue\n\n\n\nReturns\nndarray\n\n\n\n\n\n\nsource\n\n\ncalc_events_normal_direction\n\n calc_events_normal_direction (events,\n                               data:xarray.core.dataarray.DataArray)\n\nComputes the normal directions(s) at two different time steps.\n\nsource\n\n\ncalc_events_vec_change\n\n calc_events_vec_change (events, data:xarray.core.dataarray.DataArray)\n\nUtils function to calculate features related to the change of the magnetic field",
    "crumbs": [
      "Home",
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#pipelines",
    "href": "02_ids_properties.html#pipelines",
    "title": "ID properties",
    "section": "Pipelines",
    "text": "Pipelines\npatch pdp.ApplyToRows to work with modin and xorbits DataFrames\nPipelines Class for processing IDs\n\n\nIDsPdPipeline\n\n IDsPdPipeline ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nprocess_events\n\n process_events (candidates_pl:polars.dataframe.frame.DataFrame,\n                 sat_fgm:xarray.core.dataarray.DataArray,\n                 data_resolution:datetime.timedelta, modin=True, **kwargs)\n\nProcess candidates DataFrame\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncandidates_pl\nDataFrame\n\npotential candidates DataFrame\n\n\nsat_fgm\nDataArray\n\nsatellite FGM data\n\n\ndata_resolution\ntimedelta\n\ntime resolution of the data\n\n\nmodin\nbool\nTrue\n\n\n\nkwargs\n\n\n\n\n\nReturns\nDataFrame",
    "crumbs": [
      "Home",
      "ID properties"
    ]
  },
  {
    "objectID": "02_ids_properties.html#test",
    "href": "02_ids_properties.html#test",
    "title": "ID properties",
    "section": "Test",
    "text": "Test\n\nTest parallelization\nGenerally mapply and modin are the fastest. xorbits is expected to be the fastest but it is not and it is the slowest one.\n#| notest\nsat = 'jno'\ncoord = 'se'\ncols = [\"BX\", \"BY\", \"BZ\"]\ntau = timedelta(seconds=60)\ndata_resolution = timedelta(seconds=1)\n\nif True:\n    year = 2012\n    files = f'../data/{sat}_data_{year}.parquet'\n    output = f'../data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n\n    data = pl.scan_parquet(files).set_sorted('time').collect()\n\n    indices = compute_indices(data, tau)\n    # filter condition\n    sparse_num = tau / data_resolution // 3\n    filter_condition = filter_indices(sparse_num = sparse_num)\n\n    candidates = indices.filter(filter_condition).with_columns(pl_format_time(tau)).sort('time')\n    \n    data_c = compress_data_by_cands(data, candidates, tau)\n    sat_fgm = df2ts(data_c, cols, attrs={\"units\": \"nT\"})\n\n\nCode\ncandidates_pd = candidates.to_pandas()\ncandidates_modin = mpd.DataFrame(candidates_pd)\n# candidates_x = xpd.DataFrame(candidates_pd)\n\n\n\n\nTest different libraries to parallelize the computation\nif True:\n    pdp_test = pdp.ApplyToRows(\n        lambda candidate: calc_candidate_duration(candidate, sat_fgm),  # fast a little bit\n        # lambda candidate: calc_duration(get_candidate_data_xr(candidate, sat_fgm)),\n        # lambda candidate: calc_duration(sat_fgm.sel(time=slice(candidate['tstart'], candidate['tstop']))),\n        func_desc=\"calculating duration parameters\",\n    )\n    \n    # process_events(candidates_modin, sat_fgm, sat_state, data_resolution)\n    \n    # ---\n    # successful cases\n    # ---\n    # candidates_pd.mapply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works, 4.2 secs\n    # candidates_pd.mapply(calc_candidate_duration, axis=1, data=sat_fgm) # this works, but a little bit slower, 6.7 secs\n    \n    # candidates_pd.apply(calc_candidate_duration, axis=1, data=sat_fgm) # Standard case: 24+s secs\n    # candidates_pd.swifter.apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 80 secs\n    # candidates_pd.swifter.set_dask_scheduler(scheduler=\"threads\").apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 60 secs\n    # candidates_modin.apply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works with ray, 6 secs # NOTE: can not work with dask\n    # candidates_x.apply(calc_candidate_duration, axis=1, data=sat_fgm) # 30 seconds\n    # pdp_test(candidates_modin) # this works, 8 secs\n    \n    # ---\n    # failed cases\n    # ---\n    # candidates_modin.apply(calc_candidate_duration, axis=1, data=sat_fgm) # AttributeError: 'DataFrame' object has no attribute 'sel'\n\n\n\n\nCode\nimport timeit\nfrom functools import partial\n\n\n\n\nCode\ndef benchmark(task_dict, number=1):\n    results = {}\n    for name, (data, task) in task_dict.items():\n        try:\n            time_taken = timeit.timeit(\n                lambda: task(data),\n                number=number\n            )\n            results[name] = time_taken / number\n        except Exception as e:\n            results[name] = str(e)\n    return results\n\n\n\n\nCode\ndef benchmark_results(results, sat_fgm):\n    func = partial(calc_candidate_duration, data=sat_fgm)\n    task_dict = {\n        'pandas': (candidates_pd, lambda _: _.apply(func, axis=1)),\n        'pandas-mapply': (candidates_pd, lambda _: _.mapply(func, axis=1)),\n        'modin': (candidates_modin, lambda _: _.apply(func, axis=1)),\n        # 'xorbits': (candidates_x, lambda _: _.apply(func, axis=1)),\n    }\n\n    results = benchmark(task_dict)\n    return results",
    "crumbs": [
      "Home",
      "ID properties"
    ]
  },
  {
    "objectID": "properties/00_duration.html",
    "href": "properties/00_duration.html",
    "title": "Duration",
    "section": "",
    "text": "They might be multiple ways to define the duration of a discontinuity. Here are some possibilities:\nNotes:\nCaveats:",
    "crumbs": [
      "Home",
      "Properties",
      "Duration"
    ]
  },
  {
    "objectID": "properties/00_duration.html#maxium-distance-method",
    "href": "properties/00_duration.html#maxium-distance-method",
    "title": "Duration",
    "section": "Maxium distance method",
    "text": "Maxium distance method\n\nsource\n\nts_max_distance\n\n ts_max_distance (ts:xarray.core.dataarray.DataArray, coord:str='time')\n\nCompute the time interval when the timeseries has maxium cumulative variation\n\n\ntest for ts_max_distance function\ntime = pd.date_range('2000-01-01', periods=10)\nx = np.linspace(0, np.pi, 10)\n# generate data circular in three dimensions, so the biggest distance is between the first and the last point    \ndata = np.zeros((10, 3))\ndata[:, 0] = np.sin(x)\ndata[:, 1] = np.cos(x)\nts = xr.DataArray(data, coords={'time': time}, dims=['time', 'space'])\nstart, end = ts_max_distance(ts)\nassert start == time[0]\nassert end == time[-1]",
    "crumbs": [
      "Home",
      "Properties",
      "Duration"
    ]
  },
  {
    "objectID": "properties/00_duration.html#maxium-derivative-method",
    "href": "properties/00_duration.html#maxium-derivative-method",
    "title": "Duration",
    "section": "Maxium derivative method",
    "text": "Maxium derivative method\n\nsource\n\nget_time_from_condition\n\n get_time_from_condition (vec:xarray.core.dataarray.DataArray, threshold,\n                          condition_type)\n\n\nsource\n\n\nfind_start_end_times\n\n find_start_end_times (vec_diff_mag:xarray.core.dataarray.DataArray,\n                       d_time, threshold)\n\n\nsource\n\n\ncalc_d_duration\n\n calc_d_duration (vec:xarray.core.dataarray.DataArray, d_time, threshold)\n\n\nsource\n\n\nts_max_derivative\n\n ts_max_derivative (vec:xarray.core.dataarray.DataArray,\n                    threshold_ratio=0.25)\n\n\nsource\n\n\ncalc_duration\n\n calc_duration (ts:xarray.core.dataarray.DataArray,\n                method:Literal['distance','derivative']='distance',\n                **kwargs)",
    "crumbs": [
      "Home",
      "Properties",
      "Duration"
    ]
  },
  {
    "objectID": "properties/00_duration.html#obsolete-codes",
    "href": "properties/00_duration.html#obsolete-codes",
    "title": "Duration",
    "section": "Obsolete codes",
    "text": "Obsolete codes\nThis is obsolete codes because the timewindow now is overlapping. No need to consider where magnetic discontinuities happens in the boundary of one timewindow.\n\n\nCode\ndef calc_candidate_d_duration(candidate, data) -&gt; pd.Series:\n    try:\n        if pd.isnull(candidate['d_tstart']) or pd.isnull(candidate['d_tstop']):\n            candidate_data = get_candidate_data(candidate, data, neighbor=1)\n            d_time = candidate['d_time']\n            threshold = candidate['threshold']\n            return calc_d_duration(candidate_data, d_time, threshold)\n        else:\n            return pd.Series({\n                'd_tstart': candidate['d_tstart'],\n                'd_tstop': candidate['d_tstop'],\n            })\n    except Exception as e:\n        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n        raise e\n\n\n\nCalibrates candidate duration\nThis calibration is based on the assumption that the magnetic discontinuity is symmetric around the center of time, which is not always true.\nSo instead of calibrating the duration, we drop the events. - Cons: Might influence the statistics of occurrence rate, but - Pros: More robust results about the properties of the magnetic discontinuity.\n\n\nCode\ndef calibrate_candidate_duration(\n    candidate: pd.Series, data:xr.DataArray, data_resolution, ratio = 3/4\n):\n    \"\"\"\n    Calibrates the candidate duration. \n    - If only one of 'd_tstart' or 'd_tstop' is provided, calculates the missing one based on the provided one and 'd_time'.\n    - Then if this is not enough points between 'd_tstart' and 'd_tstop', returns None for both.\n    \n    \n    Parameters\n    ----------\n    - candidate (pd.Series): The input candidate with potential missing 'd_tstart' or 'd_tstop'.\n    \n    Returns\n    -------\n    - pd.Series: The calibrated candidate.\n    \"\"\"\n    \n    start_notnull = pd.notnull(candidate['d_tstart'])\n    stop_notnull = pd.notnull(candidate['d_tstop']) \n    \n    match start_notnull, stop_notnull:\n        case (True, True):\n            d_tstart = candidate['d_tstart']\n            d_tstop = candidate['d_tstop']\n        case (True, False):\n            d_tstart = candidate['d_tstart']\n            d_tstop = candidate['d_time'] -  candidate['d_tstart'] + candidate['d_time']\n        case (False, True):\n            d_tstart = candidate['d_time'] -  candidate['d_tstop'] + candidate['d_time']\n            d_tstop = candidate['d_tstop']\n        case (False, False):\n            return pandas.Series({\n                'd_tstart': None,\n                'd_tstop': None,\n            })\n    \n    duration = d_tstop - d_tstart\n    num_of_points_between = data.time.sel(time=slice(d_tstart, d_tstop)).count().item()\n    \n    if num_of_points_between &lt;= (duration/data_resolution) * ratio:\n        d_tstart = None\n        d_tstop = None\n    \n    return pandas.Series({\n        'd_tstart': d_tstart,\n        'd_tstop': d_tstop,\n    })\n\n\n\n\nCode\ndef calibrate_candidates_duration(candidates, sat_fgm, data_resolution):\n    # calibrate duration\n\n    calibrate_duration = pdp.ApplyToRows(\n        lambda candidate: calibrate_candidate_duration(\n            candidate, sat_fgm, data_resolution\n        ),\n        func_desc=\"calibrating duration parameters if needed\",\n    )\n\n    temp_candidates = candidates.loc[\n        lambda df: df[\"d_tstart\"].isnull() | df[\"d_tstop\"].isnull()\n    ]  # temp_candidates = candidates.query('d_tstart.isnull() | d_tstop.isnull()') # not implemented in `modin`\n\n    if not temp_candidates.empty:\n        temp_candidates_updated = calibrate_duration(sat_fgm, data_resolution).apply(\n            temp_candidates\n        )\n        candidates.update(temp_candidates_updated)\n    return candidates",
    "crumbs": [
      "Home",
      "Properties",
      "Duration"
    ]
  }
]