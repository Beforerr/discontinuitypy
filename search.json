[
  {
    "objectID": "01_pipelines.html",
    "href": "01_pipelines.html",
    "title": "Pipelines",
    "section": "",
    "text": "We are using Kedro to build a data pipeline. A pipeline is a collection of nodes that are connected to each other. Each node is a function that takes inputs and produces outputs. The inputs and outputs are data sets of different layer/level.\nThis notebook mainly demonstrate the concept and common building blocks of a pipeline, see each mission notebook for implementation details.\nCode\n# Kerdo\nfrom kedro.pipeline import Pipeline, node\nfrom kedro.pipeline.modular_pipeline import pipeline"
  },
  {
    "objectID": "01_pipelines.html#magnetic-field-data-pipeline",
    "href": "01_pipelines.html#magnetic-field-data-pipeline",
    "title": "Pipelines",
    "section": "Magnetic field data pipeline",
    "text": "Magnetic field data pipeline\n\nLoading data\n\n\nCode\ndef download_mag_data(\n    start: str = None,\n    end: str = None,\n    ts: str = None,  # time resolution\n    probe: str = None,\n    coord: str = None,\n):\n    \"\"\"Downloading data\n    \"\"\"\n    ...\n\n\ndef load_mag_data(\n    start: str = None,\n    end: str = None,\n    ts: str = None,  # time resolution\n    probe: str = None,\n    coord: str = None,\n):\n    \"\"\"Load data into a proper data structure, like dataframe.\n\n    - Downloading data\n    - Converting data structure\n    \"\"\"\n    ...\n\n\n\n\nPreprocessing data\n\n\nCode\ndef preprocess_mag_data(\n    raw_data: Any | pl.DataFrame = None,\n    start: str = None,\n    end: str = None,\n    ts: str = None,  # time resolution\n    coord: str = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Preprocess the raw dataset (only minor transformations)\n\n    - Applying naming conventions for columns\n    - Parsing and typing data (like from string to datetime for time columns)\n    - Structuring the data (like pivoting, unpivoting, etc.)\n    - Changing storing format (like from `csv` to `parquet`)\n    - Dropping null columns \n    - Dropping duplicate time\n    - Resampling data to a given time resolution (better to do in the next stage)\n    - ... other 'transformations' commonly performed at this stage.\n    \"\"\"\n    pass\n\n\n\n\nProcessing data\nSome common preprocessing steps are:\n\nPartition data by year, see ids_finder.utils.basic.partition_data_by_year\n\nNote: we process the data every year to minimize the memory usage and to avoid the failure of the processing (so need to process all the data again if only fails sometimes).\n\n\nCode\ndef process_mag_data(\n    raw_data: Any | pl.DataFrame,\n    ts: str = None,  # time resolution\n    coord: str = None,\n) -&gt; pl.DataFrame | Dict[str, pl.DataFrame]:\n    \"\"\"\n    Corresponding to primary data layer, where source data models are transformed into domain data models\n\n    - Transforming coordinate system if needed\n    - Smoothing data\n    - Resampling data to a given time resolution\n    - Partitioning data, for the sake of memory\n    \"\"\"\n    pass\n\ndef extract_features():\n    pass\n\n\n\nsource\n\n\nextract_features\n\n extract_features ()\n\n\nsource\n\n\nprocess_mag_data\n\n process_mag_data (raw_data:Union[Any,polars.dataframe.frame.DataFrame],\n                   ts:str=None, coord:str=None)\n\nCorresponding to primary data layer, where source data models are transformed into domain data models\n\nTransforming coordinate system if needed\nSmoothing data\nResampling data to a given time resolution\nPartitioning data, for the sake of memory\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nUnion\n\n\n\n\nts\nstr\nNone\ntime resolution\n\n\ncoord\nstr\nNone\n\n\n\nReturns\nUnion\n\n\n\n\n\n\n\nPipeline\n\n\nCode\ndef create_mag_data_pipeline(\n    sat_id: str,  # satellite id, used for namespace\n    ts: int = 1,  # time resolution,\n    tau: str = '60s',  # time window\n    **kwargs,\n) -&gt; Pipeline:\n    \n    ts_str = f\"ts_{ts}s\"\n    \n    node_load_data = node(\n        load_mag_data,\n        inputs=dict(\n            start=\"params:start_date\",\n            end=\"params:end_date\",\n        ),\n        outputs=f\"raw_mag\",\n        name=f\"load_{sat_id.upper()}_magnetic_field_data\",\n    )\n\n    node_preprocess_data = node(\n        preprocess_mag_data,\n        inputs=dict(\n            raw_data=f\"raw_mag\",\n            start=\"params:start_date\",\n            end=\"params:end_date\",\n        ),\n        outputs=f\"inter_mag_{ts_str}\",\n        name=f\"preprocess_{sat_id.upper()}_magnetic_field_data\",\n    )\n\n    node_process_data = node(\n        process_mag_data,\n        inputs=f\"inter_mag_{ts_str}\",\n        outputs=f\"primary_mag_{ts_str}\",\n        name=f\"process_{sat_id.upper()}_magnetic_field_data\",\n    )\n\n    node_extract_features = node(\n        extract_features,\n        inputs=[f\"primary_mag_{ts_str}\", \"params:tau\", \"params:extract_params\"],\n        outputs=f\"feature_tau_{tau}\",\n        name=f\"extract_{sat_id}_features\",\n    )\n\n    nodes = [\n        node_load_data,\n        node_preprocess_data,\n        node_process_data,\n        node_extract_features,\n    ]\n\n    pipelines = pipeline(\n        nodes,\n        namespace=sat_id,\n        parameters={\n            \"params:start_date\": \"params:jno_start_date\",\n            \"params:end_date\": \"params:jno_end_date\",\n            \"params:tau\": tau,\n        },\n    )\n\n    return pipelines\n\n\n\nsource\n\n\ncreate_mag_data_pipeline\n\n create_mag_data_pipeline (sat_id:str, ts:int=1, tau:str='60s', **kwargs)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\nstr\n\nsatellite id, used for namespace\n\n\nts\nint\n1\ntime resolution,\n\n\ntau\nstr\n60s\ntime window\n\n\nkwargs\n\n\n\n\n\nReturns\nPipeline\n\n\n\n\n\n\n\nCode\nclass DatasetConfig:\n    def __init__(self, sat_id, download_func, preprocess_func, process_func):\n        self.sat_id = sat_id\n        self.download_func = download_func\n        self.preprocess_func = preprocess_func\n        self.process_func = process_func\n\nclass PipelineGenerator:\n    def __init__(self, config: DatasetConfig, ts='1s', tau='60s'):\n        self.config = config\n        self.ts = ts\n        self.tau = tau\n\n    def _node(self, func, inputs, outputs, name):\n        return node(func, inputs=inputs, outputs=outputs, name=name)\n\n    def generate_pipeline(self):\n        node_download = self._node(\n            self.config.download_func,\n            inputs=dict(start=\"params:start_date\", end=\"params:end_date\"),\n            outputs=f\"raw_data_{self.ts}\",\n            name=f\"download_{self.config.sat_id.upper()}_data\"\n        )\n\n        node_preprocess = self._node(\n            self.config.preprocess_func,\n            inputs=dict(raw_data=f\"raw_data_{self.ts}\", start=\"params:start_date\", end=\"params:end_date\"),\n            outputs=f\"inter_data_{self.ts}\",\n            name=f\"preprocess_{self.config.sat_id.upper()}_data\"\n        )\n\n        node_process = self._node(\n            self.config.process_func,\n            inputs=f\"inter_data_{self.ts}\",\n            outputs=f\"primary_data_rtn_{self.ts}\",\n            name=f\"process_{self.config.sat_id.upper()}_data\"\n        )\n\n        node_extract = self._node(\n            extract_features,\n            inputs=[f\"primary_data_rtn_{self.ts}\", \"params:tau\", \"params:extract_params\"],\n            outputs=f\"feature_tau_{self.tau}\",\n            name=f\"extract_{self.config.sat_id}_features\"\n        )\n\n        return pipeline(\n            [node_download, node_preprocess, node_process, node_extract],\n            namespace=self.config.sat_id,\n            parameters={\"params:start_date\": \"params:jno_start_date\", \"params:end_date\": \"params:jno_end_date\", \"params:tau\": self.tau}\n        )"
  },
  {
    "objectID": "01_pipelines.html#state-data-pipeline",
    "href": "01_pipelines.html#state-data-pipeline",
    "title": "Pipelines",
    "section": "State data pipeline",
    "text": "State data pipeline\n\nLoading data\n\n\nCode\ndef download_state_data(\n    start: str = None,\n    end: str = None,\n    ts: str = None,  # time resolution\n    probe: str = None,\n    coord: str = None,\n):\n    ...\n\n\ndef load_state_data(\n    start: str = None,\n    end: str = None,\n    ts: str = None,  # time resolution\n    probe: str = None,\n    coord: str = None,\n):\n    \"\"\"Loading the raw dataset\n    \n    - Downloading data\n    - Reading data into a proper data structure, like dataframe.\n        - Parsing original data (dealing with delimiters, missing values, etc.)\n    \"\"\"\n    ...\n\n\n\n\nPreprocessing data\n\n\nCode\ndef preprocess_state_data(\n    raw_data: Any = None,\n    start: str = None,\n    end: str = None,\n    ts: str = None,  # time resolution\n    coord: str = None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Preprocess the raw dataset (only minor transformations)\n\n    - Applying naming conventions for columns\n    - Parsing and typing data (like from string to datetime for time columns)\n    - Structuring the data (like pivoting, unpivoting, etc.)\n    - Changing storing format (like from `csv` to `parquet`)\n    - Dropping null columns\n    - Resampling data to a given time resolution (better to do in the next stage)\n    - ... other 'transformations' commonly performed at this stage.\n    \"\"\"\n    pass\n\n\n\n\nProcessing data\n\n\nCode\ndef process_state_data(df: pl.DataFrame, columns=None,) -&gt; pl.DataFrame:\n    \"\"\"\n    Corresponding to primary data layer, where source data models are transformed into domain data models\n\n    - Transforming data to RTN (Radial-Tangential-Normal) coordinate system\n    - Discarding unnecessary columns\n    - Smoothing data\n    - Resampling data to a given time resolution\n    - Partitioning data, for the sake of memory\n    \"\"\"\n    pass\n\n\n\n\nPipeline\n\n\nCode\ndef create_state_data_pipeline(\n    sat_id,\n    ts: str = '1h',  # time resolution\n) -&gt; Pipeline:\n    \n    node_load_data = node(\n        load_state_data,\n        inputs=dict(\n            start=\"params:start_date\",\n            end=\"params:end_date\",\n        ),\n        outputs=f\"raw_state\",\n        name=f\"download_{sat_id.upper()}_state_data\",\n    )\n    \n    node_preprocess_data = node(\n        preprocess_state_data,\n        inputs=dict(\n            raw_data=f\"raw_state\",\n            start=\"params:start_date\",\n            end=\"params:end_date\",\n        ),\n        outputs=f\"inter_state_{ts}\",\n        name=f\"preprocess_{sat_id.upper()}_state_data\",\n    )\n    \n    node_process_data = node(\n        process_state_data,\n        inputs=f\"inter_state_{ts}\",\n        outputs=f\"primary_state_{ts}\",\n        name=f\"process_{sat_id.upper()}_state_data\",\n    )\n\n    nodes = [\n        node_load_data,\n        node_preprocess_data,\n        node_process_data,\n    ]\n\n    pipelines = pipeline(\n        nodes,\n        namespace=sat_id,\n        parameters={\n            \"params:start_date\": \"params:jno_start_date\",\n            \"params:end_date\": \"params:jno_end_date\",\n        },\n    )\n\n    return pipelines"
  },
  {
    "objectID": "01_pipelines.html#candidate-pipeline",
    "href": "01_pipelines.html#candidate-pipeline",
    "title": "Pipelines",
    "section": "Candidate pipeline",
    "text": "Candidate pipeline\n\n\nCode\ndef combine_features(df: pl.DataFrame, state: pl.DataFrame) -&gt; pl.DataFrame:\n    pass\n\ndef create_candidate_pipeline(sat_id, **kwargs) -&gt; Pipeline:\n    time_resolution = \"1s\"\n\n    node_combine_features = node(\n        combine_features,\n        inputs=[\n            f\"{sat_id}.feature_{time_resolution}\",\n            f\"{sat_id}.primary_state_1h\",\n        ],\n        outputs=f\"candidates.{sat_id}_{time_resolution}\",\n    )\n\n    nodes = [node_combine_features]\n    return pipeline(nodes)\n\n\n\n\nCode\ndef combine_candidates(dict):\n    pass\n\n# node_thm_extract_features = node(\n#     extract_features,\n#     inputs=[\"primary_thm_rtn_1s\", \"params:tau\", \"params:thm_1s_params\"],\n#     outputs=\"candidates_thm_rtn_1s\",\n#     name=\"extract_ARTEMIS_features\",\n# )\n\n# node_combine_candidates = node(\n#     combine_candidates,\n#     inputs=dict(\n#         sta_candidates=\"candidates_sta_rtn_1s\",\n#         jno_candidates=\"candidates_jno_ss_se_1s\",\n#         thm_candidates=\"candidates_thm_rtn_1s\",\n#     ),\n#     outputs=\"candidates_all_1s\",\n#     name=\"combine_candidates\",\n# )\n\n\n\nsource\n\ncombine_candidates\n\n combine_candidates (dict)\n\n\nsource\n\n\ncreate_pipeline\n\n create_pipeline (sat_id='sta', tau='60s', ts_mag='1s', ts_state='1h',\n                  **kwargs)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\nstr\nsta\n\n\n\ntau\nstr\n60s\n\n\n\nts_mag\nstr\n1s\ntime resolution of magnetic field data\n\n\nts_state\nstr\n1h\ntime resolution of state data\n\n\nkwargs\n\n\n\n\n\nReturns\nPipeline"
  },
  {
    "objectID": "utils/02_analysis_utils.html",
    "href": "utils/02_analysis_utils.html",
    "title": "Utils",
    "section": "",
    "text": "source"
  },
  {
    "objectID": "utils/02_analysis_utils.html#common-codes-used-across-notebooks",
    "href": "utils/02_analysis_utils.html#common-codes-used-across-notebooks",
    "title": "Utils",
    "section": "Common codes used across notebooks",
    "text": "Common codes used across notebooks"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n savefig (name, **kwargs)"
  },
  {
    "objectID": "utils.html#plotting",
    "href": "utils.html#plotting",
    "title": "Utils",
    "section": "",
    "text": "source\n\n\n\n savefig (name, **kwargs)"
  },
  {
    "objectID": "utils.html#utilities-functions",
    "href": "utils.html#utilities-functions",
    "title": "Utils",
    "section": "Utilities functions",
    "text": "Utilities functions\n\nsource\n\npmap\n\n pmap (func, *args, **kwargs)\n\nmap with partial\n\nsource\n\n\nget_code\n\n get_code (data, name)"
  },
  {
    "objectID": "utils.html#kedro",
    "href": "utils.html#kedro",
    "title": "Utils",
    "section": "Kedro",
    "text": "Kedro\nStandard import\nfrom kedro.pipeline import Pipeline, node\nfrom kedro.pipeline.modular_pipeline import pipeline\n\ndef create_pipeline(**kwargs) -&gt; Pipeline:\n    return pipeline([\n        ...\n    ])\n\n\nCode\nfrom kedro.config import OmegaConfigLoader\nfrom kedro.io import DataCatalog\n\n\n\n\nCode\ndef load_catalog(conf_source: str = \"../conf\", catalog_source: str = \"catalog\"):\n    # Initialise a ConfigLoader\n    conf_loader = OmegaConfigLoader(conf_source)\n\n    # Load the data catalog configuration from catalog.yml\n    conf_catalog = conf_loader.get(catalog_source)\n\n    # Create the DataCatalog instance from the configuration\n    catalog = DataCatalog.from_config(conf_catalog)\n    \n    return catalog\n\n\nThe following load_catalog provides project-aware access to the catalog. The preceding load_catalog only works when notebook is run from the project root.\n\nsource\n\nload_catalog\n\n load_catalog (project_path:str='../')"
  },
  {
    "objectID": "utils.html#utils",
    "href": "utils.html#utils",
    "title": "Utils",
    "section": "Utils",
    "text": "Utils\n\n\nCode\nfrom fastcore.utils import patch\nfrom speasy.products import SpeasyVariable\nfrom humanize import naturalsize\n\n\n[11/06/23 20:13:04] WARNING  Non compliant ISTP file: Epoch was      _impl.py:77\n                             marked as data variable but it has 0               \n                             support variable                                   \n[11/06/23 20:13:11] WARNING  Non compliant ISTP file: No data       _impl.py:111\n                             variable found, this is suspicious                 \n[11/06/23 20:13:28] WARNING  Non compliant ISTP file: No data       _impl.py:111\n                             variable found, this is suspicious                 \n[11/06/23 20:13:41] WARNING  Non compliant ISTP file: No data       _impl.py:111\n                             variable found, this is suspicious                 \n[11/06/23 20:13:53] WARNING  Non compliant ISTP file: No data       _impl.py:111\n                             variable found, this is suspicious                 \n[11/06/23 20:14:04] WARNING  Non compliant ISTP file: No data       _impl.py:111\n                             variable found, this is suspicious                 \n\n\n\n\nCode\n@patch\ndef preview(self: SpeasyVariable):\n    print(\"===========================================\")\n    print(f\"Name:         {self.name}\")\n    print(f\"Columns:      {self.columns}\")\n    print(f\"Values Unit:  {self.unit}\")\n    print(f\"Memory usage: {naturalsize(self.nbytes)}\")\n    print(f\"Axes Labels:  {self.axes_labels}\")\n    print(\"-------------------------------------------\")\n    print(f\"Meta-data:    {self.meta}\")\n    print(\"-------------------------------------------\")\n    print(f\"Time Axis:    {self.time[:3]}\")\n    print(\"-------------------------------------------\")\n    print(f\"Values:       {self.values[:3]}\")\n    print(\"===========================================\")"
  },
  {
    "objectID": "utils.html#configurations",
    "href": "utils.html#configurations",
    "title": "Utils",
    "section": "Configurations",
    "text": "Configurations\n\nsource\n\nDataConfig\n\n DataConfig (sat_id:str=None, start:datetime.datetime=None,\n             end:datetime.datetime=None, ts:datetime.timedelta=None,\n             coord:str=None)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model."
  },
  {
    "objectID": "utils.html#polars",
    "href": "utils.html#polars",
    "title": "Utils",
    "section": "Polars",
    "text": "Polars\n\nsource\n\nfilter_tranges_df\n\n filter_tranges_df (df:polars.dataframe.frame.DataFrame,\n                    tranges:Tuple[list,list], time_col:str='time')\n\n\nFilter data by time ranges\n\n\nsource\n\n\nfilter_tranges\n\n filter_tranges (time:polars.series.series.Series,\n                 tranges:Tuple[list,list])\n\n\nFilter data by time ranges, return the indices of the time that are in the time ranges (left inclusive, right exclusive)\n\n\nsource\n\n\ncdf2pl\n\n cdf2pl (file_path:str, var_names:Union[str,list[str]])\n\nConvert a CDF file to Polars Dataframe.\nParameters: file_path (str): The path to the CDF file. var_names (Union[str, List[str]]): The name(s) of the variable(s) to retrieve from the CDF file.\nReturns: pl.LazyFrame: A lazy dataframe containing the requested data.\n\n\n\nDataFrame.plot\n\n DataFrame.plot (*args, **kwargs)\n\n\nsource\n\n\npl_norm\n\n pl_norm (columns, *more_columns)\n\nComputes the square root of the sum of squares for the given columns.\nArgs: *columns (str): Names of the columns.\nReturns: pl.Expr: Expression representing the square root of the sum of squares.\n\nPartition the dataset by year\n\nsource\n\n\n\npartition_data_by_year\n\n partition_data_by_year (df:polars.lazyframe.frame.LazyFrame)\n\nPartition the dataset by year\nArgs: df: Input DataFrame.\nReturns: Partitioned DataFrame.\n\nsource\n\n\nconcat_partitions\n\n concat_partitions (partitioned_input:Dict[str,Callable])\n\nConcatenate input partitions into one DataFrame.\nArgs: partitioned_input: A dictionary with partition ids as keys and load functions as values.\n\nsource\n\n\nconcat_df\n\n concat_df (dfs:list[typing.Union[polars.dataframe.frame.DataFrame,polars.\n            lazyframe.frame.LazyFrame,pandas.core.frame.DataFrame]])\n\nConcatenate a list of DataFrames into one DataFrame.\n\nResample data\n\nsource\n\n\n\nresample\n\n resample\n           (df:polars.dataframe.frame.DataFrame|polars.lazyframe.frame.Laz\n           yFrame, every:datetime.timedelta|str|int,\n           period:str|datetime.timedelta=None, time_column='time')\n\nResample the DataFrame\n\nsource\n\n\nformat_timedelta\n\n format_timedelta (time)\n\nFormat timedelta to timedelta\n\nRead lbl file\n\nsource\n\n\n\nLblDataset\n\n LblDataset (filepath:str, load_type:str='table',\n             metadata:Dict[str,Any]=None)\n\nAbstractDataset is the base class for all data set implementations. All data set implementations should extend this abstract class and implement the methods marked as abstract. If a specific dataset implementation cannot be used in conjunction with the ParallelRunner, such user-defined dataset should have the attribute _SINGLE_PROCESS = True. Example: ::\n&gt;&gt;&gt; from pathlib import Path, PurePosixPath\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from kedro.io import AbstractDataset\n&gt;&gt;&gt;\n&gt;&gt;&gt;\n&gt;&gt;&gt; class MyOwnDataset(AbstractDataset[pd.DataFrame, pd.DataFrame]):\n&gt;&gt;&gt;     def __init__(self, filepath, param1, param2=True):\n&gt;&gt;&gt;         self._filepath = PurePosixPath(filepath)\n&gt;&gt;&gt;         self._param1 = param1\n&gt;&gt;&gt;         self._param2 = param2\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _load(self) -&gt; pd.DataFrame:\n&gt;&gt;&gt;         return pd.read_csv(self._filepath)\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _save(self, df: pd.DataFrame) -&gt; None:\n&gt;&gt;&gt;         df.to_csv(str(self._filepath))\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _exists(self) -&gt; bool:\n&gt;&gt;&gt;         return Path(self._filepath.as_posix()).exists()\n&gt;&gt;&gt;\n&gt;&gt;&gt;     def _describe(self):\n&gt;&gt;&gt;         return dict(param1=self._param1, param2=self._param2)\nExample catalog.yml specification: ::\nmy_dataset:\n    type: &lt;path-to-my-own-dataset&gt;.MyOwnDataset\n    filepath: data/01_raw/my_data.csv\n    param1: &lt;param1-value&gt; # param1 is a required argument\n    # param2 will be True by default\n\nsource\n\n\nload_lbl\n\n load_lbl (filepath:str, type:str='table')\n\nLoad LBL data.\nArgs: filepath: File path to load the data from. type: Type of data to load. Options are ‘table’ and ‘index’.\nReturns: A pandas DataFrame containing the loaded data.\n\nsource\n\n\nget_memory_usage\n\n get_memory_usage (data)\n\n\nsource\n\n\ncalc_vec_mag\n\n calc_vec_mag (vec)\n\n\nsource\n\n\njuno_get_state\n\n juno_get_state (df:Union[pandas.core.frame.DataFrame,polars.dataframe.fra\n                 me.DataFrame,polars.lazyframe.frame.LazyFrame])\n\n\nsource\n\n\nsat_get_fgm_from_df\n\n sat_get_fgm_from_df (df:Union[pandas.core.frame.DataFrame,polars.datafram\n                      e.frame.DataFrame,polars.lazyframe.frame.LazyFrame])\n\n\nsource\n\n\ndf2ts\n\n df2ts (df:Union[pandas.core.frame.DataFrame,polars.dataframe.frame.DataFr\n        ame,polars.lazyframe.frame.LazyFrame], cols, attrs=None,\n        name=None)\n\n\nsource\n\n\ncol_renamer\n\n col_renamer (lbl:str)\n\n\nsource\n\n\ncheck_fgm\n\n check_fgm (vec:xarray.core.dataarray.DataArray)\n\n\nsource\n\n\ndownload_file\n\n download_file (url, local_dir='./', file_name=None)\n\nDownload a file from a URL and save it locally.\nReturns: file_path (str): Path to the downloaded file.\n\n\n\ncalc_time_diff\n\n calc_time_diff (*args, **kwargs)\n\nMultiply dispatched method: calc_time_diff\nOther signatures: DataFrame LazyFrame\n\n\n\ncalc_time_diff\n\n calc_time_diff (*args, **kwargs)\n\nMultiply dispatched method: calc_time_diff\nOther signatures: DataFrame LazyFrame"
  },
  {
    "objectID": "utils.html#obsolete-codes",
    "href": "utils.html#obsolete-codes",
    "title": "Utils",
    "section": "Obsolete codes",
    "text": "Obsolete codes\n\n\nCode\nfrom flox.xarray import xarray_reduce\n\n\n\n\nCode\n@dispatch(xr.DataArray, object)\ndef compute_index_std(data: DataArray, tau):\n    \"\"\"\n    Examples\n    --------\n    &gt;&gt;&gt; i1 = index_std(juno_fgm_b, tau)\n    \"\"\"\n\n    # NOTE: large tau values will speed up the computation\n\n    # Resample the data based on the provided time interval.\n    grouped_data = data.resample(time=pandas.Timedelta(tau, unit=\"s\"))\n\n    # Compute the standard deviation for all groups\n    vec_stds = linalg.norm(grouped_data.std(dim=\"time\"), dims=\"v_dim\")\n    # vec_stds = grouped_data.map(calc_vec_std) # NOTE: This is way much slower (like 30x slower)\n\n    offset = pandas.Timedelta(tau / 2, unit=\"s\")\n    vec_stds[\"time\"] = vec_stds[\"time\"] + offset\n\n    vec_stds_next = vec_stds.assign_coords(\n        {\"time\": vec_stds[\"time\"] - pandas.Timedelta(tau, unit=\"s\")}\n    )\n    vec_stds_previous = vec_stds.assign_coords(\n        {\"time\": vec_stds[\"time\"] + pandas.Timedelta(tau, unit=\"s\")}\n    )\n    return np.minimum(vec_stds / vec_stds_next, vec_stds / vec_stds_previous)\n\n\n\n\nCode\ndef index_diff(data: DataArray, tau):\n    grouped_data = data.resample(time=pandas.Timedelta(tau, unit='s'))\n\n    dvecs = grouped_data.first()-grouped_data.last()\n    vec_mean_mags = grouped_data.map(calc_vec_mean_mag)\n    vec_diffs = linalg.norm(dvecs, dims='v_dim') / vec_mean_mags\n    \n    # vec_diffs = grouped_data.map(calc_vec_relative_diff) # NOTE: this is slower than the above implementation.\n    # INFO: Do your spatial and temporal indexing (e.g. .sel() or .isel()) early in the pipeline, especially before calling resample() or groupby(). Grouping and resampling triggers some computation on all the blocks, which in theory should commute with indexing, but this optimization hasn’t been implemented in Dask yet. (See Dask issue #746).\n    \n    offset = pandas.Timedelta(tau/2, unit='s')\n    vec_diffs['time'] = vec_diffs['time'] + offset\n    return vec_diffs\n\n\n# %%\ndef _compute_indices_old(df: pl.DataFrame, tau):\n    \"\"\"\n    Compute all index based on the given DataFrame and tau value.\n\n    Parameters\n    ----------\n    - df (pl.DataFrame): The input DataFrame.\n    - tau (int): The time interval value.\n\n    Returns\n    -------\n    - tuple: Tuple containing DataFrame results for fluctuation index, standard deviation index, and 'index_num'.\n\n    Examples\n    --------\n    &gt;&gt;&gt; indices = compute_indices(df, tau)\n\n    Notes\n    -----\n    Simply shift to calculate index_std would not work correctly if data is missing, like `std_next = pl.col(\"B_std\").shift(-2)`.\n    \"\"\"\n\n    if isinstance(tau, (int, float)):\n        tau = timedelta(seconds=tau)\n\n    b_cols = [\"BX\", \"BY\", \"BZ\"]\n    db_cols = [\"d\" + col_name + \"_vec\" for col_name in b_cols]\n\n    std_next = pl.col(\"B_std\").shift(-2)\n    std_current = pl.col(\"B_std\")\n    std_previous = pl.col(\"B_std\").shift(2)\n    pl_std_index = pl.min_horizontal(\n        [std_current / std_previous, std_current / std_next]\n    )\n\n    # Compute fluctuation index\n    group_df = (\n        df.with_columns(pl_norm(\"BX\", \"BY\", \"BZ\").alias(\"B\"))\n        .group_by_dynamic(\"time\", every=tau / 2, period=tau)\n        .agg(\n            pl.count(),\n            pl.col(b_cols),\n            pl.col(\"BX\").std().alias(\"BX_std\"),\n            pl.col(\"BY\").std().alias(\"BY_std\"),\n            pl.col(\"BZ\").std().alias(\"BZ_std\"),\n            pl.col(\"B\").mean().alias(\"B_mean\"),\n            *pl_dvec(b_cols),\n        )\n        .with_columns(\n            pl_norm(\"BX_std\", \"BY_std\", \"BZ_std\").alias(\"B_std\"),\n            pl_norm(db_cols).alias(\"dB_vec\"),\n        )\n        .drop(\"BX_std\", \"BY_std\", \"BZ_std\")\n    )\n\n    indices = (\n        group_df.with_columns(\n            calc_combined_std(\"BX\"),\n            calc_combined_std(\"BY\"),\n            calc_combined_std(\"BZ\"),\n        )\n        .drop(\"BX\", \"BY\", \"BZ\")\n        .with_columns(\n            pl_norm(\"BX_combined_std\", \"BY_combined_std\", \"BZ_combined_std\").alias(\n                \"B_combined_std\"\n            ),\n            pl.sum_horizontal(\n                pl.col(\"B_std\").shift(-2), pl.col(\"B_std\").shift(2)\n            ).alias(\"B_added_std\"),\n        )\n        .drop(\"BX_combined_std\", \"BY_combined_std\", \"BZ_combined_std\")\n        .with_columns(\n            pl_std_index.alias(\"index_std\"),\n            (pl.col(\"B_combined_std\") / pl.col(\"B_added_std\")).alias(\n                \"index_fluctuation\"\n            ),\n            (pl.col(\"dB_vec\") / pl.col(\"B_mean\")).alias(\"index_diff\"),\n        )\n    )\n    return indices"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Discontinuities? Yes!",
    "section": "",
    "text": "pip install ids-finder"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Discontinuities? Yes!",
    "section": "",
    "text": "pip install ids-finder"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "Discontinuities? Yes!",
    "section": "How to use",
    "text": "How to use\nImport the package\n\n\nCode\nfrom ids_finder.utils.basic import *\nfrom ids_finder.core import *"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Discontinuities? Yes!",
    "section": "Background",
    "text": "Background\n‘Discontinuities’ are discontinuous spatial changes in plasma parameters/characteristics and magnetic fields(Colburn and Sonett 1966).\n\nImportance\n\nContribution of Strong Discontinuities to the Power Spectrum\n\nThe strong discontinuities produce a power-law spectrum in the ‘inertial subrange’ with a spectral index near the Kolmogorov -5/3 index. The discontinuity spectrum contains about half of the power of the full solar-wind magnetic ﬁeld over this ‘inertial subrange’. (Borovsky 2010)\n\n\n\n\nMotivations\nStudying the radial distribution of occurrence rate, as well as the properties of solar wind discontinuities may help answer the following questions:\n\nHow does the discontinuities change with the radial distance from the Sun?\nHow is solar wind discontinuities formed? What is the physical mechanisms?\n\nGenerated at or near the sun?\nLocally generated in the interplanetary space by turbulence?\n\n\nJoint observations of JUNO & ARTEMIS & Other missions really provides a unique opportunity!!!\n\nFive-year cruise to Jupiter from 2011 to 2016\nOne earth flyby in 2013\nNearly the same Heliographic latitude as Earth\n\nTo eliminate the effect of the solar wind structure, we use data from other missions (mainly at 1AU) to provide a way of normalization.\n\n\n\nMission\nr [AU]\n\\(\\delta t_B\\)\n\\(\\delta t_{plasma}\\)\nData availability\n\n\n\n\nJUNO\n1-5.5\n1s averaged (64 Hz)\n1h model *\n2011 - 2016 - Today\n\n\nARTEMIS\n1\n1s averaged (8 Hz)\n1h averaged\n2009 - Today (solar wind)\n\n\nSTEREO-A\n1\n1s averaged (8 Hz)\n1h averaged\n2006 - Today\n\n\nSTEREO-B\n1\n1s averaged (8 Hz)\n1h averaged\n2006 - 2016.09\n\n\nWind\n1\n11 Hz\n1h averaged\n1994 - 2004 -2020 - Today\n\n\nSolar Orbiter\n0.28-0.91\n\n\n2020 - Today\n\n\nUlysis"
  },
  {
    "objectID": "index.html#methods",
    "href": "index.html#methods",
    "title": "Discontinuities? Yes!",
    "section": "Methods",
    "text": "Methods\nTraditional methods for ID identiﬁcation, such as the criteria of\n\nBurlaga & Ness (1969; B-criterion) : a directional change of the magnetic ﬁeld larger than 30° during 60 s\nTsurutani & Smith (1979; TS-criterion) : \\(|ΔB|/|B| \\geq 0.5\\) within 3 minutes\n\nMostly rely on magnetic ﬁeld variations with a certain time lag. B-criterion has, as its main condition.\nIn their methods, the IDs below the thresholds are artiﬁcially abandoned. Therefore, identiﬁcation criteria may affect the statistical results, and there is likely to be a discrepancy between the ﬁndings via B-criterion and TS- criterion.\n\nID identification (limited feature extraction / anomaly detection)\nLiu’s method : The first two conditions guarantee that the field changes of the IDs identiﬁed are large enough to be distinguished from the stochastic fluctuations on magnetic fields, while the third is a supplementary condition to reduce the uncertainty of recognition.\n\\[ \\textrm{Index}_1 = \\frac{\\sigma(\\vec{B})}{Max(\\sigma(\\vec{B}_-),\\sigma(\\vec{B}_+))} \\]\n\\[ \\textrm{Index}_2 = \\frac{\\sigma(\\vec{B}_- + \\vec{B}_+)} {\\sigma(\\vec{B}_-) + \\sigma(\\vec{B}_+)} \\]\n\\[ \\textrm{Index}_3 = \\frac{| \\Delta \\vec{B} |}{|B_{bg}|} \\]\n\\[ \\textrm{Index}_1 \\ge 2, \\textrm{Index}_2 \\ge 1, \\textrm{Index}_3 \\ge 0.1 \\]\n\n\nSolar Wind Model\nSadly, JUNO does not provide plasma data during the cruise phase, so to estimate the plasma state we will use MHD model.\nWe are using Michigan Solar WInd Model 2D (MSWIM2D), which models the solar wind propagation in 2D using the BATSRUS MHD solver. (Keebler et al. 2022)\nSome key points about the model\n\nRepresenting the solar wind in the ecliptic plane from 1 to 75 AU\n2D MHD model, using the BATSRUS MHD solver\nInclusion of neutral hydrogen (important for the outer heliosphere)\nInner boundary is filled by time-shifting in situ data from multiple spacecraft\n\nFor model validation part, please see JUNO Model Report."
  },
  {
    "objectID": "index.html#conclusion",
    "href": "index.html#conclusion",
    "title": "Discontinuities? Yes!",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe have collected 5 years of solar wind discontinuities from JUNO, ARTEMIS and STEREO.\nWe have developed a pipeline to identify solar wind discontinuities. (Modular, Performant, Scalable)\nThe normalized occurrence rate of IDs drops with the radial distance from the Sun, following \\(1/r\\) law.\nThe thickness of IDs increases with the radial distance from the Sun, but after normalization to ion inertial length, the thickness of IDs decreases.\nThe current intensity of IDs decrease with the radial distance from the Sun, but after normalization to the Alfven current , the current intensity of IDs increases."
  },
  {
    "objectID": "index.html#conventions",
    "href": "index.html#conventions",
    "title": "Discontinuities? Yes!",
    "section": "Conventions",
    "text": "Conventions\nAs we are dealing with multiple spacecraft, we need to be careful about naming conventions. Here are the conventions we use in this project.\n\nsat_id: name of the spacecraft. We also use abbreviation, for example\n\nsta for STEREO-A\nthb for ARTEMIS-B\n\nsat_state: state data of the spacecraft\nb_vl: maximum variance vector of the magnetic field, (major eigenvector)\n\nData Level\n\nl0: unprocessed\nl1: cleaned data, fill null value, add useful columns\nl2: time-averaged data\n\n\nColumns naming conventions\n\nradial_distance: radial distance of the spacecraft, in units of \\(AU\\)\nplasma_speed: solar wind plasma speed, in units of \\(km/s\\)\nsw_elevation: solar wind elevation angle, in units of \\(\\degree\\)\nsw_azimuth: solar wind azimuth angle, in units of \\(\\degree\\)\nv_{x,y,z} or sw_vel_{X,Y,Z}: solar wind plasma speed in the ANY coordinate system, in units of \\(km/s\\)\n\nsw_vel_{r,t,n}: solar wind plasma speed in the RTN coordinate system, in units of \\(km/s\\)\nsw_vel_gse_{x,y,z}: solar wind plasma speed in the GSE coordinate system, in units of \\(km/s\\)\nsw_vel_lmn_{x,y,z}: solar wind plasma speed in the LMN coordinate system, in units of \\(km/s\\)\n\nv_l or sw_vel_l: abbreviation for sw_vel_lmn_1\nv_mn or sw_vel_mn (deprecated)\n\n\nplasma_density: plasma density, in units of \\(1/cm^{3}\\)\nplasma_temperature: plasma temperature, in units of \\(K\\)\nB_{x,y,z}: magnetic field in ANY coordinate system\n\nb_rtn_{x,y,z} or b_{r,t,n}: magnetic field in the RTN coordinate system\nb_gse_{x,y,z}: magnetic field in the GSE coordinate system\n\nB_mag: magnetic field magnitude\nVl_{x,y,z} or b_vecL_{X,Y,Z}: maxium variance vector of the magnetic field in ANY coordinate system\n\nb_vecL_{r,t,n}: maxium variance vector of the magnetic field in the RTN coordinate system\n\nmodel_b_{r,t,n}: modelled magnetic field in the RTN coordinate system\nstate : 1 for solar wind, 0 for non-solar wind\nL_mn{_norm}: thickness of the current sheet in MN direction, in units of \\(km\\)\nj0{_norm}: current density, in units of \\(nA/m^2\\)\n\nNotes: we recommend use unique names for each variable, for example, plasma_speed instead of speed. Because it is easier to search and replace the variable names in the code whenever necessary.\nFor the unit, by default we use\n\nlength : \\(km\\)\ntime : \\(s\\)\nmagnetic field : \\(nT\\)\ncurrent : \\(nA/m^2\\)"
  },
  {
    "objectID": "index.html#todos",
    "href": "index.html#todos",
    "title": "Discontinuities? Yes!",
    "section": "TODOs",
    "text": "TODOs\nScience part\n\nAnalysis\n\nCheck STEREO-A and ARTEMIS-B data\nContribution of discontinuities to the power spectrum\nCheck Datagap\nCheck ARTEMIS-B data in different states (solar wind, magnetosheath, magnetotail, moon wake)\nDistribution of |B| over radius\nJUNO from 2012-09~2012-10 lack of IDS and extreme large thickness\nWind data\nAdd error bar\nValidate the effects of calibrate candidate duration\nValidate model density with Voyager\n\nIdentifaction\n\nEnsemble forest?\nSmoothing is important?\nCheck change point algorithm\n\nVisualize data gaps\nFeatures\n\nThickness in N direction\nUse high resolution data for feature extraction\n\nCompare with other methods of identifying IDs\n\nVerify with other methods of identifying IDs\n\nIncorporate solar wind propagation model\n\nVerify with solar wind propagation model\n\nCoordinate transformation\n\n\n\nCode part\n\nOptimization\n\nJAX library for numpy optimization\nshorten import time\n\nRefactor\n\nprocess_candidates to exclude sat_state logics\nrenaming feature layer candidates\n\nKedro\n\nModular pipelines\nIncorporate lineapy\n\nQR codes\n\n\n\nCode\n%%markdown\npython -X importtime -c 'from ids_finder.pipelines.juno.pipeline import download_juno_data, preprocess_jno' 2&gt; import.log && tuna import.log\n\npython -X importtime -c 'import ids_finder.utils.basic' 2&gt; import.log && tuna import.log\n\n\n\nbugs\n\nJUNO sw_temperature type\nSTEREO B less than zero (after downsampling?)"
  },
  {
    "objectID": "missions/stereo.html",
    "href": "missions/stereo.html",
    "title": "IDs from STEREO",
    "section": "",
    "text": "Need to run command in shell first as pipeline is project-specific command\nkedro pipeline create stearo\nTo get STEREO-A state data, run kedro run --to-outputs=sta.primary_state_rtn_1h\nTo get candidates data, run kedro run --from-inputs=sta.feature_1s --to-outputs=candidates.sta_1s\n\n\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\ncatalog = load_catalog()\n\n\n[10/24/23 14:51:06] WARNING  KedroDeprecationWarning: 'PartitionedDataset' has been moved to        warnings.py:109\n                             `kedro-datasets` and will be removed in Kedro 0.19.0.                                 \n                                                                                                                   \n\n\n\n                    WARNING  KedroDeprecationWarning: 'PartitionedDataset' has been moved to        warnings.py:109\n                             `kedro-datasets` and will be removed in Kedro 0.19.0.                                 \n                                                                                                                   \n\n\n\n                    WARNING  KedroDeprecationWarning: 'AbstractVersionedDataSet' has been renamed   warnings.py:109\n                             to 'AbstractVersionedDataset', and the alias will be removed in Kedro                 \n                             0.19.0"
  },
  {
    "objectID": "missions/stereo.html#setup",
    "href": "missions/stereo.html#setup",
    "title": "IDs from STEREO",
    "section": "",
    "text": "Need to run command in shell first as pipeline is project-specific command\nkedro pipeline create stearo\nTo get STEREO-A state data, run kedro run --to-outputs=sta.primary_state_rtn_1h\nTo get candidates data, run kedro run --from-inputs=sta.feature_1s --to-outputs=candidates.sta_1s\n\n\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\ncatalog = load_catalog()\n\n\n[10/24/23 14:51:06] WARNING  KedroDeprecationWarning: 'PartitionedDataset' has been moved to        warnings.py:109\n                             `kedro-datasets` and will be removed in Kedro 0.19.0.                                 \n                                                                                                                   \n\n\n\n                    WARNING  KedroDeprecationWarning: 'PartitionedDataset' has been moved to        warnings.py:109\n                             `kedro-datasets` and will be removed in Kedro 0.19.0.                                 \n                                                                                                                   \n\n\n\n                    WARNING  KedroDeprecationWarning: 'AbstractVersionedDataSet' has been renamed   warnings.py:109\n                             to 'AbstractVersionedDataset', and the alias will be removed in Kedro                 \n                             0.19.0"
  },
  {
    "objectID": "missions/stereo.html#magnetic-field-data-pipeline",
    "href": "missions/stereo.html#magnetic-field-data-pipeline",
    "title": "IDs from STEREO",
    "section": "Magnetic field data pipeline",
    "text": "Magnetic field data pipeline\nSTEREO magnetic field is already in RTN coordinates, so no need to transform it.\nDownload data using pyspedas, but load it using pycdfpp (using pyspedas to load the data directly into xarray is very slow)\n\nDownloading data\n\nsource\n\n\ndownload_mag_data\n\n download_mag_data (start:str=None, end:str=None, probe:str='a')\n\n\n\nPreprocessing data\n\nsource\n\n\npreprocess_mag_data\n\n preprocess_mag_data (raw_data:Iterable[str]=None, ts:str='1s')\n\nPreprocess the raw dataset (only minor transformations)\n\nDownsample the data to a given time resolution\nApplying naming conventions for columns\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nIterable\nNone\nList of CDF files\n\n\nts\nstr\n1s\ntime resolution\n\n\nReturns\nDataFrame\n\n\n\n\n\n\n\nProcessing data\n\nsource\n\n\nprocess_mag_data\n\n process_mag_data (raw_data:polars.dataframe.frame.DataFrame, ts:str=None,\n                   coord:str=None)\n\nCorresponding to primary data layer, where source data models are transformed into domain data models\n\nPartitioning data, for the sake of memory\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nDataFrame\n\n\n\n\nts\nstr\nNone\ntime resolution\n\n\ncoord\nstr\nNone\n\n\n\nReturns\nDict\n\n\n\n\n\n\n\nPipeline\n\nsource\n\n\ncreate_mag_data_pipeline\n\n create_mag_data_pipeline (sat_id, ts:str='1s', tau:str='60s', **kwargs)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\n\n\n\n\n\nts\nstr\n1s\ntime resolution,\n\n\ntau\nstr\n60s\ntime window\n\n\nkwargs\n\n\n\n\n\nReturns\nPipeline"
  },
  {
    "objectID": "missions/stereo.html#state-data-pipeline",
    "href": "missions/stereo.html#state-data-pipeline",
    "title": "IDs from STEREO",
    "section": "State data pipeline",
    "text": "State data pipeline\nFor STEREO’s mission, We use 1-hour averaged merged data from COHOWeb.\nSee STEREO ASCII merged data and one sample file [here](https://spdf.gsfc.nasa.gov/pub/data/stereo/ahead/l2/merged/stereoa2011.asc\nPlasma in RTN (Radial-Tangential-Normal) coordinate system - Proton Flow Speed, km/sec - Proton Flow Elevation Angle/Latitude, deg. - Proton Flow Azimuth Angle/Longitude, deg. - Proton Density, n/cc - Proton Temperature, K)\nNotes - Note1: There is a big gap 2014/12/16 - 2015/07/20 in plasma data - Note2: There is a big gap 2015/03/21 - 2015/07/09 and 2015/10/27 - 2015/11/15 in mag data - Note that for missing data, fill values consisting of a blank followed by 9’s which together constitute the format are used\n\nLoading data\n\nsource\n\n\ndownload_state_data\n\n download_state_data (start:str=None, end:str=None)\n\n\nsource\n\n\nload_state_data\n\n load_state_data (start:str=None, end:str=None)\n\n\nDownloading data\nReading data into a proper data structure, like dataframe.\n\nParsing original data (dealing with delimiters, missing values, etc.)\n\n\n\n\nPreprocessing data\n\nsource\n\n\npreprocess_state_data\n\n preprocess_state_data (raw_data:polars.dataframe.frame.DataFrame,\n                        ts:str=None, coord:str=None)\n\nPreprocess the raw dataset (only minor transformations)\n\nParsing and typing data (like from string to datetime for time columns)\nChanging storing format (like from csv to parquet)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nDataFrame\n\n\n\n\nts\nstr\nNone\ntime resolution\n\n\ncoord\nstr\nNone\n\n\n\nReturns\nDataFrame\n\n\n\n\n\n\nProcesss state data\n\nsource\n\n\n\nprocess_state_data\n\n process_state_data (df:polars.dataframe.frame.DataFrame,\n                     columns:list[str]=['Radial Distance, AU', 'HGI Lat.\n                     of the S/C', 'HGI Long. of the S/C', 'SW Plasma\n                     Speed, km/s', 'SW Lat. Angle RTN, deg.', 'SW Long.\n                     Angle RTN, deg.', 'SW Plasma Density, N/cm^3', 'SW\n                     Plasma Temperature, K'])\n\nCorresponding to primary data layer, where source data models are transformed into domain data models\n\nApplying naming conventions for columns\nTransforming data to RTN (Radial-Tangential-Normal) coordinate system\nDiscarding unnecessary columns\n\n\nsource\n\n\nconvert_state_to_rtn\n\n convert_state_to_rtn (df:polars.dataframe.frame.DataFrame)\n\nConvert state data to RTN coordinates\n\n\nPipelines\n\nsource\n\n\ncreate_state_data_pipeline\n\n create_state_data_pipeline (sat_id='sta', ts:str='1h', **kwargs)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\nstr\nsta\n\n\n\nts\nstr\n1h\ntime resolution\n\n\nkwargs\n\n\n\n\n\nReturns\nPipeline"
  },
  {
    "objectID": "missions/stereo.html#pipelines-1",
    "href": "missions/stereo.html#pipelines-1",
    "title": "IDs from STEREO",
    "section": "Pipelines",
    "text": "Pipelines\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='sta', tau='60s', ts_mag='1s', ts_state='1h')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\nstr\nsta\n\n\n\ntau\nstr\n60s\n\n\n\nts_mag\nstr\n1s\ntime resolution of magnetic field data\n\n\nts_state\nstr\n1h\ntime resolution of state data\n\n\nReturns\nPipeline"
  },
  {
    "objectID": "missions/stereo.html#obsolete-codes",
    "href": "missions/stereo.html#obsolete-codes",
    "title": "IDs from STEREO",
    "section": "Obsolete codes",
    "text": "Obsolete codes\nNOTE: one can also use speasy to download data, however this is slower for STEREO data.\n\n\nCode\nsat_fgm_product = cda_tree.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.BFIELD\nsat_fgm_product = 'cda/STA_L1_MAG_RTN/BFIELD'\nproducts = [sat_fgm_product]\n\ndataset = spz.get_data(products, test_trange, disable_proxy=True)\nsat_fgm_data  = dataset[0]\ndata_preview(sat_fgm_data)\n\n\nDownload data in a background thread\n\n\nCode\n@threaded\ndef download_data(products, trange):\n    logger.info(\"Downloading data\")\n    spz.get_data(products, trange, disable_proxy=True)\n    logger.info(\"Data downloaded\")\n    \ndownload_data(products, trange)\n\n\n\n\nCode\nimport speasy as spz\n\n\n06-Nov-23 20:13:09 INFO     06-Nov-23 20:13:09: Using pycdfpp     __init__.py:16\n06-Nov-23 20:13:28 WARNING  06-Nov-23 20:13:28: Non compliant ISTP  _impl.py:111\n                            file: No data variable found, this is               \n                            suspicious                                          \n06-Nov-23 20:14:04 WARNING  06-Nov-23 20:14:04: Non compliant ISTP  _impl.py:111\n                            file: No data variable found, this is               \n                            suspicious                                          \n\n\n\n\nCode\ncda_tree: spz.SpeasyIndex = spz.inventories.tree.cda\nproduct = cda_tree.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN\n\nlogger.info(product.description)\nlogger.info(product.ID)\nlogger.info(product.BFIELD.CATDESC)\nlogger.info(product.BFIELD.spz_uid())\n\n# spz.inventories.data_tree.cda.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.\n# spz.inventories.data_tree.cda.STEREO.STEREOA.IMPACT_MAG.STA_LB_MAG_RTN.description\n# spz.inventories.data_tree.cda.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.MAGFLAGUC.CATDESC\nspz.inventories.data_tree.cda.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.BFIELD.CATDESC\n# spz.inventories.data_tree.cda.STEREO.Ahead.IMPACT_MAG.STA_L1_MAG_RTN.BFIELD."
  },
  {
    "objectID": "missions/index.html",
    "href": "missions/index.html",
    "title": "Missions",
    "section": "",
    "text": "We try to employ our method and find IDs from different missions."
  },
  {
    "objectID": "analysis/properties.html",
    "href": "analysis/properties.html",
    "title": "SWD Properties",
    "section": "",
    "text": "Code\nimport polars as pl\nimport polars.selectors as cs\nimport pandas as pd\nimport numpy as np\n\nfrom loguru import logger\n\n\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\ncatalog = load_catalog('../../')\n\n\nConnect python with R kernel\n\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n06-Nov-23 08:41:17 WARNING  06-Nov-23 08:41:17: R[write to console]:                               callbacks.py:124\n                            Attaching package: ‘dplyr’                                                             \n                                                                                                                   \n                                                                                                                   \n\n\n\n                   WARNING  06-Nov-23 08:41:17: R[write to console]: The following objects are     callbacks.py:124\n                            masked from ‘package:stats’:                                                           \n                                                                                                                   \n                                filter, lag                                                                        \n                                                                                                                   \n                                                                                                                   \n\n\n\n                   WARNING  06-Nov-23 08:41:17: R[write to console]: The following objects are     callbacks.py:124\n                            masked from ‘package:base’:                                                            \n                                                                                                                   \n                                intersect, setdiff, setequal, union                                                \n                                                                                                                   \n                                                                                                                   \n\n\n\n\n\n\n\nCode\nfrom ids_finder.candidates import cIDsDataset\n\nsta_dataset = cIDsDataset(sat_id=\"STA\", tau=60, ts=1, catalog=catalog)\njno_dataset = cIDsDataset(sat_id=\"JNO\", tau=60, ts=1, catalog=catalog)\nthb_dataset = cIDsDataset(sat_id=\"THB\", tau=60, ts=1, catalog=catalog)\n\n\n06-Nov-23 08:33:48 INFO     06-Nov-23 08:33:48: Loading data from                               data_catalog.py:502\n                            'candidates.STA_ts_1s_tau_60s' (LazyPolarsDataset)...                                  \n\n\n\n                   WARNING  06-Nov-23 08:33:48:                                                       logger.py:205\n                            /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ked              \n                            ro/io/partitioned_dataset.py:200: KedroDeprecationWarning:                             \n                            'PartitionedDataset' has been moved to `kedro-datasets` and will be                    \n                            removed in Kedro 0.19.0.                                                               \n                              warnings.warn(                                                                       \n                                                                                                                   \n\n\n\n                   INFO     06-Nov-23 08:33:48: Loading data from 'STA.primary_mag_ts_1s'       data_catalog.py:502\n                            (PartitionedDataset)...                                                                \n\n\n\n                   INFO     06-Nov-23 08:33:48: Loading data from                               data_catalog.py:502\n                            'candidates.JNO_ts_1s_tau_60s' (LazyPolarsDataset)...                                  \n\n\n\n                   INFO     06-Nov-23 08:33:48: Loading data from 'JNO.primary_mag_ts_1s'       data_catalog.py:502\n                            (PartitionedDataset)...                                                                \n\n\n\n                   INFO     06-Nov-23 08:33:48: Loading data from                               data_catalog.py:502\n                            'candidates.THB_ts_1s_tau_60s' (LazyPolarsDataset)...                                  \n\n\n\n                   INFO     06-Nov-23 08:33:48: Loading data from 'THB.primary_mag_ts_1s'       data_catalog.py:502\n                            (PartitionedDataset)...                                                                \n\n\n\n\n\nCode\nfrom beforerr.basics import pmap\nfrom ids_finder.utils.analysis import filter_tranges_ds\n\n\n\n\nCode\nthb_inter_state_sw: pl.LazyFrame = catalog.load('thb.inter_state_sw')\nstart, end = thb_inter_state_sw.select(['start', 'end']).collect()\n\nthb_sw_dataset = filter_tranges_ds(thb_dataset, (start, end))\n\n\n                   INFO     06-Nov-23 08:33:48: Loading data from 'thb.inter_state_sw'          data_catalog.py:502\n                            (LazyPolarsDataset)...                                                                 \n\n\n\n\n\nCode\nall_datasets = [sta_dataset, jno_dataset, thb_sw_dataset]\n\n\n\n\nCode\nall_candidates_l0 : pl.DataFrame = pl.concat(\n    all_datasets | pmap(lambda x: x.candidates),\n    how=\"diagonal\",\n)"
  },
  {
    "objectID": "analysis/properties.html#setup",
    "href": "analysis/properties.html#setup",
    "title": "SWD Properties",
    "section": "",
    "text": "Code\nimport polars as pl\nimport polars.selectors as cs\nimport pandas as pd\nimport numpy as np\n\nfrom loguru import logger\n\n\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\ncatalog = load_catalog('../../')\n\n\nConnect python with R kernel\n\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n06-Nov-23 08:41:17 WARNING  06-Nov-23 08:41:17: R[write to console]:                               callbacks.py:124\n                            Attaching package: ‘dplyr’                                                             \n                                                                                                                   \n                                                                                                                   \n\n\n\n                   WARNING  06-Nov-23 08:41:17: R[write to console]: The following objects are     callbacks.py:124\n                            masked from ‘package:stats’:                                                           \n                                                                                                                   \n                                filter, lag                                                                        \n                                                                                                                   \n                                                                                                                   \n\n\n\n                   WARNING  06-Nov-23 08:41:17: R[write to console]: The following objects are     callbacks.py:124\n                            masked from ‘package:base’:                                                            \n                                                                                                                   \n                                intersect, setdiff, setequal, union                                                \n                                                                                                                   \n                                                                                                                   \n\n\n\n\n\n\n\nCode\nfrom ids_finder.candidates import cIDsDataset\n\nsta_dataset = cIDsDataset(sat_id=\"STA\", tau=60, ts=1, catalog=catalog)\njno_dataset = cIDsDataset(sat_id=\"JNO\", tau=60, ts=1, catalog=catalog)\nthb_dataset = cIDsDataset(sat_id=\"THB\", tau=60, ts=1, catalog=catalog)\n\n\n06-Nov-23 08:33:48 INFO     06-Nov-23 08:33:48: Loading data from                               data_catalog.py:502\n                            'candidates.STA_ts_1s_tau_60s' (LazyPolarsDataset)...                                  \n\n\n\n                   WARNING  06-Nov-23 08:33:48:                                                       logger.py:205\n                            /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ked              \n                            ro/io/partitioned_dataset.py:200: KedroDeprecationWarning:                             \n                            'PartitionedDataset' has been moved to `kedro-datasets` and will be                    \n                            removed in Kedro 0.19.0.                                                               \n                              warnings.warn(                                                                       \n                                                                                                                   \n\n\n\n                   INFO     06-Nov-23 08:33:48: Loading data from 'STA.primary_mag_ts_1s'       data_catalog.py:502\n                            (PartitionedDataset)...                                                                \n\n\n\n                   INFO     06-Nov-23 08:33:48: Loading data from                               data_catalog.py:502\n                            'candidates.JNO_ts_1s_tau_60s' (LazyPolarsDataset)...                                  \n\n\n\n                   INFO     06-Nov-23 08:33:48: Loading data from 'JNO.primary_mag_ts_1s'       data_catalog.py:502\n                            (PartitionedDataset)...                                                                \n\n\n\n                   INFO     06-Nov-23 08:33:48: Loading data from                               data_catalog.py:502\n                            'candidates.THB_ts_1s_tau_60s' (LazyPolarsDataset)...                                  \n\n\n\n                   INFO     06-Nov-23 08:33:48: Loading data from 'THB.primary_mag_ts_1s'       data_catalog.py:502\n                            (PartitionedDataset)...                                                                \n\n\n\n\n\nCode\nfrom beforerr.basics import pmap\nfrom ids_finder.utils.analysis import filter_tranges_ds\n\n\n\n\nCode\nthb_inter_state_sw: pl.LazyFrame = catalog.load('thb.inter_state_sw')\nstart, end = thb_inter_state_sw.select(['start', 'end']).collect()\n\nthb_sw_dataset = filter_tranges_ds(thb_dataset, (start, end))\n\n\n                   INFO     06-Nov-23 08:33:48: Loading data from 'thb.inter_state_sw'          data_catalog.py:502\n                            (LazyPolarsDataset)...                                                                 \n\n\n\n\n\nCode\nall_datasets = [sta_dataset, jno_dataset, thb_sw_dataset]\n\n\n\n\nCode\nall_candidates_l0 : pl.DataFrame = pl.concat(\n    all_datasets | pmap(lambda x: x.candidates),\n    how=\"diagonal\",\n)"
  },
  {
    "objectID": "analysis/properties.html#processing-datasets",
    "href": "analysis/properties.html#processing-datasets",
    "title": "SWD Properties",
    "section": "Processing datasets",
    "text": "Processing datasets\nSome extreme values are present in the data. We will remove them.\n\n\nCode\nNVARS = ['d_star', 'L_mn', 'L_mn_norm', 'j0', 'j0_norm', 'duration', 'v_mn']\nDISPLAY_VARS = ['time', 'sat'] + NVARS\n\n\ndef check_candidates(df):\n    return df[NVARS].describe()\n\ncheck_candidates(all_candidates_l0)\n\n\n\n\n\n\nshape: (9, 8)\n\n\n\ndescribe\nd_star\nL_mn\nL_mn_norm\nj0\nj0_norm\nduration\nv_mn\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"count\"\n185066.0\n185066.0\n185066.0\n185066.0\n185066.0\n\"185066\"\n185066.0\n\n\n\"null_count\"\n0.0\n4120.0\n4389.0\n4120.0\n4389.0\n\"0\"\n4120.0\n\n\n\"mean\"\n2.611712\n2798.843381\n22.307474\n11.654787\n4.713652\n\"0:00:08.198437…\n343.811034\n\n\n\"std\"\n491.756741\n2179.474212\n20.649185\n2894.040891\n1473.838227\nnull\n99.930132\n\n\n\"min\"\n0.019601\n3.381065\n0.014144\n0.0561\n0.00082\n\"0:00:01.999999…\n0.41411\n\n\n\"25%\"\n0.247087\n1582.102536\n11.284664\n0.601477\n0.028203\n\"0:00:05\"\n286.126017\n\n\n\"50%\"\n0.510951\n2240.279834\n17.513617\n1.239019\n0.051221\n\"0:00:07\"\n343.325961\n\n\n\"75%\"\n0.983944\n3346.020528\n27.236719\n2.34897\n0.091488\n\"0:00:10\"\n402.282733\n\n\n\"max\"\n152023.367594\n103745.212024\n1614.132093\n1.1500e6\n583059.205803\n\"0:03:16\"\n864.604665\n\n\n\n\n\n\n\n\nCode\nfrom datetime import timedelta\ndef process_candidates_l1(raw_df: pl.DataFrame):\n    \"clean data to remove extreme values\"\n\n    df = raw_df.filter(\n        pl.col(\"d_star\") &lt; 100, # exclude JUNO extreme values\n        pl.col('v_mn') &gt; 10,\n        pl.col('duration') &lt; timedelta(seconds=60),\n        # pl.col(\"j0\") &lt; 100\n    ).with_columns(\n        pl.col('radial_distance').fill_null(1) # by default, fill with 1 AU\n    ).with_columns(\n        r_bin = pl.col('radial_distance').round(),\n        j0_norm_log = pl.col('j0_norm').log10(),\n        L_mn_norm_log = pl.col('L_mn_norm').log10(),\n    )\n\n    logger.info(\n        f\"candidates_l1: {len(df)}, with effective ratio: {len(df) / len(raw_df):.2%}\"\n    )\n\n    return df\n\nall_candidates_l1 = process_candidates_l1(all_candidates_l0)\n\ncheck_candidates(all_candidates_l1)\n\n\n2023-11-06 09:25:13.643 | INFO     | __main__:process_candidates_l1:18 - candidates_l1: 180718, with effective ratio: 97.65%\n\n\n\n\n\n\nshape: (9, 8)\n\n\n\ndescribe\nd_star\nL_mn\nL_mn_norm\nj0\nj0_norm\nduration\nv_mn\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"count\"\n180718.0\n180718.0\n180718.0\n180718.0\n180718.0\n\"180718\"\n180718.0\n\n\n\"null_count\"\n0.0\n0.0\n264.0\n0.0\n264.0\n\"0\"\n0.0\n\n\n\"mean\"\n0.745751\n2768.506268\n22.033678\n1.865352\n0.075518\n\"0:00:08.118150…\n343.880697\n\n\n\"std\"\n0.771981\n1909.065522\n17.629565\n2.599027\n0.097857\nnull\n99.846681\n\n\n\"min\"\n0.019601\n48.94197\n0.124168\n0.0561\n0.00082\n\"0:00:01.999999…\n10.240242\n\n\n\"25%\"\n0.243875\n1581.58393\n11.279769\n0.60174\n0.028229\n\"0:00:05\"\n286.190021\n\n\n\"50%\"\n0.50421\n2238.553736\n17.499906\n1.239576\n0.051251\n\"0:00:07\"\n343.360031\n\n\n\"75%\"\n0.97075\n3340.552536\n27.186555\n2.348456\n0.091501\n\"0:00:10\"\n402.301723\n\n\n\"max\"\n13.805873\n35975.767016\n439.323024\n393.479096\n9.634978\n\"0:00:59\"\n864.604665\n\n\n\n\n\n\n\n\nCode\njno_candidates_l1 = all_candidates_l1.filter(pl.col('sat') == 'JNO')\n\n\n\n\nCode\nfrom ids_finder.utils.analysis import filter_before_jupiter\nfrom ids_finder.utils.analysis import link_coord2dim\n\n\n\n\nCode\ndef process_candidates_l2(raw_df: pl.DataFrame, avg_window=\"30d\"):\n    time_col = \"time\"\n\n    candidate = (\n        raw_df.sort(time_col)\n        .group_by_dynamic(time_col, every=avg_window, by=\"sat\")\n        .agg(cs.numeric().mean(), cs.duration().mean(), id_count=pl.count())\n        .filter(pl.col(\"id_count\") &gt; 50)  # filter out JUNO extreme large thickness\n        .sort(time_col)\n        .upsample(time_col, every=avg_window, by=\"sat\", maintain_order=True)\n        .with_columns(pl.col(\"sat\").forward_fill())\n    )\n    return candidate\n\n\n\n\nCode\nall_candidates_l2: pl.DataFrame = (\n    all_candidates_l1.pipe(filter_before_jupiter)\n    .pipe(process_candidates_l2)\n    .pipe(link_coord2dim)\n)\n\n\n\n\nCode\ninspect_df = all_candidates_l2[NVARS]\ninspect_df.describe()\n\n\n\n\n\n\nshape: (9, 8)\n\n\n\ndescribe\nd_star\nL_mn\nL_mn_norm\nj0\nj0_norm\nduration\nv_mn\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nstr\nf64\n\n\n\n\n\"count\"\n172.0\n172.0\n172.0\n172.0\n172.0\n\"172\"\n172.0\n\n\n\"null_count\"\n19.0\n19.0\n19.0\n19.0\n19.0\n\"19\"\n19.0\n\n\n\"mean\"\n0.706261\n2922.959632\n22.028999\n1.937378\n0.090728\n\"0:00:08.719631…\n337.428018\n\n\n\"std\"\n0.358616\n512.032439\n8.140589\n1.077249\n0.051647\nnull\n37.917741\n\n\n\"min\"\n0.108318\n1877.983131\n7.074407\n0.229362\n0.042024\n\"0:00:06.751012…\n256.771354\n\n\n\"25%\"\n0.331532\n2590.280777\n14.498058\n0.795284\n0.060267\n\"0:00:07.707419…\n315.324913\n\n\n\"50%\"\n0.794667\n2786.745403\n22.804505\n2.087583\n0.069789\n\"0:00:08.730158…\n335.332916\n\n\n\"75%\"\n0.931735\n3182.843841\n27.726721\n2.633037\n0.094061\n\"0:00:09.315238…\n359.837854\n\n\n\"max\"\n1.539393\n4458.507484\n41.436617\n4.784021\n0.306938\n\"0:00:12.305699…\n445.849288\n\n\n\n\n\n\n\n\nCode\nfrom ids_finder.utils.analysis import n2_normalize\n\nall_candidates_l2_n2 = n2_normalize(all_candidates_l2, NVARS)"
  },
  {
    "objectID": "analysis/properties.html#plotting-function",
    "href": "analysis/properties.html#plotting-function",
    "title": "SWD Properties",
    "section": "Plotting function",
    "text": "Plotting function\nPlotting function for Level 1 data.\nSimilar to the geom_bin2d function, but with added functionality\n\nNormalize the data to every x-axis value\nAdd peak values\nAdd mean values with error bars\n\n\n\nCode\nlibrary(scales)\n# Helper function to calculate summary statistics for x-binned data\ncalculate_summary &lt;- function(data, x_col, y_col, x_seq) {\n  data %&gt;%\n    mutate(!!x_col := x_seq[findInterval(data[[x_col]], x_seq, rightmost.closed = TRUE)]) %&gt;%\n    group_by(!!sym(x_col)) %&gt;%\n    summarise(\n      mean_y = mean(!!sym(y_col), na.rm = TRUE),\n      sd_y = sd(!!sym(y_col), na.rm = TRUE),\n      se_y = sd_y / sqrt(n())\n    )\n}\n\n\nplot_binned_data &lt;- function(data, x_col, y_col, x_bins, y_bins, y_lim=NULL, log_y=FALSE) {\n  \n  # If y_lim is provided, filter the data\n  if (!is.null(y_lim)) {\n    data &lt;- data %&gt;%\n      filter(!!sym(y_col) &gt;= y_lim[1], !!sym(y_col) &lt;= y_lim[2])\n  }\n  \n  # If transform_log_y is TRUE, transform y_col to log scale\n  if (log_y) {\n    data[[y_col]] &lt;- log10(data[[y_col]])\n    y_label &lt;- paste(\"Log10\", y_col)\n  } else {\n    y_label &lt;- y_col\n  }\n  \n  # Define bins for x and y based on the input parameters\n  x_seq &lt;- seq(min(data[[x_col]]), max(data[[x_col]]), length.out = x_bins + 1)\n  y_seq &lt;- seq(min(data[[y_col]]), max(data[[y_col]]), length.out = y_bins + 1)\n  \n  data_binned_normalized &lt;- data %&gt;%\n    mutate(\n      !!x_col := x_seq[findInterval(data[[x_col]], x_seq, rightmost.closed = TRUE,)],\n      !!y_col := y_seq[findInterval(data[[y_col]], y_seq, rightmost.closed = TRUE,)]\n    ) %&gt;%\n    count(!!sym(x_col), !!sym(y_col)) %&gt;%\n    group_by(!!sym(x_col)) %&gt;%\n    mutate(n = n/sum(n))\n\n  plot &lt;- ggplot() +\n    geom_tile(data = data_binned_normalized, aes(x = !!sym(x_col), y = !!sym(y_col), fill = n))\n\n  # Calculate mode for each x-bin\n  modes &lt;- data_binned_normalized %&gt;%\n    group_by(!!sym(x_col)) %&gt;%\n    slice_max(n, n = 1)\n    \n # Add the mode line\n  plot &lt;- plot + geom_line(data = modes, aes(x = !!sym(x_col), y = !!sym(y_col), group = 1), linetype = \"dashed\")\n\n  data_xbinned &lt;- calculate_summary(data, x_col, y_col, x_seq)\n  \n  plot &lt;- plot +\n    geom_errorbar(data = data_xbinned, aes(x = !!sym(x_col), ymin = mean_y - sd_y, ymax = mean_y + sd_y), width = 0.2) +\n    geom_line(data = data_xbinned, aes(x = !!sym(x_col), y = mean_y))\n    # Note: ggline will produce another figure, so we use geom_line instead\n\n\n  plot &lt;- plot + labs(y = y_label) + # Set y-axis label\n    scale_fill_viridis_c() +\n    # scale_fill_viridis_c(trans = 'log', labels = label_number(accuracy = 0.001)) +\n    theme_pubr(base_size = 16, legend = \"r\")\n\n  return(plot)\n}\n\n\nPlotting function for Level 2 averaged data.\n\n\nCode\n# Utility function for plotting\nplot_util &lt;- function(df, x_var, y_var, y_lab, y_var_norm, y_lab_norm) {\n  # Plot for the main variable\n  p1 &lt;- ggplot(df, aes(x = .data[[x_var]], y = .data[[y_var]], color = .data$sat, linetype = .data$sat)) + \n    geom_line() + geom_point() +\n    labs(y = y_lab)\n      \n  # Plot for normalized variable\n  p2 &lt;- ggplot(df, aes(x = .data[[x_var]], y = .data[[y_var_norm]], color = .data$sat, linetype = .data$sat)) + \n    geom_line() + geom_point() +\n    labs(y = y_lab_norm)\n\n  # Common elements for plots\n  common_elements &lt;- list(\n    labs(x = x_var, color=\"Satellites\", linetype=\"Satellites\"),\n    theme_pubr(base_size = 16),\n    theme(legend.text = element_text(size=16)),\n    scale_color_okabeito(palette = \"black_first\")\n  )\n  \n  # Apply common elements and combine the plots vertically\n  p1 &lt;- ggpar(p1 + common_elements, xlab=FALSE)\n  p2 &lt;- ggpar(p2 + common_elements, legend = \"none\")\n  p &lt;- p1 / p2\n  \n  return(p)\n}\n\n\nHistogram"
  },
  {
    "objectID": "analysis/properties.html#thickness",
    "href": "analysis/properties.html#thickness",
    "title": "SWD Properties",
    "section": "Thickness",
    "text": "Thickness\nNote since want different y-axis titles (labels) for each facet, not different facet titles, it is not clear how to do this with facet_wrap after pivot_longer. Also these are different units, so it is better to plot them separately and combine them together.\n\nEvolution\n\n\nCode\nplot_thickness &lt;- function(df, x_var = \"time\") {\n  plot_util(df, x_var, \"L_mn\", \"Thickness (km)\", \"L_mn_norm\", \"Normalized thickness (d_i)\")\n}\n\n\n\n\nCode\np &lt;- plot_thickness(all_candidates_l2)\np &lt;- ggpar(p, xlab=\"Time\")\nprint(p)\n\nsave_plot(\"thickness_time\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nIn addition: Warning messages:\n1: Removed 19 rows containing missing values (`geom_point()`). \n2: Removed 19 rows containing missing values (`geom_point()`). \n3: Removed 19 rows containing missing values (`geom_point()`). \n4: Removed 19 rows containing missing values (`geom_point()`). \n5: Removed 19 rows containing missing values (`geom_point()`). \n6: Removed 19 rows containing missing values (`geom_point()`). \n\n\n\n\n\n\n\n\n\n\nCode\nplot_thickness_n2 &lt;- function(df, x_var = \"time\") {\n  plot_util(df, x_var, \"L_mn\", \"Thickness (km)\", \"L_mn_n2\", \"Normalized thickness (d_i)\")\n}\n\np &lt;- plot_thickness_n2(all_candidates_l2_n2, x_var=\"ref_radial_distance\")\nprint(p)\n\n\nIn addition: Warning messages:\n1: Removed 13 rows containing missing values (`geom_line()`). \n2: Removed 28 rows containing missing values (`geom_point()`). \n3: Removed 13 rows containing missing values (`geom_line()`). \n4: Removed 28 rows containing missing values (`geom_point()`). \n\n\n\n\n\n\n\n\n\n\nCode\np &lt;- plot_thickness(all_candidates_l2, x_var=\"ref_radial_distance\")\np &lt;- ggpar(p, xlab=\"Referred Radial Distance (AU)\")\nprint(p)\n\nsave_plot(\"thickness_r\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nIn addition: There were 12 warnings (use warnings() to see them)\n\n\n\n\n\n\n\n\n\n\nMap\n\n\nCode\ny_lim &lt;- NULL\np &lt;- plot_binned_data(jno_candidates_l1, x_col = \"radial_distance\", y_col = \"L_mn\", x_bins = 8, y_bins = 32, y_lim = y_lim, log_y = TRUE)\np &lt;- ggpar(p, xlab=\"Radial Distance (AU)\", ylab=\"Log Thickness (km)\")\nprint(p)\n\nsave_plot(\"thickness_r_dist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\n\n\n\n\n\n\n\n\nCode\ny_lim &lt;- c(0,100)\np &lt;- plot_binned_data(jno_candidates_l1, x_col = \"radial_distance\", y_col = \"L_mn_norm\", x_bins = 8, y_bins = 32, y_lim = y_lim, log_y = TRUE)\np &lt;- p + labs(x = \"Radial Distance (AU)\", y= expression(Log~Normalized~Thickness~(d[i])))\nprint(p)\n\nsave_plot(\"thickness_N1_r_dist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\n\n\n\n\n\n\n\n\nHistogram\n\n\nCode\nx &lt;- \"L_mn_norm\"\nx_lim &lt;- c(0,60)\nfacet_var &lt;- \"r_bin\"\n\np &lt;- plot_limited_histogram(all_candidates_l1, x = x, x_lim = x_lim , bins = 10, facet_var=facet_var)\nprint(p)\n\nsave_plot(\"thickness_N1_r_hist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- \"L_mn_norm_log\"\nx_lim &lt;- c(0, 2)\nfacet_var &lt;- \"r_bin\"\n\np &lt;- plot_limited_histogram(all_candidates_l1, x = x, x_lim = x_lim , bins = 10, facet_var=facet_var)\nprint(p)\n\nsave_plot(\"thickness_N1_log_r_hist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image"
  },
  {
    "objectID": "analysis/properties.html#current-intensity",
    "href": "analysis/properties.html#current-intensity",
    "title": "SWD Properties",
    "section": "Current intensity",
    "text": "Current intensity\n\nMean value\n\n\nCode\nplot_j &lt;- function(df, x_var = \"time\") {\n  plot_util(df, x_var, \"j0\", \"J (nA/m^2)\", \"j0_norm\", \"Normalized J (J_A)\")\n}\n\n\n\n\nCode\np &lt;- plot_j(all_candidates_l2, x_var=\"time\")\np &lt;- ggpar(p, xlab=\"Time\")\nprint(p)\n\nsave_plot(\"current_time\")\n\n\n\n\nCode\np &lt;- plot_j(all_candidates_l2, x_var=\"ref_radial_distance\")\np &lt;- ggpar(p, xlab=\"Referred Radial Distance (AU)\")\nprint(p)\n\nsave_plot(\"current_r\")\n\n\n\n\nMap\n\n\nCode\ny_lim &lt;- c(0, 15)\np &lt;- plot_binned_data(jno_candidates_l1, x_col = \"radial_distance\", y_col = \"j0\", x_bins = 8, y_bins = 32, y_lim = y_lim, log_y = TRUE)\np &lt;- p + labs(x = \"Radial Distance (AU)\", y= expression(Log~J~(nA~m^-2)))\nprint(p)\n\nsave_plot(\"current_r_dist\")\n\n\n\n\nCode\np&lt;-plot_binned_data(jno_candidates_l1, x_col = \"radial_distance\", y_col = \"j0_norm\", x_bins = 8, y_bins = 32, y_lim = c(0, 1), log_y = TRUE)\np &lt;- p + labs(x = \"Radial Distance (AU)\", y= expression(Log~Normalized~J~(J[A])))\nprint(p)\n\nsave_plot(\"current_N1_r_dist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\n\n\n\n\n\n\n\n\nHistogram\n\n\nCode\nx &lt;- \"j0_norm\"\nx_lim &lt;- c(0, 1)\nfacet_var &lt;- \"r_bin\"\n\np &lt;- plot_limited_histogram(all_candidates_l1, x = x, x_lim = x_lim, bins = 8, facet_var=facet_var)\nprint(p)\n\nsave_plot(\"current_N1_r_hist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\n\n\n\n\n\n\n\n\nCode\nx &lt;- \"j0_norm_log\"\nx_lim &lt;- c(-2, 0)\nfacet_var &lt;- \"r_bin\"\n\np &lt;- plot_limited_histogram(all_candidates_l1, x = x, x_lim = x_lim, bins = 8, facet_var=facet_var)\nprint(p)\n\nsave_plot(\"current_N1_log_r_hist\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image"
  },
  {
    "objectID": "analysis/properties.html#map-of-thickness-and-current-intensity",
    "href": "analysis/properties.html#map-of-thickness-and-current-intensity",
    "title": "SWD Properties",
    "section": "Map of thickness and current intensity",
    "text": "Map of thickness and current intensity\n\n\nCode\np &lt;- ggplot(all_candidates_l1, aes(x = L_mn_norm, y = j0_norm)) +\n  stat_density_2d(aes(fill = ..density..), geom = \"raster\", contour = FALSE) +\n  facet_wrap(~ sat, scales = \"free\") +\n  scale_x_log10() + \n  scale_y_log10()\n\nprint(p)\n\n\n\n\nCode\np &lt;- ggplot(jno_candidates_l1, aes(x = L_mn_norm, y = j0_norm)) +\n  stat_density_2d(aes(fill = ..density..), geom = \"raster\", contour = FALSE) +\n  facet_wrap(~ r_bin, ncol = length(unique(jno_candidates_l1$r_bin))) +\n  scale_x_log10() + \n  scale_y_log10() +\n  labs(fill = \"Density\")\n\nprint(p)\n\n\n\n\nCode\np &lt;- ggplot(jno_candidates_l1, aes(x = L_mn_norm, y = j0_norm)) +\n  stat_density_2d(aes(fill = ..density..), geom = \"raster\", contour = FALSE) +\n  facet_wrap(~ r_bin, nrow = length(unique(jno_candidates_l1$r_bin))) +\n  scale_x_log10() + \n  scale_y_log10() +\n  labs(fill = \"Density\")\n\nprint(p)"
  },
  {
    "objectID": "analysis/or.html",
    "href": "analysis/or.html",
    "title": "SWD Occurence Rate",
    "section": "",
    "text": "Code\nfrom fastcore.utils import *\nfrom fastcore.test import *\n\nfrom ids_finder.utils.basic import *\n\nfrom datetime import timedelta\n\nimport polars as pl\nimport polars.selectors as cs\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\n\n\n\n\nCode\nimport warnings\n\n# This will filter out all FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n\n\n\n\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n\n\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\ncatalog = load_catalog('../../')\ncatalog.list()\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport scienceplots\nfrom ids_finder.utils.basic import savefig\n\nplt.style.use(['science', 'nature', 'notebook'])\n\n\n\n\n\n\n\nCode\nfrom ids_finder.candidates import cIDsDataset\n\n\n\n\nCode\nsta_candidate = cIDsDataset(sat_id=\"STA\", tau=60, ts=1, catalog=catalog)\njno_candidate = cIDsDataset(sat_id=\"JNO\", tau=60, ts=1, catalog=catalog)\nthb_candidate = cIDsDataset(sat_id=\"THB\", tau=60, ts=1, catalog=catalog)\n\n\n                    INFO     Loading data from 'candidates.STA_ts_1s_tau_60s'                   data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'STA.primary_mag_ts_1s' (PartitionedDataset)...  data_catalog.py:502\n\n\n\n                    INFO     Loading data from 'candidates.JNO_ts_1s_tau_60s'                   data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'JNO.primary_mag_ts_1s' (PartitionedDataset)...  data_catalog.py:502\n\n\n\n                    INFO     Loading data from 'candidates.THB_ts_1s_tau_60s'                   data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'THB.primary_mag_ts_1s' (PartitionedDataset)...  data_catalog.py:502\n\n\n\nARTEMIS missions needs additional care as they are not always in the solar wind.\n\n\nCode\nfrom ids_finder.utils.basic import filter_tranges_df\n\n\n\n\nCode\nthb_inter_state_sw: pl.LazyFrame = catalog.load('thb.inter_state_sw')\nstart, end = thb_inter_state_sw.select(['start', 'end']).collect()\n\nthb_sw_candidate = cIDsDataset(\n    sat_id=\"THB\", tau=60, ts=1, catalog=catalog,\n    candidates = filter_tranges_df(thb_candidate.candidates, (start, end)), \n    data = filter_tranges_df(thb_candidate.data.collect(), (start, end)).lazy()\n)\n\n\n                    INFO     Loading data from 'thb.inter_state_sw' (LazyPolarsDataset)...      data_catalog.py:502\n\n\n\n\n\nCode\nsat_candidates = [sta_candidate, jno_candidate, thb_sw_candidate]"
  },
  {
    "objectID": "analysis/or.html#setup",
    "href": "analysis/or.html#setup",
    "title": "SWD Occurence Rate",
    "section": "",
    "text": "Code\nfrom fastcore.utils import *\nfrom fastcore.test import *\n\nfrom ids_finder.utils.basic import *\n\nfrom datetime import timedelta\n\nimport polars as pl\nimport polars.selectors as cs\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\n\n\n\n\nCode\nimport warnings\n\n# This will filter out all FutureWarnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n\n\n\n\n\nCode\nfrom beforerr.r import py2rpy_polars\nimport rpy2.robjects as robjects\nr = robjects.r\nr.source('utils.R')\n\nconv_pl = py2rpy_polars()\n\n\nThe rpy2.ipython extension is already loaded. To reload it, use:\n  %reload_ext rpy2.ipython\n\n\n\n\n\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\ncatalog = load_catalog('../../')\ncatalog.list()\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport scienceplots\nfrom ids_finder.utils.basic import savefig\n\nplt.style.use(['science', 'nature', 'notebook'])\n\n\n\n\n\n\n\nCode\nfrom ids_finder.candidates import cIDsDataset\n\n\n\n\nCode\nsta_candidate = cIDsDataset(sat_id=\"STA\", tau=60, ts=1, catalog=catalog)\njno_candidate = cIDsDataset(sat_id=\"JNO\", tau=60, ts=1, catalog=catalog)\nthb_candidate = cIDsDataset(sat_id=\"THB\", tau=60, ts=1, catalog=catalog)\n\n\n                    INFO     Loading data from 'candidates.STA_ts_1s_tau_60s'                   data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'STA.primary_mag_ts_1s' (PartitionedDataset)...  data_catalog.py:502\n\n\n\n                    INFO     Loading data from 'candidates.JNO_ts_1s_tau_60s'                   data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'JNO.primary_mag_ts_1s' (PartitionedDataset)...  data_catalog.py:502\n\n\n\n                    INFO     Loading data from 'candidates.THB_ts_1s_tau_60s'                   data_catalog.py:502\n                             (LazyPolarsDataset)...                                                                \n\n\n\n                    INFO     Loading data from 'THB.primary_mag_ts_1s' (PartitionedDataset)...  data_catalog.py:502\n\n\n\nARTEMIS missions needs additional care as they are not always in the solar wind.\n\n\nCode\nfrom ids_finder.utils.basic import filter_tranges_df\n\n\n\n\nCode\nthb_inter_state_sw: pl.LazyFrame = catalog.load('thb.inter_state_sw')\nstart, end = thb_inter_state_sw.select(['start', 'end']).collect()\n\nthb_sw_candidate = cIDsDataset(\n    sat_id=\"THB\", tau=60, ts=1, catalog=catalog,\n    candidates = filter_tranges_df(thb_candidate.candidates, (start, end)), \n    data = filter_tranges_df(thb_candidate.data.collect(), (start, end)).lazy()\n)\n\n\n                    INFO     Loading data from 'thb.inter_state_sw' (LazyPolarsDataset)...      data_catalog.py:502\n\n\n\n\n\nCode\nsat_candidates = [sta_candidate, jno_candidate, thb_sw_candidate]"
  },
  {
    "objectID": "analysis/or.html#data-normalization",
    "href": "analysis/or.html#data-normalization",
    "title": "SWD Occurence Rate",
    "section": "Data normalization",
    "text": "Data normalization\nDifferent levels of normalization are applied to the data. The normalization is done in the following order:\n\nN1: normalize the data by the the effective time of every duration due to the data gap as we may miss some potential IDs. We assume the data gap is independent of the magnetic discontinuities.\nN2: normalize the data by the mean value of data near 1 AU. This is to remove the effect of the temporal variation of the solar wind.\n\n\n\nCode\ndef calc_n1_factor(\n    data: pl.LazyFrame,\n    s_resolution: timedelta,\n    l_resolution: timedelta,\n):\n    return (\n        data.sort(\"time\")\n        .group_by_dynamic(\"time\", every=s_resolution)\n        .agg(pl.lit(1).alias(\"availablity\"))\n        .group_by_dynamic(\"time\", every=l_resolution)\n        .agg(n1_factor=pl.sum(\"availablity\") * s_resolution / l_resolution)\n    )\n\n\ndef n1_normalize(\n    df: pl.DataFrame,  # the dataframe with count to be normalized\n    data: pl.LazyFrame,  # the data used to calculate the duration ratio\n    s_resolution,  # the smallest resolution to check if the data is available\n    avg_window,\n):\n    duration_df = calc_n1_factor(data, s_resolution, avg_window)\n\n    return df.lazy().join(duration_df, how=\"left\", on=\"time\").with_columns(\n        o_rates_normalized=pl.col(\"o_rates\") / pl.col(\"n1_factor\")\n    ).collect()\n\n\n\n\nCode\ndef n2_normalize(df: pl.DataFrame):\n    avg_sats = [\"STA\", \"THB\"]\n    avg_df = (\n        df.filter(pl.col(\"sat\").is_in(avg_sats))\n        .group_by(\"time\")\n        .agg(n2_factor=pl.mean(\"o_rates_normalized\"))\n    )\n    return df.join(avg_df, on=\"time\").with_columns(\n        o_rates_normalized=pl.col(\"o_rates_normalized\") / pl.col(\"n2_factor\")\n    )"
  },
  {
    "objectID": "analysis/or.html#distance-and-occurrence-rates-versus-time-for-juno",
    "href": "analysis/or.html#distance-and-occurrence-rates-versus-time-for-juno",
    "title": "SWD Occurence Rate",
    "section": "Distance and Occurrence rates versus time for JUNO",
    "text": "Distance and Occurrence rates versus time for JUNO\n\n\nCode\ndef calc_or_df(candidates: pl.DataFrame, avg_window=\"5d\", col_names=None, by=None):\n    \"\"\"Calculate the occurence rate of the candidates with the average window.\n\n    Notes: occurence rate is defined as the number of candidates per day.\n    \"\"\"\n\n    every = pd.Timedelta(avg_window)\n    or_factor = every / pd.Timedelta(\"1d\")\n\n    cols = cs.by_name(col_names) if col_names != None else cs.float()\n    \n    temp_df = (\n        candidates.sort(\"time\")\n        .group_by_dynamic(\"time\", every=every, by=by)\n        .agg(\n            cols.mean(),\n            o_rates = pl.count() / or_factor,\n        )\n        .upsample(\"time\", every=every) # upsample to fill the missing time\n    )\n    return temp_df\n\n@patch\ndef calc_or(self: cIDsDataset, avg_window=\"5d\", col_names=None):\n    return calc_or_df(self.candidates, avg_window, col_names)\n\n@patch\ndef calc_or_normalized(self: cIDsDataset, s_resolution: timedelta, avg_window: timedelta):\n    count_df = self.calc_or(avg_window)\n    return n1_normalize(count_df, self.data, s_resolution, avg_window)\n\n\n\n\nCode\ndf = jno_candidate.calc_or()\n\n\n\n\nCode\np1 &lt;- ggplot(df, aes(x = time, y = radial_distance)) + \n  geom_line() + # Plot distance by date\n  labs(x = \"Date\", y = \"Distance (AU)\") +\n  theme_pubr(base_size = 16) + \n  theme(aspect.ratio=0.25)\n  \np2 &lt;- ggplot(df, aes(x = time, y = o_rates)) + \n  geom_line() + # Plot distance by date\n  labs(x = \"Date\", y = \"Occurrence Rates (#/day)\") +\n  theme_pubr(base_size = 16) + \n  theme(aspect.ratio=0.25)\n\np &lt;- ggarrange(p1, p2, nrow = 2)\n\n# save_plot(\"distance_and_or\")\np\n\n\n\n\n\n\n\n\n\n\nCode\nfrom beforerr.basics import pmap\n\n\n\n\nCode\ns_resolution = timedelta(minutes=1)\navg_window = timedelta(days=30)\n\n\n\n\nCode\nall_candidates_or_N1: pl.DataFrame = pl.concat(\n    sat_candidates\n    | pmap(\n        lambda x: x.calc_or_normalized(s_resolution, avg_window).with_columns(\n            sat=pl.lit(x.sat_id)\n        )\n    ),\n    how=\"diagonal\",\n)\n\n\n\n\nCode\nall_candidates_or_N2 = n2_normalize(all_candidates_or_N1)"
  },
  {
    "objectID": "analysis/or.html#occurrence-rates-versus-time-for-all-missions",
    "href": "analysis/or.html#occurrence-rates-versus-time-for-all-missions",
    "title": "SWD Occurence Rate",
    "section": "Occurrence rates versus time for all missions",
    "text": "Occurrence rates versus time for all missions\n\n\nCode\ndef plot_or_time(df: pl.DataFrame):\n    \"\"\"Plot the occurence rate of the candidates with time.    \n    \"\"\"\n    # Create a unique list of all satellites and sort them to let JNO' be plotted first\n    all_sats = df[\"sat\"].unique().to_list()\n    all_sats.sort(key=lambda x: x != \"JNO\")\n\n    # Plot each satellite separately\n    for sat in all_sats:\n        sat_df = df.filter(sat=sat)\n        if sat == \"JNO\":\n            sns.lineplot(sat_df, x=\"time\", y=\"o_rates_normalized\", label=sat)\n        else:\n            # Making the other satellites more distinct with linestyle and alpha\n            sns.lineplot(\n                sat_df,\n                x=\"time\",\n                y=\"o_rates_normalized\",\n                linestyle=\"--\",  # dashed line style\n                alpha=0.5,  # keep the order of the legend\n                label=sat,\n            )\n\n    ax = plt.gca()  # Get current axis\n    # Set the y-axis and x-axis labels\n    ax.set_ylabel(\"Occurrence Rates (#/day)\")\n    ax.set_xlabel(\"Date\")\n    ax.legend(title=\"Satellites\")\n\n    # savefig(\"occurrence_rates\")\n    return ax.figure\n\n\n\n\nCode\nfig = plot_or_time(\n    all_candidates_or_N1\n)\n\n\n\n\n\n\n\n\nNotes: seaborn.lineplot drops nans from the DataFrame before plotting, this is not desired…\n\n\nCode\nplot_or_time &lt;- function(df) {\n  p &lt;- ggline(\n    df, x = \"time\", y = \"o_rates_normalized\", \n    color = \"sat\", linetype = \"sat\") \n  \n  p &lt;- p +   \n    labs(x = \"Date\", y = \"Occurrence Rates (#/day)\", color=\"Satellites\", linetype=\"Satellites\") + \n    theme_pubr(base_size = 16) + \n    theme(legend.text = element_text(size=16)) +\n    scale_color_okabeito(palette = \"black_first\")\n    \n\n  \n  print(p)\n  return(p)\n}\n\n\n\n\nCode\n\n\n\nIn addition: Warning message:\nRemoved 6 rows containing missing values (`geom_point()`). \n\n\n\n\n\n\n\n\nWe noticed some anomalies in the occurrence rates of the magnetic discontinuities for Stereo-A data. Also for Juno, its occurrence rate is much higher when approaching Jupiter.\n\n\nCode\nall_candidates_or_N1.filter(\n    pl.col('time').is_in(pd.date_range('2014-01-01', '2015-01-01')),\n    sat='STA'\n)[[\"time\", \"o_rates\", \"o_rates_normalized\", 'n1_factor']]\n\n\n\n\n\n\nshape: (12, 4)\n\n\n\ntime\no_rates\no_rates_normalized\nn1_factor\n\n\ndatetime[ns]\nf64\nf64\nf64\n\n\n\n\n2014-01-10 00:00:00\n71.5\n74.697105\n0.957199\n\n\n2014-02-09 00:00:00\n79.1\n79.134805\n0.99956\n\n\n2014-03-11 00:00:00\n79.366667\n82.101482\n0.96669\n\n\n2014-04-10 00:00:00\n78.933333\n78.944298\n0.999861\n\n\n2014-05-10 00:00:00\n80.1\n80.116691\n0.999792\n\n\n2014-06-09 00:00:00\n72.4\n78.784856\n0.918958\n\n\n2014-07-09 00:00:00\n74.0\n83.265179\n0.888727\n\n\n2014-08-08 00:00:00\n4.4\n9.83342\n0.447454\n\n\n2014-09-07 00:00:00\n12.266667\n77.541703\n0.158194\n\n\n2014-10-07 00:00:00\n7.066667\n82.619756\n0.085532\n\n\n2014-11-06 00:00:00\n11.6\n78.238876\n0.148264\n\n\n2014-12-06 00:00:00\n10.666667\n69.554717\n0.153356\n\n\n\n\n\n\nSurprisingly, we found out that the anomaly of STEREO-A data is not mainly due to data gap. We can inspect this data further. See appendix.\nWe remove the anomaly of STEREO-A and the restrict the time range to exclude Jupiter’s effect.\n\n\nCode\nall_candidates_or_N1_cleaned = (\n    all_candidates_or_N1.sort(\"time\")\n    .filter(pl.col(\"time\") &lt; pd.Timestamp(\"2016-06-01\"), pl.col(\"o_rates\") &gt; 10)\n    .upsample(\"time\", every=avg_window, by=\"sat\", maintain_order=True)\n    .with_columns(pl.col(\"sat\").forward_fill())\n)\n\nall_candidates_or_N2_cleaned = n2_normalize(all_candidates_or_N1_cleaned)\n\n\n\n\nCode\nplot_or_time(all_candidates_or_N1_cleaned)\nsave_plot(\"ocr_time_cleaned\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nIn addition: Warning messages:\n1: Removed 20 rows containing missing values (`geom_point()`). \n2: Removed 20 rows containing missing values (`geom_point()`). \n3: Removed 20 rows containing missing values (`geom_point()`). \n\n\n\n\n\n\n\n\n\n\nCode\np &lt;- plot_or_time(all_candidates_or_N2_cleaned)\np &lt;- p + labs(y = \"Normalized Occurrence Rates\")\nsave_plot(\"ocr_time_N2_cleaned\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nIn addition: Warning messages:\n1: Removed 20 rows containing missing values (`geom_point()`). \n2: Removed 20 rows containing missing values (`geom_point()`). \n3: Removed 20 rows containing missing values (`geom_point()`). \n\n\n\n\n\n\n\n\n\nPlot the occurrence rates with radial distance\n\n\nCode\ndef link_coord2dim(df: pl.DataFrame, dim=\"time\", coord: str = \"radial_distance\"):\n    \"\"\"Link the coord to a dimension across different subgroups\n\n    Note: this idea is borrowed from the `xarray.DataArray.coords`.\n    \"\"\"\n    base_df = df.filter(sat=\"JNO\").select(dim, coord).rename({coord: f\"ref_{coord}\"})\n    return df.join(base_df, on=dim)\n\n\ndef plot_or_r(df: pl.DataFrame):\n    \"plot normalized occurence rate over radial distance\"\n\n    sns.lineplot(x=\"ref_radial_distance\", y=\"o_rates_normalized\", hue=\"sat\", data=df)\n\n    ax = plt.gca()  # Get current axis\n    ax.set_yscale(\"log\")\n    ax.set_xlabel(\"Referred Radial Distance (AU)\")\n    ax.set_ylabel(\"Normalized Occurrence Rate\")\n    # savefig('occurrence_rate_ratio')\n\n    return ax.figure\n\n\n\n\nCode\nplot_or_r &lt;- function(df, target_sat = \"JNO\") {\n  \"plot normalized occurrence rate over radial distance\"\n  \n  # Filter data for target_sat\n  df_target &lt;- df[df$sat == target_sat,]\n  \n  # Compute the linear model\n  fit &lt;- lm(o_rates_normalized ~ I(1/ref_radial_distance), data = df_target)\n  \n  # Extract coefficients\n  intercept &lt;- coef(fit)[1]\n  slope &lt;- coef(fit)[2]\n  \n  # Format equation\n  equation &lt;- sprintf(\"y ~ %.2f / x\", slope)\n  \n  p &lt;- ggscatter(df, x = \"ref_radial_distance\", y = \"o_rates_normalized\", color = \"sat\") +\n    geom_smooth(data = df_target, formula = y ~ I(1/x), method = \"lm\", color=\"gray\", linetype=\"dashed\")\n    \n  p &lt;- p +\n    labs(x = \"Referred Radial Distance (AU)\", y = \"Occurrence Rate  (#/day)\", color=\"Satellites\") +\n    annotate(\"text\", label = equation, x = Inf, y = Inf, hjust = 1.1, vjust = 1.5, size = 7) +\n    theme_pubr(base_size = 16) + \n    theme(legend.text = element_text(size=16)) +\n    scale_color_okabeito(palette = \"black_first\")\n  \n  return(p)\n}\n\n\n\n\nCode\ndf = link_coord2dim(all_candidates_or_N1_cleaned).sort(\"ref_radial_distance\")\n\n\n\n\nCode\np &lt;- plot_or_r(df)\nprint(p)\nsave_plot(\"ocr_r_cleaned\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nIn addition: Warning messages:\n1: Removed 10 rows containing non-finite values (`stat_smooth()`). \n2: Removed 40 rows containing missing values (`geom_point()`). \n3: Removed 10 rows containing non-finite values (`stat_smooth()`). \n4: Removed 40 rows containing missing values (`geom_point()`). \n5: Removed 10 rows containing non-finite values (`stat_smooth()`). \n6: Removed 40 rows containing missing values (`geom_point()`). \n\n\n\n\n\n\n\n\n\n\nCode\ndf = link_coord2dim(all_candidates_or_N2_cleaned).sort(\"ref_radial_distance\")\n\n\n\n\nR Code\np &lt;- plot_or_r(df) + labs(y = \"Normalized Occurrence Rates\")\nprint(p)\nsave_plot(\"ocr_r_N2_cleaned\")\n\n\nSaving 6.67 x 6.67 in image\nSaving 6.67 x 6.67 in image\n\n\nIn addition: Warning messages:\n1: Removed 10 rows containing non-finite values (`stat_smooth()`). \n2: Removed 40 rows containing missing values (`geom_point()`). \n3: Removed 10 rows containing non-finite values (`stat_smooth()`). \n4: Removed 40 rows containing missing values (`geom_point()`). \n5: Removed 10 rows containing non-finite values (`stat_smooth()`). \n6: Removed 40 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "analysis/or.html#appendix",
    "href": "analysis/or.html#appendix",
    "title": "SWD Occurence Rate",
    "section": "Appendix",
    "text": "Appendix\n\nSTEREO-A anomaly during 2014-08 period\n\n\nCode\nmag: pl.DataFrame = catalog.load('sta.inter_mag_1s').filter(pl.col('B')&gt;0).collect()\n\n\n\n\nCode\nimport plotly.graph_objects as go;\nfrom plotly_resampler import FigureResampler\nimport plotly.express as px\n\n\n\n\nCode\n# px.line(mag, x='time', y='B') # This is extremely slow for large datasets\n\nfig = FigureResampler(go.Figure())\nfig.add_trace({\"x\": mag[\"time\"], \"y\": mag['B']})\nfig"
  },
  {
    "objectID": "analysis/10_classification.html",
    "href": "analysis/10_classification.html",
    "title": "ID classification",
    "section": "",
    "text": "Code\nimport numpy as np\nIn this method, TDs and RDs satisfy $ &lt; 0.2$ and $ | | &gt; 0.4$ B BN bg ∣∣ ∣∣ , &lt; D 0.2 B B bg ∣∣ ∣ ∣ , respectively. Moreover, IDs with &lt; 0.4 B BN bg ∣∣ ∣∣ , &lt; D 0.2 B B bg ∣∣ ∣ ∣ could be either TDs or RDs, and so are termed EDs. Similarly, NDs are defined as &gt; 0.4 B BN bg ∣∣ ∣∣ , &gt; D 0.2 B B bg ∣∣ ∣ ∣ because they can be neither TDs nor RDs. It is worth noting that EDs and NDs here are not physical concepts like RDs and TDs. RDs or TDs correspond to specific types of structures in the MHD framework, while EDs and NDs are introduced just to better quantify the statistical results.\nCriteria Used to Classify Discontinuities on the Basis of Magnetic Data Type\nCode\nBnOverB_RD_lower_threshold = 0.4\ndBOverB_RD_upper_threshold = 0.2\n\nBnOverB_TD_upper_threshold = 0.2\ndBOverB_TD_lower_threshold = dBOverB_RD_upper_threshold\n\nBnOverB_ED_upper_threshold = BnOverB_RD_lower_threshold\ndBOverB_ED_upper_threshold = dBOverB_TD_lower_threshold\n\nBnOverB_ND_lower_threshold = BnOverB_TD_upper_threshold\ndBOverB_ND_lower_threshold = dBOverB_RD_upper_threshold\nCode\ndef classify_id(BnOverB, dBOverB):\n    BnOverB = np.abs(np.asarray(BnOverB))\n    dBOverB = np.asarray(dBOverB)\n\n    s1 = (BnOverB &gt; BnOverB_RD_lower_threshold)\n    s2 = (dBOverB &gt; dBOverB_RD_upper_threshold)\n    s3 = (BnOverB &gt; BnOverB_TD_upper_threshold)\n    s4 = s2 # note: s4 = (dBOverB &gt; dBOverB_TD_lower_threshold)\n    \n    RD = s1 & ~s2\n    TD = ~s3 & s4\n    ED = ~s1 & ~s4\n    ND = s3 & s2\n\n    # Create an empty result array with the same shape\n    result = np.empty_like(BnOverB, dtype=object)\n\n    result[RD] = \"RD\"\n    result[TD] = \"TD\"\n    result[ED] = \"ED\"\n    result[ND] = \"ND\"\n\n    return result"
  },
  {
    "objectID": "analysis/10_classification.html#classification-of-candidates",
    "href": "analysis/10_classification.html#classification-of-candidates",
    "title": "ID classification",
    "section": "Classification of candidates",
    "text": "Classification of candidates\n\n\nCode\nsns.jointplot(\n    candidates_jno_tau_60s,\n    x='dBOverB', y='BnOverB',\n    # kind='kde',\n    kind=\"hex\",\n)\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:2                                                                                    │\n│                                                                                                  │\n│   1 sns.jointplot(                                                                               │\n│ ❱ 2 │   candidates_jno_tau_60s,                                                                  │\n│   3 │   x='dBOverB', y='BnOverB',                                                                │\n│   4 │   # kind='kde',                                                                            │\n│   5 │   kind=\"hex\",                                                                              │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'candidates_jno_tau_60s' is not defined\n\n\n\n\n\nCode\nsns.jointplot(\n    all_candidates,\n    x='dBOverB', y='BnOverB',\n    # kind='kde',\n    kind=\"hex\",\n)\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:2                                                                                    │\n│                                                                                                  │\n│   1 sns.jointplot(                                                                               │\n│ ❱ 2 │   all_candidates,                                                                          │\n│   3 │   x='dBOverB', y='BnOverB',                                                                │\n│   4 │   # kind='kde',                                                                            │\n│   5 │   kind=\"hex\",                                                                              │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'all_candidates' is not defined\n\n\n\n\nDistribution of the DD types\n\nPlot distribution of types for each missions\n\n\nCode\nalt.Chart(all_candidates).encode(\n    x=alt.X(\"count()\").stack(\"normalize\").title(\"Share of ID types\"),\n    y=alt.Y('sat').title(None),\n    color='type',\n).mark_bar()\n\n# alt.Chart(distributions).encode(\n#     alt.X('ratio:Q', title='DD type distribution').axis(format='.0%'),\n#     y=alt.Y('sat', title=None),\n#     color='type',\n# ).mark_bar()\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱  1 alt.Chart(all_candidates).encode(                                                           │\n│    2 │   x=alt.X(\"count()\").stack(\"normalize\").title(\"Share of ID types\"),                       │\n│    3 │   y=alt.Y('sat').title(None),                                                             │\n│    4 │   color='type',                                                                           │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'alt' is not defined\n\n\n\n\n\nCode\ndistributions = all_candidates.group_by(\"sat\", \"type\").agg(pl.count()).with_columns(\n    (pl.col(\"count\") / pl.sum(\"count\").over(\"sat\")).alias(\"ratio\")\n)\ncount_table = distributions.to_pandas().pivot(index='sat', columns='type', values='count')[['RD', 'TD', 'ED', 'ND']]\nratio_table = distributions.to_pandas().pivot(index='sat', columns='type', values='ratio')[['RD', 'TD', 'ED', 'ND']]\n# display(distributions.to_pandas().pivot(index='sat', columns='type', values='ratio')[['RD', 'TD', 'ED', 'ND']].style.format(\"{:.0%}\"))\ndisplay(count_table, ratio_table.style.format(\"{:.0%}\"))\n\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 distributions = all_candidates.group_by(\"sat\", \"type\").agg(pl.count()).with_columns(         │\n│   2 │   (pl.col(\"count\") / pl.sum(\"count\").over(\"sat\")).alias(\"ratio\")                           │\n│   3 )                                                                                            │\n│   4 count_table = distributions.to_pandas().pivot(index='sat', columns='type', values='count     │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'all_candidates' is not defined\n\n\n\n\n\nPlot distribution of types for each missions over time\n\n\nCode\nalt.Chart(all_candidates).mark_bar(binSpacing=0).encode(\n    x=\"yearmonth(time)\",\n    y=alt.Y(\"count()\").stack(\"normalize\").title(\"Share of ID types\"),\n    row=alt.Row(\"sat\").title(None),\n    color=\"type\",\n).configure_axis(grid=False).properties(height=100)\n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 alt.Chart(all_candidates).mark_bar(binSpacing=0).encode(                                     │\n│   2 │   x=\"yearmonth(time)\",                                                                     │\n│   3 │   y=alt.Y(\"count()\").stack(\"normalize\").title(\"Share of ID types\"),                        │\n│   4 │   row=alt.Row(\"sat\").title(None),                                                          │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nNameError: name 'alt' is not defined\n\n\n\n\n\nCode\nplot_type_distribution &lt;- function(data, bin_width = 30) { \n    data$date_only &lt;- as.Date(data$time)\n    \n    p &lt;- ggplot(data, aes(date_only, fill = type)) +\n        geom_histogram(binwidth = bin_width, position = \"fill\") + \n        theme_pubr(base_size = 16)\n        \n    return(p)\n}\n\np1 &lt;- plot_type_distribution(jno_candidates)\np2 &lt;- plot_type_distribution(thb_candidates)\np3 &lt;- plot_type_distribution(sta_candidates)\n\np &lt;- ggarrange(\n    p1 + rremove(\"xlab\"),\n    p2 + rremove(\"xlab\"), p3, \n    nrow = 3, align = 'hv', \n    labels=list(\"JUNO\", \"ARTEMIS-B\", \"STEREO-A\"), hjust=0, vjust=0,\n    legend = 'right', common.legend = TRUE\n)\n# save_plot(filename = \"type_distribution\")\np\n\n\nUsageError: Cell magic `%%R` not found."
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Results",
    "section": "",
    "text": "For code, see noteboook.\nResults:\n\n\n\n\n\n\n\n\n\n\nFigure 1: Occurrence rates with time\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Occurrence rates with radial distance\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Thickness with time and radial distance\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Current density with time and radial distance\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Thickness distribution with radial distance\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Current distribution with radial distance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: ?(caption)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Current distribution with radial distance"
  },
  {
    "objectID": "analysis/index.html#occurrence-rates",
    "href": "analysis/index.html#occurrence-rates",
    "title": "Results",
    "section": "",
    "text": "For code, see noteboook.\nResults:\n\n\n\n\n\n\n\n\n\n\nFigure 1: Occurrence rates with time\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Occurrence rates with radial distance\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Thickness with time and radial distance\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Current density with time and radial distance\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Thickness distribution with radial distance\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Current distribution with radial distance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: ?(caption)\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Current distribution with radial distance"
  },
  {
    "objectID": "analysis/20_model.html",
    "href": "analysis/20_model.html",
    "title": "Models",
    "section": "",
    "text": "Need to run command in shell first as pipeline is project-specific command\nkedro pipeline create model\nkedro run --to-outputs=jno.primary_state_rtn_1h\n\n\nCode\nfrom loguru import logger\nfrom typing import Union, Collection, Callable, Optional, Tuple\nfrom typing import Any, Dict\n\n\nimport numpy as np\nimport polars as pl\nimport pandas\n\n\n\n\n\n\nCode\nfrom kedro.pipeline import Pipeline, node\nfrom kedro.pipeline.modular_pipeline import pipeline\n\n\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\n\n\n\nCode\ncatalog = load_catalog()\ncatalog.list()"
  },
  {
    "objectID": "analysis/20_model.html#setup",
    "href": "analysis/20_model.html#setup",
    "title": "Models",
    "section": "",
    "text": "Need to run command in shell first as pipeline is project-specific command\nkedro pipeline create model\nkedro run --to-outputs=jno.primary_state_rtn_1h\n\n\nCode\nfrom loguru import logger\nfrom typing import Union, Collection, Callable, Optional, Tuple\nfrom typing import Any, Dict\n\n\nimport numpy as np\nimport polars as pl\nimport pandas\n\n\n\n\n\n\nCode\nfrom kedro.pipeline import Pipeline, node\nfrom kedro.pipeline.modular_pipeline import pipeline\n\n\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\n\n\n\nCode\ncatalog = load_catalog()\ncatalog.list()"
  },
  {
    "objectID": "analysis/20_model.html#process-model-data",
    "href": "analysis/20_model.html#process-model-data",
    "title": "Models",
    "section": "Process model data",
    "text": "Process model data\n\n\nCode\ndef overview(df: pl.DataFrame):\n    \"\"\"Overview of the data\"\"\"\n    df_pd = df.to_pandas()\n    df_pd.hvplot(x=\"time\", y=bcols_rtn)\n    \n    b_fig = df_pd.hvplot.line(x=\"time\", y=bcols_rtn)\n    v_fig = df_pd.hvplot.line(x=\"time\", y=vcols_rtn)\n    rho_fig = df_pd.hvplot.line(x=\"time\", y=\"rho\", logy=True)\n    Ti_fig = df_pd.hvplot.line(x=\"time\", y=\"Ti\", logy=True)\n    return (b_fig + v_fig + rho_fig + Ti_fig).cols(1).opts(shared_axes=False)\n\n\n# jno_mswim2d_rtn.pipe(overview)\n\n\n\n\nCode\ncatalog.load('jno.primary_state_rtn_1h')\n\n\n[10/15/23 16:13:04] INFO     Loading data from 'jno.primary_state_rtn_1h' (GenericDataSet)...   data_catalog.py:492\n\n\n\n\n\n\n\nshape: (42_385, 10)\n\n\n\nradial_distance\nplasma_density\nplasma_temperature\ntime\nmodel_b_r\nmodel_b_t\nmodel_b_n\nsw_vel_r\nsw_vel_t\nsw_vel_n\n\n\nf64\nf64\ni64\ndatetime[ns]\nf64\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n1.004\n2.77562\n76394\n2011-08-25 00:00:00\n-1.214485\n2.724603\n0.578\n442.731905\n1.438722\n-0.0\n\n\n1.003\n2.82988\n76600\n2011-08-25 01:00:00\n-1.101071\n2.656081\n0.523\n440.550352\n-6.464314\n-0.0\n\n\n1.003\n2.94476\n74014\n2011-08-25 02:00:00\n-0.721961\n2.640019\n0.949\n441.072265\n-7.503789\n-0.0\n\n\n1.004\n2.89843\n70426\n2011-08-25 03:00:00\n-0.69794\n2.757097\n1.18\n439.112842\n-4.326845\n0.0\n\n\n1.004\n2.71252\n68131\n2011-08-25 04:00:00\n-0.902173\n3.002557\n1.08\n437.440827\n0.11421\n-0.0\n\n\n1.004\n2.89406\n73654\n2011-08-25 05:00:00\n-0.715813\n2.763938\n0.807\n437.643073\n1.68236\n-0.1\n\n\n1.004\n3.1065\n80005\n2011-08-25 06:00:00\n-0.824154\n2.315087\n0.336\n435.357101\n-5.304179\n-0.1\n\n\n1.003\n3.29236\n78831\n2011-08-25 07:00:00\n-0.680414\n2.02755\n0.576\n434.080668\n-11.782328\n0.0\n\n\n1.004\n3.45049\n76311\n2011-08-25 08:00:00\n-0.418169\n2.058053\n1.08\n434.591819\n-12.90159\n0.0\n\n\n1.004\n3.43728\n75513\n2011-08-25 09:00:00\n-0.407522\n2.487763\n1.27\n433.637723\n-8.415747\n0.0\n\n\n1.004\n3.23587\n74375\n2011-08-25 10:00:00\n-0.587603\n2.911467\n1.01\n432.557123\n-3.750324\n0.0\n\n\n1.004\n3.14977\n78652\n2011-08-25 11:00:00\n-0.446334\n2.612638\n0.805\n432.903021\n-2.360952\n0.1\n\n\n…\n…\n…\n…\n…\n…\n…\n…\n…\n…\n\n\n5.41\n0.02643\n25693\n2016-06-29 13:00:00\n0.008422\n-0.085061\n0.0572\n476.984855\n5.561302\n0.1\n\n\n5.41\n0.02693\n25473\n2016-06-29 14:00:00\n0.008344\n-0.085453\n0.0581\n476.161236\n5.520671\n0.0\n\n\n5.41\n0.02741\n25243\n2016-06-29 15:00:00\n0.008148\n-0.085924\n0.0588\n475.239525\n5.499484\n0.0\n\n\n5.41\n0.02788\n25007\n2016-06-29 16:00:00\n0.007953\n-0.086395\n0.0596\n474.415905\n5.458853\n0.0\n\n\n5.41\n0.02833\n24769\n2016-06-29 17:00:00\n0.007719\n-0.087062\n0.0603\n473.494194\n5.437666\n0.0\n\n\n5.41\n0.02876\n24531\n2016-06-29 18:00:00\n0.007387\n-0.08771\n0.061\n472.690019\n5.495126\n-0.0\n\n\n5.41\n0.02917\n24295\n2016-06-29 19:00:00\n0.007114\n-0.088574\n0.0616\n471.768308\n5.473939\n-0.0\n\n\n5.41\n0.02956\n24063\n2016-06-29 20:00:00\n0.006723\n-0.089516\n0.0622\n470.944688\n5.433308\n-0.0\n\n\n5.41\n0.02992\n23837\n2016-06-29 21:00:00\n0.006411\n-0.090575\n0.0628\n470.022977\n5.412121\n-0.0\n\n\n5.41\n0.03026\n23619\n2016-06-29 22:00:00\n0.006001\n-0.091615\n0.0634\n469.101266\n5.390934\n-0.0\n\n\n5.41\n0.03057\n23409\n2016-06-29 23:00:00\n0.005552\n-0.092852\n0.064\n468.297091\n5.448394\n-0.0\n\n\n5.41\n0.03085\n23217\n2016-06-30 00:00:00\n0.005085\n-0.094089\n0.0645\n467.395876\n5.435565\n-0.1"
  },
  {
    "objectID": "analysis/20_model.html#compare-juno-data-with-model",
    "href": "analysis/20_model.html#compare-juno-data-with-model",
    "title": "Models",
    "section": "Compare JUNO data with model",
    "text": "Compare JUNO data with model\nWe are using juno 1min data to compare with model data\n\n\nCode\nfrom ids_finder.utils.basic import pl_norm, resample\n\n\n\n\nCode\nfrom ids_finder.pipelines.juno.pipeline import download_juno_data, preprocess_jno\n\n\n\n\nCode\ndef create_jno_data_pipeline(**kwargs) -&gt; Pipeline:\n    nodes = [\n        node(download_juno_data, inputs=None, outputs=\"raw_jno_ss_se_1min\", name=\"download_JUNO_data_1min\"),\n        node(preprocess_jno, inputs=\"raw_jno_ss_se_1min\", outputs=\"preprocessed_jno_ss_se_1min\", name=\"preprocess_JUNO_node_1min\",),\n    ]\n    return pipeline(nodes, namespace=\"model\")\n\n\n\n\nload jno data and resample to 1h to match model resolution\npreprocessed_jno_ss_se_1min: pl.DataFrame = catalog.load('preprocessed_jno_ss_se_1min')\njno_ss_se_1min = preprocessed_jno_ss_se_1min.lazy().rename(\n    {\"BX SE\": \"br\", \"BY SE\": \"bt\", \"BZ SE\": \"bn\"}\n)\n\njno_ss_se_1h = jno_ss_se_1min.pipe(resample, every=\"1h\", period='2h')\n\n\n\n\nCode\nimport plotly.graph_objects as go;\nfrom plotly_resampler import register_plotly_resampler\nfrom plotly_resampler import FigureResampler\nimport plotly.express as px\n\n\n\n\nCode\njno_mswim2d_1h = processed_jno_mswim2d.lazy()\n\n\n\n\nCode\ndef _tf(df: pl.DataFrame):\n    \"temporal function to select interesting columns and add norm\"\n    cols = [\"time\", \"br\", \"bt\", \"bn\"]\n\n    return df.select(cols).with_columns(\n        b=pl_norm(bcols_rtn),\n    )\n\n\njno_joint_1h_wide: pl.DataFrame = (\n    jno_ss_se_1h.pipe(_tf).join(\n        jno_mswim2d_1h.pipe(_tf),\n        on=\"time\",\n        suffix=\"_model\",\n    )\n).collect()\n\njno_joint_1h_long = pl.concat(\n    [\n        jno_ss_se_1h.pipe(_tf).with_columns(type=pl.lit(\"1h\")),\n        jno_mswim2d_1h.pipe(_tf).with_columns(type=pl.lit(\"1h_model\")),\n    ]\n).collect()\n\n\n\nData Porfiling\nResults are showed in the following links\nTimeseries Report Result\nComparison Report Result\n\n\nCode\nfrom ydata_profiling import ProfileReport, compare\n\n\n\n\nCode\nfrom fastcore.utils import threaded\n\n\n\n\nCode\n@threaded\ndef get_report_t(df: pl.DataFrame, output, **kwargs):\n    '''get report and save to file in a thread\n    '''\n    get_report(df, **kwargs).to_file(output)\n    return output\n\ndef get_report(df: pl.DataFrame, **kwargs):\n    return ProfileReport(\n        df.to_pandas().set_index(\"time\"), **kwargs\n    )\n\ndef get_comparison_report(df: pl.DataFrame, compare_col=None, tsmode=False, **kwargs):\n    \n    dfs_dict: Dict[str, pl.DataFrame] = df.partition_by(compare_col, as_dict=True)\n    \n    if tsmode:\n        raise NotImplementedError(\"tsmode for comparison is not implemented yet in `ydata_profiling`\")\n        # UnionMatchError: can not match type \"list\" to any type of \"time_index_analysis.period\" union: typing.Union[float,  \n        \n        # Notes: for `tsmode`, we need to match the time first\n        # select common timestamps\n        from functools import reduce\n        basetimestamps = reduce(np.intersect1d, [df.get_column('time') for df in dfs_dict.values()])\n        dfs_dict = {\n            k: df.filter(pl.col(\"time\").is_in(basetimestamps))\n            for k, df in dfs_dict.items()\n        }\n\n        for k, df in dfs_dict.items():\n            logger.info(f\"{k}: {len(df)}\")\n    \n    comparison_report = compare(\n        [get_report(df, title=k, **kwargs) for k, df in dfs_dict.items()]\n    )\n    \n    # Obtain merged statistics\n    comparison_report.get_description()\n\n    return comparison_report\n\n\n\n\nCode\nget_report_t(\n    jno_joint_1h_wide,\n    output=\"jno_model_ts.html\",\n    tsmode=True,\n    title=\"JUNO Model Timeseries Report\",\n)\n\n\n\n\nCode\nget_comparison_report(jno_joint_1h_long, compare_col=\"type\").to_file(\n    \"jno_model_comparison.html\"\n)"
  },
  {
    "objectID": "analysis/20_model.html#validation",
    "href": "analysis/20_model.html#validation",
    "title": "Models",
    "section": "Validation",
    "text": "Validation\n\nConnect python with R kernel\n\n\nCode\nfrom ids_finder.utils.r import py2rpy_polars\nconv_pl = py2rpy_polars()\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(viridis)\n\nlibrary(glue)\nlibrary(arrow)\n\n\n\n\nCode\nimport warnings\n\n\n\n\nCode\nfig = px.line(\n    jno_joint_1h_long.sort(\"time\"),\n    x=\"time\",\n    y=\"b\",\n    color=\"type\",\n)\nfig\n\n\n\n\nCompare directly with scatter plot\n\n\nCode\ndef bb_jointplot(data):\n    g = sns.jointplot(\n        x = 'b',\n        y = 'b_model',\n        data = data,\n        kind = 'hist',\n    )\n    return g\n\n\n\n\nCode\ng = bb_jointplot(jno_joint_1h_wide)\ng.ax_marg_x.set_xlim(0, 5)\ng.ax_marg_y.set_ylim(0, 5)\n\n\n\n\nCode\np1 &lt;- ggplot(jno_joint_1h_wide, aes(x=b, y=b_model) ) +\n  geom_bin2d() +\n  geom_density_2d( colour=\"white\" ) +\n  scale_fill_continuous(trans=\"log\", type = \"viridis\") +\n  stat_regline_equation() + \n  xlim(-0.1, 10) +  # Set x-axis limits\n  ylim(-0.1, 10) +  # Set y-axis limits\n  theme_pubr(legend = 'right')\n  # theme(legend.position = c(0.8,0.8))\n  # stat_density_2d(aes(fill = ..level..), geom = \"polygon\", colour=\"white\")\n\np1\n\n\n\nTest: remove outliers\n\n\nCode\nfrom pyod.models.ecod import ECOD\n\n\n\n\nCode\ndata = jno_joint_1h_wide[['b', 'b_model']]\n\nclf = ECOD()\nclf.fit(data)\n\n\n\n\nCode\ny_train_scores = clf.decision_scores_  # raw outlier scores on the train data\ny_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n\n\n\n\nCode\nbb_jointplot(jno_joint_1h_wide.filter(y_train_pred==1))\ndata = jno_joint_1h_wide.filter(y_train_pred==0)\ng = sns.jointplot(\n    x = 'b',\n    y = 'b_model',\n    data = data,\n    kind = 'hist',\n)\n\ng.plot_joint(sns.kdeplot, color=\"r\", zorder=0, levels=6)\n\n\n\n\nCode\n# def create_pipeline(**kwargs) -&gt; Pipeline:\n    # return create_jno_model_pipeline(**kwargs) + create_jno_data_pipeline(**kwargs)"
  },
  {
    "objectID": "missions/artemis.html",
    "href": "missions/artemis.html",
    "title": "IDs from ARTHEMIS",
    "section": "",
    "text": "ARTEMIS spacecrafts will be exposed in the solar wind at 1 AU during its orbits around the Moon. So it’s very interesting to look into its data.\n\nFor time inteval for THEMIS-B in solar wind, see Link\nFor time inteval for THEMIS-C in solar wind, see Link"
  },
  {
    "objectID": "missions/artemis.html#background",
    "href": "missions/artemis.html#background",
    "title": "IDs from ARTHEMIS",
    "section": "",
    "text": "ARTEMIS spacecrafts will be exposed in the solar wind at 1 AU during its orbits around the Moon. So it’s very interesting to look into its data.\n\nFor time inteval for THEMIS-B in solar wind, see Link\nFor time inteval for THEMIS-C in solar wind, see Link"
  },
  {
    "objectID": "missions/artemis.html#setup",
    "href": "missions/artemis.html#setup",
    "title": "IDs from ARTHEMIS",
    "section": "Setup",
    "text": "Setup\nNeed to run command in shell first as pipeline is project-specific command\nkedro pipeline create themis\nTo get candidates data, run kedro run --from-inputs=jno.feature_1s --to-outputs=candidates.jno_1s\n\nKerdo\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\n\n\n\nCode\ncatalog = load_catalog()\n\njno_start_date = catalog.load(\"params:jno_start_date\")\njno_end_date = catalog.load(\"params:jno_end_date\")\ntrange = [jno_start_date, jno_end_date]"
  },
  {
    "objectID": "missions/artemis.html#magnetic-field-data-pipeline",
    "href": "missions/artemis.html#magnetic-field-data-pipeline",
    "title": "IDs from ARTHEMIS",
    "section": "Magnetic field data pipeline",
    "text": "Magnetic field data pipeline\n\nFor convenience, we choose magnetic field data in GSE coordinate system\nThe fgs data are in 3-4s resolution\n\n\nDownloading data\n\nsource\n\n\ncheck_dataype\n\n check_dataype (ts:int)\n\n\nsource\n\n\nload_mag_data\n\n load_mag_data (start:str, end:str, ts:int=None, probe:str='b',\n                coord='gse')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart\nstr\n\n\n\n\nend\nstr\n\n\n\n\nts\nint\nNone\ntime resolution\n\n\nprobe\nstr\nb\n\n\n\ncoord\nstr\ngse\n\n\n\n\n\nsource\n\n\nspz2df\n\n spz2df (raw_data:speasy.products.variable.SpeasyVariable)\n\n\nsource\n\n\ndownload_mag_data\n\n download_mag_data (trange, probe:str='b', datatype='fgs', coord='gse')\n\n\n\nPreprocessing data\n\nsource\n\n\npreprocess_mag_data\n\n preprocess_mag_data (raw_data:polars.lazyframe.frame.LazyFrame,\n                      datatype:str=None, coord:str='gse')\n\nPreprocess the raw dataset (only minor transformations)\n\nApplying naming conventions for columns\nDropping duplicate time\nChanging storing format to parquet\n\n\n\nProcessing data\n\nsource\n\n\nprocess_mag_data\n\n process_mag_data (raw_data:polars.lazyframe.frame.LazyFrame, ts:int=None)\n\nPartitioning data, for the sake of memory\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nLazyFrame\n\n\n\n\nts\nint\nNone\ntime resolution\n\n\nReturns\nUnion\n\n\n\n\n\n\n\nPipeline\n\n\nCode\ndef create_mag_data_pipeline(\n    sat_id: str,  # satellite id, used for namespace\n    ts: int = 1,  # time resolution, in seconds\n    tau: str = \"60s\",  # time window\n    **kwargs,\n) -&gt; Pipeline:\n    \n    datatype = check_dataype(\n        ts\n    )  # get the datatype from the time resolution, which is reasonable but not always is the case\n    ts_str = f\"ts_{ts}s\"\n\n    node_load_data = node(\n        load_mag_data,\n        inputs=dict(\n            start=\"params:start_date\",\n            end=\"params:end_date\",\n            ts=\"params:mag.time_resolution\",\n        ),\n        outputs=f\"raw_mag\",\n        name=f\"load_{sat_id.upper()}_magnetic_field_data\",\n    )\n\n    node_preprocess_data = node(\n        preprocess_mag_data,\n        inputs=dict(\n            raw_data=f\"raw_mag\",\n            datatype=datatype,\n        ),\n        outputs=f\"inter_mag_{datatype}\",\n        name=f\"preprocess_{sat_id.upper()}_magnetic_field_data\",\n    )\n\n    node_process_data = node(\n        process_mag_data,\n        inputs=dict(\n            raw_data=f\"inter_mag_{datatype}\",\n            ts=\"params:mag.time_resolution\",\n        ),\n        outputs=f\"primary_mag_{ts_str}\",\n        name=f\"process_{sat_id.upper()}_magnetic_field_data\",\n    )\n\n    node_extract_features = node(\n        extract_features,\n        inputs=[f\"primary_mag_{ts_str}\", \"params:tau\", \"params:mag\"],\n        outputs=f\"feature_{ts_str}_tau_{tau}\",\n        name=f\"extract_{sat_id}_features\",\n    )\n\n    nodes = [\n        node_load_data,\n        node_preprocess_data,\n        node_process_data,\n        node_extract_features,\n    ]\n\n    pipelines = pipeline(\n        nodes,\n        namespace=sat_id,\n        parameters={\n            \"params:start_date\": \"params:jno_start_date\",\n            \"params:end_date\": \"params:jno_end_date\",\n            \"params:tau\": \"params:tau\",\n        },\n    )\n\n    return pipelines\n\n\n\nsource\n\n\ncreate_mag_data_pipeline\n\n create_mag_data_pipeline (sat_id:str, ts:int=1, tau:str='60s', **kwargs)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\nstr\n\nsatellite id, used for namespace\n\n\nts\nint\n1\ntime resolution, in seconds\n\n\ntau\nstr\n60s\ntime window\n\n\nkwargs\n\n\n\n\n\nReturns\nPipeline\n\n\n\n\n\n\nTest\n\n\nCode\nts = \"4s\"\nthb_inter_mag = catalog.load(f\"thb.inter_mag_{ts}\")\nthb_inter_mag.columns\n\n\n\n\nCode\nfrom ids_finder.utils.basic import check_fgm"
  },
  {
    "objectID": "missions/artemis.html#state-data-pipeline",
    "href": "missions/artemis.html#state-data-pipeline",
    "title": "IDs from ARTHEMIS",
    "section": "State data pipeline",
    "text": "State data pipeline\nWe use low resolution OMNI data for plasma state data, see details.\n\nData gaps were filled with dummy numbers for the missing hours or entire days to make all files of equal length. The character ‘9’ is used to fill all fields for missing data according to their format, e.g. ’ 9999.9’ for a field with the FORTRAN format F7.1. Note that format F7.1 below really means (1X,F6.1),etc.\n\nThe flow OMNI \"phi\" angle is opposite GSE \"phi\" angle, threrfore, Flow-vector cartesian components in GSE coordinates may be derived from the given speed and angles as\n\nVx = - V * cos(theta) * cos(phi)\nVy = + V * cos(theta) * sin(phi)\nVz = + V * sin(theta)\nand vise versa: two angles may be derived from the given speed and Vx,Vy,Vz comp. as  \n          a_theta=vz/V\n          theta=(180.*asin(a_theta))/!PI\n         a_phi=Vy/(-Vx)\n        phi=(180.*atan(a_phi))/!PI\n   (*)   Quasi-GSE for the flow longitude angle means the angle increases from zero\n         to positive values as the flow changes from being aligned along the -X(GSE)\n         axis towards the +Y(GSE) axis.  The flow longitude angle is positive for \n         flow from west of the sun, towards +Y(GSE).\n         The flow latitude angle is positive for flow from south of the sun, \n         towards +Z(GSE)\n\nDownloading data\n\nsource\n\n\nload_state_data\n\n load_state_data (start:str=None, end:str=None, ts:str=None,\n                  vars:dict=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart\nstr\nNone\n\n\n\nend\nstr\nNone\n\n\n\nts\nstr\nNone\ntime resolution\n\n\nvars\ndict\nNone\n\n\n\n\n\nsource\n\n\ndownload_state_data\n\n download_state_data (start:str=None, end:str=None, ts:str=None,\n                      probe:str=None, coord:str=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart\nstr\nNone\n\n\n\nend\nstr\nNone\n\n\n\nts\nstr\nNone\ntime resolution\n\n\nprobe\nstr\nNone\n\n\n\ncoord\nstr\nNone\n\n\n\n\n\n\nPreprocessing data\n\nsource\n\n\npreprocess_state_data\n\n preprocess_state_data (raw_data:polars.lazyframe.frame.LazyFrame,\n                        vars:dict)\n\nPreprocess the raw dataset (only minor transformations)\n\nApplying naming conventions for columns\nExtracting variables from CDF files, and convert them to DataFrame\n\nAlso we have additional data file that indicate if THEMIS is in solar wind or not.\n\nsource\n\n\npreprocess_sw_state_data\n\n preprocess_sw_state_data (raw_data:pandas.core.frame.DataFrame)\n\n\nApplying naming conventions for columns\nParsing and typing data (like from string to datetime for time columns)\n\n\n\nProcessing data\n\nsource\n\n\nprocess_state_data\n\n process_state_data (df:polars.lazyframe.frame.LazyFrame,\n                     state:polars.lazyframe.frame.LazyFrame=None)\n\n\nTransforming data to GSE coordinate system\nCombine state data with additional plasma state data\n\n\nsource\n\n\nadd_state\n\n add_state (l_df:polars.lazyframe.frame.LazyFrame,\n            l_state:polars.lazyframe.frame.LazyFrame)\n\n\nsource\n\n\nfilter_tranges\n\n filter_tranges (time:polars.series.series.Series,\n                 tranges:Tuple[list,list])\n\n\nFilter data by time ranges, return the indices of the time that are in the time ranges\n\n\nsource\n\n\nflow2gse\n\n flow2gse (df:polars.lazyframe.frame.LazyFrame)\n\n\nTransforming solar wind data from Quasi-GSE coordinate to GSE coordinate system\n\n\n\nPipelines\n\nsource\n\n\ncreate_state_data_pipeline\n\n create_state_data_pipeline (sat_id, ts:str='1h')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\n\n\n\n\n\nts\nstr\n1h\ntime resolution\n\n\nReturns\nPipeline\n\n\n\n\n\n\n\nCode\ncatalog.load(\"thb.primary_state_1h\").collect().describe()\n# catalog.load('thb.feature_tau_60s').collect()"
  },
  {
    "objectID": "missions/artemis.html#processing-the-whole-data",
    "href": "missions/artemis.html#processing-the-whole-data",
    "title": "IDs from ARTHEMIS",
    "section": "Processing the whole data",
    "text": "Processing the whole data\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='thb', tau=60, ts_state='1h', ts_mag=1)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\nstr\nthb\n\n\n\ntau\nint\n60\ntime window, in seconds\n\n\nts_state\nstr\n1h\ntime resolution of state data\n\n\nts_mag\nint\n1\ntime resolution of mag data, in seconds\n\n\nReturns\nPipeline\n\n\n\n\n\n\n\nCode\n# catalog.load('thb.inter_mag_4s').collect().describe()\n# catalog.load('thb.primary_state_1h').collect().describe()"
  },
  {
    "objectID": "missions/artemis.html#obsolete-codes",
    "href": "missions/artemis.html#obsolete-codes",
    "title": "IDs from ARTHEMIS",
    "section": "Obsolete codes",
    "text": "Obsolete codes\n\nCheck and preprocess the data\nAs we are only interested in the data when THEMIS is in the solar wind, for simplicity we will only keep the data when X, SSE and X, GSE is positive.\n\nState data time resolution is 1 minute…\nFGS data time resolution is 4 second…\n\n\n\nCode\ndef get_thm_state(sat):\n    sat_pos_sse_files = f\"../data/{sat}_pos_sse.parquet\"\n    sat_pos_sse = pl.scan_parquet(sat_pos_sse_files).set_sorted(\"time\")\n    sat_pos_gse_files = f\"../data/{sat}_pos_gse.parquet\"\n    sat_pos_gse = pl.scan_parquet(sat_pos_gse_files).set_sorted(\"time\")\n    sat_state = sat_pos_sse.join(sat_pos_gse, on=\"time\", how=\"inner\")\n    return sat_state\n\n\n\n\nCode\ndf = (\n    sat_state_sw.upsample(\"time\", every=\"1m\")\n    .group_by_dynamic(\"time\", every=\"1d\")\n    .agg(pl.col(\"X, SSE\").null_count().alias(\"null_count\"))\n    .with_columns(\n        pl.when(pl.col(\"null_count\") &gt; 720).then(0).otherwise(1).alias(\"availablity\")\n    )\n)\n\nproperties = {\n    'width': 800,\n}\n\nchart1 = alt.Chart(df).mark_point().encode(\n    x='time',\n    y='null_count'\n).properties(**properties)\n\nchart2  = alt.Chart(df).mark_point().encode(\n    x='time',\n    y='availablity'\n).properties(**properties)\n\n(chart1 & chart2)"
  },
  {
    "objectID": "missions/juno.html",
    "href": "missions/juno.html",
    "title": "IDs from Juno",
    "section": "",
    "text": "Spacecraft-Solar equatorial\nhttps://pds-ppi.igpp.ucla.edu/data/JNO-SS-3-FGM-CAL-V1.0/INDEX/INDEX.TAB\n\n\n\nSE (Solar Equatorial)\n\nCode: se\nResampling options:\n\nNumber of seconds (1 or 60): se_rN[N]s\nResampled 1 hour: se_r1h\n\n\nPC (Planetocentric)\n\nCode: pc\nResampling options:\n\nNumber of seconds (1 or 60): pc_rN[N]s\n\n\nSS (Sun-State)\n\nCode: ss\nResampling options:\n\nNumber of seconds (1 or 60): ss_rN[N]s\n\n\nPL (Payload)\n\nCode: pl\nResampling options:\n\nNumber of seconds (1 or 60): pl_rN[N]s\n\n\n\n------------------------------------------------------------------------------\nJuno Mission Phases                                                           \n------------------------------------------------------------------------------\nStart       Mission                                                           \nDate        Phase                                                             \n==============================================================================\n2011-08-05  Launch                                                            \n2011-08-08  Inner Cruise 1                                                    \n2011-10-10  Inner Cruise 2                                                    \n2013-05-28  Inner Cruise 3                                                    \n2013-11-05  Quiet Cruise                                                      \n2016-01-05  Jupiter Approach                                                  \n2016-06-30  Jupiter Orbital Insertion                                         \n2016-07-05  Capture Orbit                                                     \n2016-10-19  Period Reduction Maneuver                                         \n2016-10-20  Orbits 1-2                                                        \n2016-11-09  Science Orbits                                                    \n2017-10-11  Deorbit\nFile Naming Convention                                                        \n==============================================================================\nConvention:                                                                   \n   fgm_jno_LL_CCYYDDDxx_vVV.ext                                               \nWhere:                                                                        \n   fgm - Fluxgate Magnetometer three character instrument abbreviation        \n   jno - Juno                                                                 \n    LL - CODMAC Data level, for example, l3 for level 3                       \n    CC - The century portion of a date, 20                                    \n    YY - The year of century portion of a date, 00-99                         \n   DDD - The day of year, 001-366                                             \n    xx - Coordinate system of data (se = Solar equatorial, ser = Solar        \n         equatorial resampled, pc = Planetocentric, ss = Sun-State,           \n         pl = Payload)                                                        \n     v - separator to denote Version number                                   \n    VV - version                                                              \n   ext - file extension (sts = Standard Time Series (ASCII) file, lbl = Label \n         file)                                                                \nExample:                                                                      \n   fgm_jno_l3_2014055se_v00.sts    \nThere are three principal coordinate systems used to represent the data in this archive. The SE coordinate system is a Spacecraft- Solar equatorial system and it will be used for cruise data only. The sun-state (ss) and planetocentric (pc) will be used for Earth Fly By (EFB) and Jupiter orbital data. Cartesian representations are used for all three coordinate systems. These coordinate systems are specified relative to a “target body” which may be any solar system object (but for this orbital operations will Jupiter). In what follows we will reference Jupiter as the target body, but, for example, if observations near a satellite (such as Io) are desired in Io-centric coordinates, the satellite Io may be specified as the target body.\nThe SE coordinate system is defined using the sun-spacecraft vector as the primary reference vector; sun’s rotation axis as the secondary reference vector (z). The x axis lies along the sun-spacecraft vector, the z axis is in the plane defined by the Sun’s rotation axis and the spacecraft-sun vector. The y axis completes the system.\nThe ss coordinate system is defined using the instantaneous Jupiter-Sun vector as the primary reference vector (x direction). The X-axis lies along this vector and is taken to be positive toward the Sun. The Jupiter orbital velocity vector is the second vector used to define the coordinate system; the y axis lies in the plane determined by the Jupiter-Sun vector and the velocity vector and is orthogonal to the x axis (very nearly the negative of the velocity vector). The vector cross product of x and y yields a vector z parallel to the northward (upward) normal of the orbit plane of Jupiter. This system is sometimes called a sun-state (ss) coordinate system since its principal vectors are the Sun vector and the Jupiter state vector."
  },
  {
    "objectID": "missions/juno.html#background",
    "href": "missions/juno.html#background",
    "title": "IDs from Juno",
    "section": "",
    "text": "Spacecraft-Solar equatorial\nhttps://pds-ppi.igpp.ucla.edu/data/JNO-SS-3-FGM-CAL-V1.0/INDEX/INDEX.TAB\n\n\n\nSE (Solar Equatorial)\n\nCode: se\nResampling options:\n\nNumber of seconds (1 or 60): se_rN[N]s\nResampled 1 hour: se_r1h\n\n\nPC (Planetocentric)\n\nCode: pc\nResampling options:\n\nNumber of seconds (1 or 60): pc_rN[N]s\n\n\nSS (Sun-State)\n\nCode: ss\nResampling options:\n\nNumber of seconds (1 or 60): ss_rN[N]s\n\n\nPL (Payload)\n\nCode: pl\nResampling options:\n\nNumber of seconds (1 or 60): pl_rN[N]s\n\n\n\n------------------------------------------------------------------------------\nJuno Mission Phases                                                           \n------------------------------------------------------------------------------\nStart       Mission                                                           \nDate        Phase                                                             \n==============================================================================\n2011-08-05  Launch                                                            \n2011-08-08  Inner Cruise 1                                                    \n2011-10-10  Inner Cruise 2                                                    \n2013-05-28  Inner Cruise 3                                                    \n2013-11-05  Quiet Cruise                                                      \n2016-01-05  Jupiter Approach                                                  \n2016-06-30  Jupiter Orbital Insertion                                         \n2016-07-05  Capture Orbit                                                     \n2016-10-19  Period Reduction Maneuver                                         \n2016-10-20  Orbits 1-2                                                        \n2016-11-09  Science Orbits                                                    \n2017-10-11  Deorbit\nFile Naming Convention                                                        \n==============================================================================\nConvention:                                                                   \n   fgm_jno_LL_CCYYDDDxx_vVV.ext                                               \nWhere:                                                                        \n   fgm - Fluxgate Magnetometer three character instrument abbreviation        \n   jno - Juno                                                                 \n    LL - CODMAC Data level, for example, l3 for level 3                       \n    CC - The century portion of a date, 20                                    \n    YY - The year of century portion of a date, 00-99                         \n   DDD - The day of year, 001-366                                             \n    xx - Coordinate system of data (se = Solar equatorial, ser = Solar        \n         equatorial resampled, pc = Planetocentric, ss = Sun-State,           \n         pl = Payload)                                                        \n     v - separator to denote Version number                                   \n    VV - version                                                              \n   ext - file extension (sts = Standard Time Series (ASCII) file, lbl = Label \n         file)                                                                \nExample:                                                                      \n   fgm_jno_l3_2014055se_v00.sts    \nThere are three principal coordinate systems used to represent the data in this archive. The SE coordinate system is a Spacecraft- Solar equatorial system and it will be used for cruise data only. The sun-state (ss) and planetocentric (pc) will be used for Earth Fly By (EFB) and Jupiter orbital data. Cartesian representations are used for all three coordinate systems. These coordinate systems are specified relative to a “target body” which may be any solar system object (but for this orbital operations will Jupiter). In what follows we will reference Jupiter as the target body, but, for example, if observations near a satellite (such as Io) are desired in Io-centric coordinates, the satellite Io may be specified as the target body.\nThe SE coordinate system is defined using the sun-spacecraft vector as the primary reference vector; sun’s rotation axis as the secondary reference vector (z). The x axis lies along the sun-spacecraft vector, the z axis is in the plane defined by the Sun’s rotation axis and the spacecraft-sun vector. The y axis completes the system.\nThe ss coordinate system is defined using the instantaneous Jupiter-Sun vector as the primary reference vector (x direction). The X-axis lies along this vector and is taken to be positive toward the Sun. The Jupiter orbital velocity vector is the second vector used to define the coordinate system; the y axis lies in the plane determined by the Jupiter-Sun vector and the velocity vector and is orthogonal to the x axis (very nearly the negative of the velocity vector). The vector cross product of x and y yields a vector z parallel to the northward (upward) normal of the orbit plane of Jupiter. This system is sometimes called a sun-state (ss) coordinate system since its principal vectors are the Sun vector and the Jupiter state vector."
  },
  {
    "objectID": "missions/juno.html#setup",
    "href": "missions/juno.html#setup",
    "title": "IDs from Juno",
    "section": "Setup",
    "text": "Setup\nNeed to run command in shell first as pipeline is project-specific command\nkedro pipeline create juno\nTo get candidates data, run kedro run --from-inputs=jno.feature_1s --to-outputs=candidates.jno_1s\n\nKerdo\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\ncatalog = load_catalog()\ncatalog.list()\n\n\n[10/20/23 11:56:32] WARNING  KedroDeprecationWarning: 'PartitionedDataset' has been moved to        warnings.py:109\n                             `kedro-datasets` and will be removed in Kedro 0.19.0.                                 \n                                                                                                                   \n\n\n\n                    WARNING  KedroDeprecationWarning: 'AbstractVersionedDataSet' has been renamed   warnings.py:109\n                             to 'AbstractVersionedDataset', and the alias will be removed in Kedro                 \n                             0.19.0                                                                                \n                                                                                                                   \n\n\n\n\n\n\n\n[\n    'thb.raw_state_sw',\n    'sta.raw_state',\n    'JNO_index',\n    'jno.raw_mag_1s',\n    'jno.raw_state',\n    'model.raw_jno_ss_se_1min',\n    'model.preprocessed_jno_ss_se_1min',\n    'parameters',\n    'params:tau',\n    'params:jno_start_date',\n    'params:jno_end_date',\n    'params:jno_1s_params',\n    'params:jno_1s_params.bcols',\n    'params:jno_1s_params.data_resolution',\n    'params:jno.extract_params',\n    'params:jno.extract_params.bcols',\n    'params:jno.extract_params.data_resolution',\n    'params:sta.extract_params',\n    'params:sta.extract_params.bcols',\n    'params:sta.extract_params.data_resolution',\n    'params:thb.mag',\n    'params:thb.mag.coords',\n    'params:thb.extract_params',\n    'params:thb.extract_params.bcols',\n    'params:thb.extract_params.data_resolution',\n    'params:omni_vars',\n    'params:omni_vars.N',\n    'params:omni_vars.N.COLNAME',\n    'params:omni_vars.N.FIELDNAM',\n    'params:omni_vars.N.UNITS',\n    'params:omni_vars.T',\n    'params:omni_vars.T.COLNAME',\n    'params:omni_vars.T.FIELDNAM',\n    'params:omni_vars.T.UNITS',\n    'params:omni_vars.V',\n    'params:omni_vars.V.COLNAME',\n    'params:omni_vars.V.FIELDNAM',\n    'params:omni_vars.V.UNITS',\n    'params:omni_vars.THETA-V',\n    'params:omni_vars.THETA-V.COLNAME',\n    'params:omni_vars.THETA-V.FIELDNAM',\n    'params:omni_vars.THETA-V.UNITS',\n    'params:omni_vars.PHI-V',\n    'params:omni_vars.PHI-V.COLNAME',\n    'params:omni_vars.PHI-V.FIELDNAM',\n    'params:omni_vars.PHI-V.UNITS'\n]"
  },
  {
    "objectID": "missions/juno.html#dataset-overview",
    "href": "missions/juno.html#dataset-overview",
    "title": "IDs from Juno",
    "section": "Dataset Overview",
    "text": "Dataset Overview\n\nIndex\n\n\nCode\npds_dir = \"https://pds-ppi.igpp.ucla.edu/data\"\n\npossible_coords = [\"se\", \"ser\", \"pc\", \"ss\", \"pl\"]\npossible_exts = [\"sts\", \"lbl\"]\npossible_data_rates = [\"1s\", \"1min\", \"1h\"]\n\njuno_ss_config = {\n    \"DATA_SET_ID\": \"JNO-SS-3-FGM-CAL-V1.0\",\n    \"FILE_SPECIFICATION_NAME\": \"INDEX/INDEX.LBL\",\n}\n\njuno_j_config = {\n    \"DATA_SET_ID\": \"JNO-J-3-FGM-CAL-V1.0\",\n    \"FILE_SPECIFICATION_NAME\": \"INDEX/INDEX.LBL\",\n}\n\n\n\nProcess index\n\nsource\n\n\n\nprocess_jno_index\n\n process_jno_index (df:pandas.core.frame.DataFrame)\n\n\nPipleline\n\nsource\n\n\n\ncreate_jno_index_pipeline\n\n create_jno_index_pipeline ()\n\n\n\nCode\nraw_JNO_SS_index = catalog.load('raw_JNO_SS_index')\nraw_JNO_J_index = catalog.load('raw_JNO_J_index')\njno_index = catalog.load('JNO_index')\n\njno_ss_index = jno_index[lambda df: df[\"DATA_SET_ID\"] == \"JNO-SS-3-FGM-CAL-V1.0\"]\njno_j_index  = jno_index[lambda df: df[\"DATA_SET_ID\"] == \"JNO-J-3-FGM-CAL-V1.0\"]\n\n\n                    INFO     Loading data from 'raw_JNO_SS_index' (CSVDataset)...               data_catalog.py:502\n\n\n\n                    INFO     Loading data from 'raw_JNO_J_index' (CSVDataset)...                data_catalog.py:502\n\n\n\n                    INFO     Loading data from 'JNO_index' (ParquetDataset)...                  data_catalog.py:502\n\n\n\n\nCheck the data\n\n\nJNO-SS Starting date: 2011-08-25\nJNO-SS Ending date: 2016-06-29\nJNO-J Starting date: 2016-07-07\nJNO-J Ending date: 2022-12-15\n\n\n\n\nThe following days are missing\n(#2353) ['2016-07-07','2016-07-08','2016-07-09','2016-07-10','2016-07-11','2016-07-12','2016-07-13','2016-07-14','2016-07-15','2016-07-16'...]"
  },
  {
    "objectID": "missions/juno.html#magnetic-field-data",
    "href": "missions/juno.html#magnetic-field-data",
    "title": "IDs from Juno",
    "section": "Magnetic field data",
    "text": "Magnetic field data\n\nDownloading data\n\nsource\n\n\nload_mag_data\n\n load_mag_data (start:str|None=None, end:str|None=None, ts:str='1sec')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart\nstr | None\nNone\n\n\n\nend\nstr | None\nNone\n\n\n\nts\nstr\n1sec\ntime resolution\n\n\nReturns\nDataFrame\n\n\n\n\n\n\nsource\n\n\ndownload_mag_data\n\n download_mag_data (start=None, end=None, ts:str='1sec')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nstart\nNoneType\nNone\n\n\n\nend\nNoneType\nNone\n\n\n\nts\nstr\n1sec\ntime resolution\n\n\nReturns\nlist\n\n\n\n\n\n\n\nPreprocessing data\nConvert all files from lbl format to parquet format for faster processing\n\nsource\n\n\npreprocess_mag_data\n\n preprocess_mag_data (raw_data:polars.dataframe.frame.DataFrame)\n\nPreprocess the raw dataset (only minor transformations)\n\nApplying naming conventions for columns\nParsing and typing data\nChanging storing format (from lbl to parquet)\nDropping useless columns\n\n\n\nProcessing data\n\nsource\n\n\nprocess_mag_data\n\n process_mag_data (raw_data:polars.dataframe.frame.DataFrame, ts:str=None,\n                   coord:str=None)\n\nPartitioning data, for the sake of memory\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nDataFrame\n\n\n\n\nts\nstr\nNone\ntime resolution\n\n\ncoord\nstr\nNone\n\n\n\nReturns\nUnion\n\n\n\n\n\n\n\nPipeline\n\nsource\n\n\ncreate_mag_data_pipeline\n\n create_mag_data_pipeline (sat_id, ts:str='1s', tau:str='60s', **kwargs)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\n\n\n\n\n\nts\nstr\n1s\ntime resolution,\n\n\ntau\nstr\n60s\ntime window\n\n\nkwargs\n\n\n\n\n\nReturns\nPipeline"
  },
  {
    "objectID": "missions/juno.html#state-data-pipeline",
    "href": "missions/juno.html#state-data-pipeline",
    "title": "IDs from Juno",
    "section": "State data pipeline",
    "text": "State data pipeline\n\nDownloading data\nFor interpolated solar wind at JUNO’s location, see model output file.\n\n\nPreprocessing data\nCoordinate System:  HGI\nVariables:\n  Date_Time: date and time in ISO format [UT]\n  hour: elapsed time since trajectory start [hr]\n  r: radial coordinate in HGI [AU]\n  phi: longitude coordinate in HGI [deg]\n  Rho: density [amu/cm^3]\n  Ux, Uy, Uz: bulk velocity components in HGI [km/s]\n  Bx, By, Bz: magnetic field components in HGI [nT]\n  Ti: ion temperature [K]\n\nsource\n\n\npreprocess_state_data\n\n preprocess_state_data (raw_data:pandas.core.frame.DataFrame,\n                        start:str=None, end:str=None, ts:str=None,\n                        coord:str=None)\n\nPreprocess the raw dataset (only minor transformations)\n\nParsing and typing data (like from string to datetime for time columns)\nChanging storing format (like from csv to parquet)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nDataFrame\n\n\n\n\nstart\nstr\nNone\n\n\n\nend\nstr\nNone\n\n\n\nts\nstr\nNone\ntime resolution\n\n\ncoord\nstr\nNone\n\n\n\nReturns\nDataFrame\n\n\n\n\n\n\n\nProcessing data\n\nsource\n\n\nhgi2rtn\n\n hgi2rtn\n          (df:polars.lazyframe.frame.LazyFrame|polars.dataframe.frame.Data\n          Frame)\n\nTransform coordinates from HGI to RTN\n\nsource\n\n\nprocess_state_data\n\n process_state_data (df:polars.dataframe.frame.DataFrame)\n\nCorresponding to primary data layer, where source data models are transformed into domain data models\n\nTransforming data to RTN (Radial-Tangential-Normal) coordinate system\nApplying naming conventions for columns\n\n\n\nPipeline\n\nsource\n\n\ncreate_state_data_pipeline\n\n create_state_data_pipeline (sat_id, ts:str='1h')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\n\n\n\n\n\nts\nstr\n1h\ntime resolution\n\n\nReturns\nPipeline"
  },
  {
    "objectID": "missions/juno.html#processing-the-whole-data",
    "href": "missions/juno.html#processing-the-whole-data",
    "title": "IDs from Juno",
    "section": "Processing the whole data",
    "text": "Processing the whole data\n\nsource\n\ncreate_pipeline\n\n create_pipeline (sat_id='jno', tau='60s', ts_mag='1s', ts_state='1h')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsat_id\nstr\njno\n\n\n\ntau\nstr\n60s\n\n\n\nts_mag\nstr\n1s\ntime resolution of magnetic field data\n\n\nts_state\nstr\n1h\ntime resolution of state data\n\n\nReturns\nPipeline"
  },
  {
    "objectID": "missions/juno.html#test",
    "href": "missions/juno.html#test",
    "title": "IDs from Juno",
    "section": "Test",
    "text": "Test\n\n\nCode\njno_ss_se_1s = catalog.load('primary_jno_ss_se_1s')\njno_1s_params = catalog.load('params:jno_1s_params')\ncandidates_jno_ss_se_1s = catalog.load('candidates_jno_ss_se_1s')\n\n\n\nEstimate\n1 day of data resampled by 1 sec is about 12 MB.\nSo 1 year of data is about 4 GB, and 6 years of JUNO Cruise data is about 24 GB.\nDownloading rate is about 250 KB/s, so it will take about 3 days to download all the data.\n\n\nCode\nnum_of_files = 6*365\njno_file_size = 12e3\nthm_file_size = 40e3\nfiles_size = jno_file_size + thm_file_size\ndownloading_rate = 250\nprocessing_rate = 1/60\n\ntime_to_download = num_of_files * files_size / downloading_rate / 60 / 60\nspace_required = num_of_files * files_size / 1e6\ntime_to_process = num_of_files / processing_rate / 60 / 60\n\nprint(f\"Time to download: {time_to_download:.2f} hours\")\nprint(f\"Disk space required: {space_required:.2f} GB\")\nprint(f\"Time to process: {time_to_process:.2f} hours\")\n\n\nTime to download: 126.53 hours\nDisk space required: 113.88 GB\nTime to process: 36.50 hours"
  },
  {
    "objectID": "missions/wind.html",
    "href": "missions/wind.html",
    "title": "IDs from Wind",
    "section": "",
    "text": "Need to run command in shell first as pipeline is project-specific command\nkedro pipeline create wind\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\n\n\n\nCode\ncatalog = load_catalog('../..')\n\njno_start_date = catalog.load(\"params:jno_start_date\")\njno_end_date = catalog.load(\"params:jno_end_date\")\ntrange = [jno_start_date, jno_end_date]\n\n\n[10/28/23 09:23:14] WARNING  /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ke logger.py:205\n                             dro/io/partitioned_dataset.py:200: KedroDeprecationWarning:                           \n                             'PartitionedDataset' has been moved to `kedro-datasets` and will be                   \n                             removed in Kedro 0.19.0.                                                              \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    WARNING  /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ke logger.py:205\n                             dro_datasets/polars/lazy_polars_dataset.py:14: KedroDeprecationWarning:               \n                             'AbstractVersionedDataSet' has been renamed to                                        \n                             'AbstractVersionedDataset', and the alias will be removed in Kedro                    \n                             0.19.0                                                                                \n                               from kedro.io.core import (                                                         \n                                                                                                                   \n\n\n\n                    INFO     Loading data from 'params:jno_start_date' (MemoryDataset)...       data_catalog.py:502\n\n\n\n                    INFO     Loading data from 'params:jno_end_date' (MemoryDataset)...         data_catalog.py:502"
  },
  {
    "objectID": "missions/wind.html#setup",
    "href": "missions/wind.html#setup",
    "title": "IDs from Wind",
    "section": "",
    "text": "Need to run command in shell first as pipeline is project-specific command\nkedro pipeline create wind\n\n\nCode\nfrom ids_finder.utils.basic import load_catalog\n\n\n\n\nCode\ncatalog = load_catalog('../..')\n\njno_start_date = catalog.load(\"params:jno_start_date\")\njno_end_date = catalog.load(\"params:jno_end_date\")\ntrange = [jno_start_date, jno_end_date]\n\n\n[10/28/23 09:23:14] WARNING  /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ke logger.py:205\n                             dro/io/partitioned_dataset.py:200: KedroDeprecationWarning:                           \n                             'PartitionedDataset' has been moved to `kedro-datasets` and will be                   \n                             removed in Kedro 0.19.0.                                                              \n                               warnings.warn(                                                                      \n                                                                                                                   \n\n\n\n                    WARNING  /Users/zijin/miniforge3/envs/cool_planet/lib/python3.10/site-packages/ke logger.py:205\n                             dro_datasets/polars/lazy_polars_dataset.py:14: KedroDeprecationWarning:               \n                             'AbstractVersionedDataSet' has been renamed to                                        \n                             'AbstractVersionedDataset', and the alias will be removed in Kedro                    \n                             0.19.0                                                                                \n                               from kedro.io.core import (                                                         \n                                                                                                                   \n\n\n\n                    INFO     Loading data from 'params:jno_start_date' (MemoryDataset)...       data_catalog.py:502\n\n\n\n                    INFO     Loading data from 'params:jno_end_date' (MemoryDataset)...         data_catalog.py:502"
  },
  {
    "objectID": "missions/wind.html#magnetic-field-data-pipeline",
    "href": "missions/wind.html#magnetic-field-data-pipeline",
    "title": "IDs from Wind",
    "section": "Magnetic field data pipeline",
    "text": "Magnetic field data pipeline\n\nFor convenience, we choose magnetic field data in GSE coordinate system\nThe fgs data are in 3-4s resolution\n\n\nsource\n\nload_mag_data\n\n load_mag_data (start:str=None, end:str=None, trange:list[str]=None,\n                datatype='h4-rtn')\n\n\nsource\n\n\ndownload_mag_data\n\n download_mag_data (trange:list[str], datatype)\n\n\n\nCode\ndef preprocess_mag_data(\n    raw_data: pl.LazyFrame,\n) -&gt; pl.LazyFrame:\n    \"\"\"\n    Preprocess the raw dataset (only minor transformations)\n\n    - Downsample the data to a given time resolution\n    - Applying naming conventions for columns\n    \"\"\"\n    name_mapping = {\n        \"BRTN_0\": \"B_x\",\n        \"BRTN_1\": \"B_y\",\n        \"BRTN_2\": \"B_z\",\n        \"BF1\": \"B_mag\",\n    }\n\n    return raw_data.rename(name_mapping)\n\n\n\nsource\n\n\nprocess_mag_data\n\n process_mag_data (raw_data:polars.lazyframe.frame.LazyFrame, ts:str=None,\n                   coord:str=None)\n\nCorresponding to primary data layer, where source data models are transformed into domain data models\n\nPartitioning data, for the sake of memory\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nraw_data\nLazyFrame\n\n\n\n\nts\nstr\nNone\ntime resolution\n\n\ncoord\nstr\nNone"
  },
  {
    "objectID": "00_ids_finder.html",
    "href": "00_ids_finder.html",
    "title": "Finding magnetic discontinuities",
    "section": "",
    "text": "Smoothing\nInterpolating"
  },
  {
    "objectID": "00_ids_finder.html#processing-stages",
    "href": "00_ids_finder.html#processing-stages",
    "title": "Finding magnetic discontinuities",
    "section": "",
    "text": "Smoothing\nInterpolating"
  },
  {
    "objectID": "00_ids_finder.html#id-identification-limited-feature-extraction-anomaly-detection",
    "href": "00_ids_finder.html#id-identification-limited-feature-extraction-anomaly-detection",
    "title": "Finding magnetic discontinuities",
    "section": "ID identification (limited feature extraction / anomaly detection)",
    "text": "ID identification (limited feature extraction / anomaly detection)\nThe first index is \\[ \\frac{\\sigma(B)}{Max(\\sigma(B_-),\\sigma(B_+))} \\] The second index is \\[ \\frac{\\sigma(B_- + B_+)} {\\sigma(B_-) + \\sigma(B_+)} \\] The ﬁrst two conditions guarantee that the ﬁeld changes of the IDs identiﬁed are large enough to be distinguished from the stochastic ﬂuctuations on magnetic ﬁelds, while the third is a supplementary condition toreduce the uncertainty of recognition.\nthird index (relative field jump) is \\[ \\frac{| \\Delta \\vec{B} |}{|B_{bg}|} \\] a supplementary condition to reduce the uncertainty of recognition\n\nsource\n\npl_dvec\n\n pl_dvec (columns, *more_columns)\n\n\nsource\n\n\npl_format_time\n\n pl_format_time\n                 (df:polars.lazyframe.frame.LazyFrame|polars.dataframe.fra\n                 me.DataFrame, tau:datetime.timedelta)\n\n\nsource\n\n\ncompute_combinded_std\n\n compute_combinded_std\n                        (df:polars.dataframe.frame.DataFrame|polars.lazyfr\n                        ame.frame.LazyFrame, tau, cols)\n\n\nsource\n\n\ncompute_std\n\n compute_std\n              (df:polars.dataframe.frame.DataFrame|polars.lazyframe.frame.\n              LazyFrame, tau, b_cols=['BX', 'BY', 'BZ'])\n\n\n\nIndex of the standard deviation\n[11/06/23 20:12:59] WARNING UserWarning: potentially wrong warnings.py:109 underline length…\nInputs: &lt;LazyFrame, object&gt;\n—————————- in\nMultiply dispatched method:\ncompute_index_std\n…\n                WARNING  UserWarning: Unknown section        warnings.py:109\n                         Inputs: &lt;lazyframe, Object&gt;                        \n                                                                            \n                WARNING  UserWarning: Unknown section        warnings.py:109\n                         Examples                                           \n                                                                            \n                WARNING  UserWarning: Unknown section Notes  warnings.py:109\n                                                                            \n\n\n\ncompute_index_std\n\n compute_index_std (*args, **kwargs)\n\nMultiply dispatched method: compute_index_std\n\n\nIndex of fluctuation\n\n\nIndex of the relative field jump\n\nsource\n\n\ncompute_index_diff\n\n compute_index_diff (df:polars.dataframe.frame.DataFrame,\n                     tau:datetime.timedelta, cols)\n\n\nsource\n\n\ncompute_indices\n\n compute_indices (df:polars.dataframe.frame.DataFrame,\n                  tau:datetime.timedelta, bcols:list[str]=['BX', 'BY',\n                  'BZ'])\n\nCompute all index based on the given DataFrame and tau value.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\nDataFrame\n\nInput DataFrame.\n\n\ntau\ntimedelta\n\nTime interval value.\n\n\nbcols\nlist\n[‘BX’, ‘BY’, ‘BZ’]\n\n\n\nReturns\nDataFrame\n\nTuple containing DataFrame results for fluctuation index,standard deviation index, and ‘index_num’."
  },
  {
    "objectID": "00_ids_finder.html#id-parameters-full-feature-extraction",
    "href": "00_ids_finder.html#id-parameters-full-feature-extraction",
    "title": "Finding magnetic discontinuities",
    "section": "ID parameters (full feature extraction)",
    "text": "ID parameters (full feature extraction)\n\nUtils\n\nsource\n\n\nget_candidates\n\n get_candidates (candidates:modin.pandas.dataframe.DataFrame,\n                 candidate_type=None, num:int=4)\n\n                WARNING  UserWarning: potentially wrong      warnings.py:109\n                         underline length...                                \n                         Inputs: &lt;object, DataFrame&gt;                        \n                         ---------------------------- in                    \n                         Multiply dispatched method:                        \n                         get_candidate_data                                 \n                         ...                                                \n                                                                            \n                WARNING  UserWarning: Unknown section        warnings.py:109\n                         Inputs: &lt;object, Dataframe&gt;                        \n                                                                            \n\n\n\nget_candidate_data\n\n get_candidate_data (*args, **kwargs)\n\nMultiply dispatched method: get_candidate_data\n\n\n\nget_candidate_data\n\n get_candidate_data (*args, **kwargs)\n\nMultiply dispatched method: get_candidate_data\n\n\nDuration\nDefinitions of duration - Define $d^* = ( | dB / dt | ) $, and then define time interval where \\(| dB/dt |\\) decreases to \\(d^*/4\\)\n\nsource\n\n\nget_time_from_condition\n\n get_time_from_condition (vec:xarray.core.dataarray.DataArray, threshold,\n                          condition_type)\n\n\nsource\n\n\nfind_start_end_times\n\n find_start_end_times (vec_diff_mag:xarray.core.dataarray.DataArray,\n                       d_time, threshold)\n\n\nsource\n\n\ncalc_d_duration\n\n calc_d_duration (vec:xarray.core.dataarray.DataArray, d_time, threshold)\n\n\nsource\n\n\ncalc_duration\n\n calc_duration (vec:xarray.core.dataarray.DataArray, threshold_ratio=0.25)\n\n\nsource\n\n\ncalc_candidate_duration\n\n calc_candidate_duration (candidate:modin.pandas.series.Series, data)\n\n\nCalibrates candidate duration\n\nsource\n\n\n\ncalibrate_candidate_duration\n\n calibrate_candidate_duration (candidate:modin.pandas.series.Series,\n                               data:xarray.core.dataarray.DataArray,\n                               data_resolution, ratio=0.75)\n\nCalibrates the candidate duration. - If only one of ‘d_tstart’ or ‘d_tstop’ is provided, calculates the missing one based on the provided one and ‘d_time’. - Then if this is not enough points between ‘d_tstart’ and ‘d_tstop’, returns None for both.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncandidate\nSeries\n\n\n\n\ndata\nDataArray\n\n\n\n\ndata_resolution\n\n\n\n\n\nratio\nfloat\n0.75\n\n\n\nReturns\n- pd.Series: The calibrated candidate.\n\n\n\n\n\n\n\nminimum variance analysis (MVA) features\nTo ensure the accuracy of MVA, only when the ratio of the middle to the minimum eigenvalue (labeled QMVA for simplicity) is larger than 3 are the results used for further analysis.\n\n\nCode\ndef minvar(data):\n    \"\"\"\n    see `pyspedas.cotrans.minvar`\n    This program computes the principal variance directions and variances of a\n    vector quantity as well as the associated eigenvalues.\n\n    Parameters\n    -----------\n    data:\n        Vxyz, an (npoints, ndim) array of data(ie Nx3)\n\n    Returns\n    -------\n    vrot:\n        an array of (npoints, ndim) containing the rotated data in the new coordinate system, ijk.\n        Vi(maximum direction)=vrot[0,:]\n        Vj(intermediate direction)=vrot[1,:]\n        Vk(minimum variance direction)=Vrot[2,:]\n    v:\n        an (ndim,ndim) array containing the principal axes vectors\n        Maximum variance direction eigenvector, Vi=v[*,0]\n        Intermediate variance direction, Vj=v[*,1] (descending order)\n    w:\n        the eigenvalues of the computation\n    \"\"\"\n\n    #  Min var starts here\n    # data must be Nx3\n    vecavg = np.nanmean(np.nan_to_num(data, nan=0.0), axis=0)\n\n    mvamat = np.zeros((3, 3))\n    for i in range(3):\n        for j in range(3):\n            mvamat[i, j] = np.nanmean(np.nan_to_num(data[:, i] * data[:, j], nan=0.0)) - vecavg[i] * vecavg[j]\n\n    # Calculate eigenvalues and eigenvectors\n    w, v = np.linalg.eigh(mvamat, UPLO='U')\n\n    # Sorting to ensure descending order\n    w = np.abs(w)\n    idx = np.flip(np.argsort(w))\n\n    # IDL compatability\n    if True:\n        if np.sum(w) == 0.0:\n            idx = [0, 2, 1]\n\n    w = w[idx]\n    v = v[:, idx]\n\n    # Rotate intermediate var direction if system is not Right Handed\n    YcrossZdotX = v[0, 0] * (v[1, 1] * v[2, 2] - v[2, 1] * v[1, 2])\n    if YcrossZdotX &lt; 0:\n        v[:, 1] = -v[:, 1]\n        # v[:, 2] = -v[:, 2] # Should not it is being flipped at Z-axis?\n\n    # Ensure minvar direction is along +Z (for FAC system)\n    if v[2, 2] &lt; 0:\n        v[:, 2] = -v[:, 2]\n        v[:, 1] = -v[:, 1]\n\n    vrot = np.array([np.dot(row, v) for row in data])\n\n    return vrot, v, w\n\n\n                WARNING  UserWarning: potentially wrong      warnings.py:109\n                         underline length...                                \n                         Parameters                                         \n                         ----------- in                                     \n                         see `pyspedas.cotrans.minvar`                      \n                         This program computes the principal                \n                         variance directions and variances                  \n                         of a...                                            \n                                                                            \n\nsource\n\n\nminvar\n\n minvar (data)\n\nsee pyspedas.cotrans.minvar This program computes the principal variance directions and variances of a vector quantity as well as the associated eigenvalues.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\n\n\n\n\nReturns\nvrot:\nan array of (npoints, ndim) containing the rotated data in the new coordinate system, ijk.Vi(maximum direction)=vrot[0,:]Vj(intermediate direction)=vrot[1,:]Vk(minimum variance direction)=Vrot[2,:]\n\n\n\n\nsource\n\n\nmva_features\n\n mva_features (data:numpy.ndarray)\n\nCompute MVA features based on the given data array.\nParameters: - data (np.ndarray): Input data\nReturns: - List: Computed features\n\nTest\n\n\nCode\nfrom fastcore.test import *\n\n# Generate synthetic data\nnp.random.seed(42)  # for reproducibility\ndata = np.random.rand(100, 3)  # 100 time points, 3-dimensional data\n# Call the mva_features function\nfeatures = mva_features(data)\n_features = [0.3631060892452051, 0.8978455426527485, -0.24905290500542857, 0.09753158579102299, 0.086943767300213, 0.07393142040422575, 1.1760056390752571, 0.9609421690770317, 0.6152039820297959, -0.5922397773398479, 0.6402091632847049, 0.61631157045453, 1.2956351134759623, 0.19091785005728523, 0.5182488424049534, 0.4957624347593598]\ntest_eq(features, _features)\n\n\n\n\n\nField rotation angles\nThe PDF of the field rotation angles across the solar-wind IDs is well fitted by the exponential function exp(−θ/)…\n\nsource\n\n\ncalc_candidate_rotation_angle\n\n calc_candidate_rotation_angle (candidates,\n                                data:xarray.core.dataarray.DataArray)\n\nComputes the rotation angle(s) at two different time steps.\n\nsource\n\n\ncalc_rotation_angle\n\n calc_rotation_angle (v1:numpy.ndarray, v2:numpy.ndarray)\n\nComputes the rotation angle between two vectors.\nParameters: - v1: The first vector. - v2: The second vector."
  },
  {
    "objectID": "00_ids_finder.html#processing-the-whole-dataset",
    "href": "00_ids_finder.html#processing-the-whole-dataset",
    "title": "Finding magnetic discontinuities",
    "section": "Processing the whole dataset",
    "text": "Processing the whole dataset\n\nsource\n\nfilter_indices\n\n filter_indices\n                 (df:polars.dataframe.frame.DataFrame|polars.lazyframe.fra\n                 me.LazyFrame, index_std_threshold=2,\n                 index_fluc_threshold=1, index_diff_threshold=0.1,\n                 sparse_num=15)\n\npatch pdp.ApplyToRows to work with modin and xorbits DataFrames\n\nsource\n\n\ncalc_candidate_mva_features\n\n calc_candidate_mva_features (candidate,\n                              data:xarray.core.dataarray.DataArray)\n\n\nsource\n\n\nconvert_to_dataframe\n\n convert_to_dataframe\n                       (data:polars.dataframe.frame.DataFrame|polars.lazyf\n                       rame.frame.LazyFrame)\n\nconvert data into a pandas/modin DataFrame\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\npolars.dataframe.frame.DataFrame | polars.lazyframe.frame.LazyFrame\norignal Dataframe\n\n\nReturns\nDataFrame\n\n\n\n\nPipelines Class for processing IDs\n\nsource\n\n\nIDsPipeline\n\n IDsPipeline ()\n\nInitialize self. See help(type(self)) for accurate signature.\nNotes that the candidates only require a small portion of the data so we can compress the data to speed up the processing.\n\nsource\n\n\ncompress_data_by_cands\n\n compress_data_by_cands (data:polars.dataframe.frame.DataFrame,\n                         candidates:polars.dataframe.frame.DataFrame,\n                         tau:datetime.timedelta)\n\nCompress the data for parallel processing\n\nsource\n\n\nprocess_candidates\n\n process_candidates (candidates_pl:polars.dataframe.frame.DataFrame,\n                     sat_fgm:xarray.core.dataarray.DataArray,\n                     data_resolution:datetime.timedelta)\n\nProcess candidates DataFrame\n\n\n\n\nType\nDetails\n\n\n\n\ncandidates_pl\nDataFrame\npotential candidates DataFrame\n\n\nsat_fgm\nDataArray\nsatellite FGM data\n\n\ndata_resolution\ntimedelta\ntime resolution of the data\n\n\nReturns\nDataFrame\n\n\n\n\n\nsource\n\n\nsort_df\n\n sort_df (df:polars.dataframe.frame.DataFrame, col='time')"
  },
  {
    "objectID": "00_ids_finder.html#pipeline",
    "href": "00_ids_finder.html#pipeline",
    "title": "Finding magnetic discontinuities",
    "section": "Pipeline",
    "text": "Pipeline\n\nsource\n\nextract_features\n\n extract_features (partitioned_input:Dict[str,Callable], tau:float,\n                   params)\n\n\nsource\n\n\nids_finder\n\n ids_finder (data:polars.lazyframe.frame.LazyFrame, tau:float,\n             params:dict)"
  },
  {
    "objectID": "00_ids_finder.html#test-1",
    "href": "00_ids_finder.html#test-1",
    "title": "Finding magnetic discontinuities",
    "section": "Test",
    "text": "Test\nGenerally mapply and modin are the fastest. xorbits is expected to be the fastest but it is not and it is the slowest one.\n\n\nCode\nsat = 'jno'\ncoord = 'se'\ncols = [\"BX\", \"BY\", \"BZ\"]\ntau = timedelta(seconds=60)\ndata_resolution = timedelta(seconds=1)\n\nif True:\n    year = 2012\n    files = f'../data/{sat}_data_{year}.parquet'\n    output = f'../data/{sat}_candidates_{year}_tau_{tau.seconds}.parquet'\n\n    data = pl.scan_parquet(files).set_sorted('time').collect()\n\n    indices = compute_indices(data, tau)\n    # filter condition\n    sparse_num = tau / data_resolution // 3\n    filter_condition = filter_indices(sparse_num = sparse_num)\n\n    candidates = indices.filter(filter_condition).with_columns(pl_format_time(tau)).sort('time')\n    \n    data_c = compress_data_by_cands(data, candidates, tau)\n    sat_fgm = df2ts(data_c, cols, attrs={\"units\": \"nT\"})\n\n\n\nTest parallelization\n\n\nCode\ncandidates_pd = candidates.to_pandas()\ncandidates_modin = mpd.DataFrame(candidates_pd)\n# candidates_x = xpd.DataFrame(candidates_pd)\n\n\n\n\nTest different libraries to parallelize the computation\nif True:\n    pdp_test = pdp.ApplyToRows(\n        lambda candidate: calc_candidate_duration(candidate, sat_fgm),  # fast a little bit\n        # lambda candidate: calc_duration(get_candidate_data_xr(candidate, sat_fgm)),\n        # lambda candidate: calc_duration(sat_fgm.sel(time=slice(candidate['tstart'], candidate['tstop']))),\n        func_desc=\"calculating duration parameters\",\n    )\n    \n    # process_candidates(candidates_modin, sat_fgm, sat_state, data_resolution)\n    \n    # ---\n    # successful cases\n    # ---\n    # candidates_pd.mapply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works, 4.2 secs\n    # candidates_pd.mapply(calc_candidate_duration, axis=1, data=sat_fgm) # this works, but a little bit slower, 6.7 secs\n    \n    # candidates_pd.apply(calc_candidate_duration, axis=1, data=sat_fgm) # Standard case: 24+s secs\n    # candidates_pd.swifter.apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 80 secs\n    # candidates_pd.swifter.set_dask_scheduler(scheduler=\"threads\").apply(calc_candidate_duration, axis=1, data=sat_fgm) # this works with dask, 60 secs\n    # candidates_modin.apply(lambda candidate: calc_candidate_duration(candidate, sat_fgm), axis=1) # this works with ray, 6 secs # NOTE: can not work with dask\n    # candidates_x.apply(calc_candidate_duration, axis=1, data=sat_fgm) # 30 seconds\n    # pdp_test(candidates_modin) # this works, 8 secs\n    \n    # ---\n    # failed cases\n    # ---\n    # candidates_modin.apply(calc_candidate_duration, axis=1, data=sat_fgm) # AttributeError: 'DataFrame' object has no attribute 'sel'\n\n\n\n\nTest feature engineering\n\n\nCode\n# from tsflex.features import MultipleFeatureDescriptors, FeatureCollection\n\n# from tsflex.features.integrations import catch22_wrapper\n# from pycatch22 import catch22_all\n\n\n\n\nCode\n# tau_pd = pd.Timedelta(tau)\n\n# catch22_feats = MultipleFeatureDescriptors(\n#     functions=catch22_wrapper(catch22_all),\n#     series_names=bcols,  # list of signal names\n#     windows = tau_pd, strides=tau_pd/2,\n# )\n\n# fc = FeatureCollection(catch22_feats)\n# features = fc.calculate(data, return_df=True)  # calculate the features on your data\n\n\n\n\nCode\n# features_pl = pl.DataFrame(features.reset_index()).sort('time')\n# df = candidates_pl.join_asof(features_pl, on='time').to_pandas()\n\n\n\n\nCode\n# profile = ProfileReport(df, title=\"JUNO Candidates Report\")\n# profile.to_file(\"jno.html\")\n\n\n\n\nBenchmark\n\n\nCode\nimport timeit\n\n\n\n\nCode\ndef benchmark(task_dict, number=1):\n    results = {}\n    for name, (data, task) in task_dict.items():\n        try:\n            time_taken = timeit.timeit(\n                lambda: task(data),\n                number=number\n            )\n            results[name] = time_taken / number\n        except Exception as e:\n            results[name] = str(e)\n    return results\n\n\n\n\nCode\nfunc = lambda candidate: calc_candidate_duration(candidate, sat_fgm)\ntask_dict = {\n    'pandas': (candidates_pd, lambda _: _.apply(func, axis=1)),\n    'pandas-mapply': (candidates_pd, lambda _: _.mapply(func, axis=1)),\n    'modin': (candidates_modin, lambda _: _.apply(func, axis=1)),\n    # 'xorbits': (candidates_x, lambda _: _.apply(func, axis=1)),\n}\n\nresults = benchmark(task_dict)"
  },
  {
    "objectID": "00_ids_finder.html#notes",
    "href": "00_ids_finder.html#notes",
    "title": "Finding magnetic discontinuities",
    "section": "Notes",
    "text": "Notes\n\nTODOs\n\nFeature engineering\nFeature selection"
  },
  {
    "objectID": "00_ids_finder.html#obsolete-codes",
    "href": "00_ids_finder.html#obsolete-codes",
    "title": "Finding magnetic discontinuities",
    "section": "Obsolete codes",
    "text": "Obsolete codes\nThis is obsolete codes because the timewindow now is overlapping. No need to consider where magnetic discontinuities happens in the boundary of one timewindow.\n\n\nCode\ndef calc_candidate_d_duration(candidate, data) -&gt; pd.Series:\n    try:\n        if pd.isnull(candidate['d_tstart']) or pd.isnull(candidate['d_tstop']):\n            candidate_data = get_candidate_data(candidate, data, neighbor=1)\n            d_time = candidate['d_time']\n            threshold = candidate['threshold']\n            return calc_d_duration(candidate_data, d_time, threshold)\n        else:\n            return pandas.Series({\n                'd_tstart': candidate['d_tstart'],\n                'd_tstop': candidate['d_tstop'],\n            })\n    except Exception as e:\n        # logger.debug(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n        print(f\"Error for candidate {candidate} at {candidate['time']}: {str(e)}\")\n        raise e\n\npdp.ApplyToRows(\n    lambda candidate: calc_candidate_d_duration(candidate, sat_fgm),\n    func_desc=\"calculating duration parameters if needed\"\n)\n\n\nObsolete codes for xarray related calculations.\n\n\nCode\ndef calc_vec_mean_mag(vec: xr.DataArray):\n    return linalg.norm(vec, dims=\"v_dim\").mean(dim=\"time\")\n\n\ndef calc_vec_std(vec: xr.DataArray):\n    \"\"\"\n    Computes the standard deviation of a vector.\n    \"\"\"\n    return linalg.norm(vec.std(dim=\"time\"), dims=\"v_dim\")\n\n\ndef calc_vec_relative_diff(vec: xr.DataArray):\n    \"\"\"\n    Computes the relative difference between the last and first elements of a vector.\n    \"\"\"\n    dvec = vec.isel(time=-1) - vec.isel(time=0)\n    return linalg.norm(dvec, dims=\"v_dim\") / linalg.norm(vec, dims=\"v_dim\").mean(\n        dim=\"time\"\n    )\n\n\n\nprocess_candidates\nAssign coordinates using Dataframe.apply is not optimized, quite slow…\n\n\nCode\ndef process_candidates(\n    candidates: pd.DataFrame, # potential candidates DataFrame\n    sat_fgm: xr.DataArray, # satellite FGM data\n    sat_state: xr.DataArray, # satellite state data\n    data_resolution: timedelta, # time resolution of the data\n) -&gt; pd.DataFrame: # processed candidates DataFrame\n    id_pipelines = IDsPipeline()\n\n    candidates = id_pipelines.calc_duration(sat_fgm).apply(candidates)\n\n    # calibrate duration\n    temp_candidates = candidates.loc[\n        lambda df: df[\"d_tstart\"].isnull() | df[\"d_tstop\"].isnull()\n    ]  # temp_candidates = candidates.query('d_tstart.isnull() | d_tstop.isnull()') # not implemented in `modin`\n\n    if not temp_candidates.empty:\n        candidates.update(\n            id_pipelines.calibrate_duration(sat_fgm, data_resolution).apply(\n                temp_candidates\n            )\n        )\n\n    ids = (\n        id_pipelines.calc_mva_features(sat_fgm)\n        + id_pipelines.calc_rotation_angle(sat_fgm)\n        + id_pipelines.assign_coordinates(sat_state)\n    ).apply(\n        candidates.dropna()  # Remove candidates with NaN values)\n    )\n\n    return ids"
  },
  {
    "objectID": "utils/01_plotting.html",
    "href": "utils/01_plotting.html",
    "title": "MVA plotting",
    "section": "",
    "text": "source\n\nsetup_mva_plot\n\n setup_mva_plot (data:xarray.core.dataarray.DataArray,\n                 tstart:datetime.datetime, tstop:datetime.datetime,\n                 mva_tstart:datetime.datetime=None,\n                 mva_tstop:datetime.datetime=None)\n\n\nsource\n\n\ntime_stamp\n\n time_stamp (ts)\n\nReturn POSIX timestamp as float.\n\n\nCode\ndef format_candidate_title(candidate: dict):\n    format_float = (\n        lambda x: rf\"$\\bf {x:.2f} $\" if isinstance(x, (float, int)) else rf\"$\\bf {x} $\"\n    )\n\n    base_line = rf'$\\bf {candidate.get(\"type\", \"N/A\")} $ candidate (time: {candidate.get(\"time\", \"N/A\")}) with index '\n    index_line = rf'i1: {format_float(candidate.get(\"index_std\", \"N/A\"))}, i2: {format_float(candidate.get(\"index_fluctuation\", \"N/A\"))}, i3: {format_float(candidate.get(\"index_diff\", \"N/A\"))}'\n    info_line = rf'$B_n/B$: {format_float(candidate.get(\"BnOverB\", \"N/A\"))}, $dB/B$: {format_float(candidate.get(\"dBOverB\", \"N/A\"))}, $(dB/B)_{{max}}$: {format_float(candidate.get(\"dBOverB_max\", \"N/A\"))},  $Q_{{mva}}$: {format_float(candidate.get(\"Q_mva\", \"N/A\"))}'\n    title = rf\"\"\"{base_line}\n    {index_line}\n    {info_line}\"\"\"\n    return title\n\n\n\nsource\n\n\nplot_candidate\n\n plot_candidate (candidate:dict, sat_fgm:xarray.core.dataarray.DataArray)\n\n\n\nCode\ndef plot_candidates(\n    candidates: pandas.DataFrame, candidate_type=None, num=4, plot_func=plot_candidate\n):\n    \"\"\"Plot a set of candidates.\n\n    Parameters:\n    - candidates (pd.DataFrame): DataFrame containing the candidates.\n    - candidate_type (str, optional): Filter candidates based on a specific type.\n    - num (int): Number of candidates to plot, selected randomly.\n    - plot_func (callable): Function used to plot an individual candidate.\n\n    \"\"\"\n\n    # Filter by candidate_type if provided\n    \n    candidates = get_candidates(candidates, candidate_type, num)\n\n    # Plot each candidate using the provided plotting function\n    for _, candidate in candidates.iterrows():\n        plot_func(candidate)\n\n\n\n\nCode\ndef plot_basic(\n    data: xr.DataArray, \n    tstart: datetime, \n    tstop: datetime,\n    tau: timedelta, \n    mva_tstart=None, mva_tstop=None, neighbor: int = 1\n):\n    if mva_tstart is None:\n        mva_tstart = tstart\n    if mva_tstop is None:\n        mva_tstop = tstop\n\n    mva_b = data.sel(time=slice(mva_tstart, mva_tstop))\n    store_data(\"fgm\", data={\"x\": mva_b.time, \"y\": mva_b})\n    minvar_matrix_make(\"fgm\")  # get the MVA matrix\n\n    temp_tstart = tstart - neighbor * tau\n    temp_tstop = tstop + neighbor * tau\n\n    temp_b = data.sel(time=slice(temp_tstart, temp_tstop))\n    store_data(\"fgm\", data={\"x\": temp_b.time, \"y\": temp_b})\n    temp_btotal = calc_vec_mag(temp_b)\n    store_data(\"fgm_btotal\", data={\"x\": temp_btotal.time, \"y\": temp_btotal})\n\n    tvector_rotate(\"fgm_mva_mat\", \"fgm\")\n    split_vec(\"fgm_rot\")\n    pytplot.data_quants[\"fgm_btotal\"][\"time\"] = pytplot.data_quants[\"fgm_rot\"][\n        \"time\"\n    ]  # NOTE: whenever using `get_data`, we may lose precision in the time values. This is a workaround.\n    join_vec(\n        [\n            \"fgm_rot_x\",\n            \"fgm_rot_y\",\n            \"fgm_rot_z\",\n            \"fgm_btotal\",\n        ],\n        new_tvar=\"fgm_all\",\n    )\n\n    options(\"fgm\", \"legend_names\", [r\"$B_x$\", r\"$B_y$\", r\"$B_z$\"])\n    options(\"fgm_all\", \"legend_names\", [r\"$B_l$\", r\"$B_m$\", r\"$B_n$\", r\"$B_{total}$\"])\n    options(\"fgm_all\", \"ysubtitle\", \"[nT LMN]\")\n    tstart_ts = time_stamp(tstart)\n    tstop_ts = time_stamp(tstop)\n    # .replace(tzinfo=ZoneInfo('UTC')).timestamp()\n    highlight([\"fgm\", \"fgm_all\"], [tstart_ts, tstop_ts])\n    \n    degap(\"fgm\")\n    degap(\"fgm_all\")"
  },
  {
    "objectID": "20_candidates.html",
    "href": "20_candidates.html",
    "title": "Datasets",
    "section": "",
    "text": "Code\ncatalog = load_catalog('../')\ncatalog.list()"
  },
  {
    "objectID": "20_candidates.html#combining-magnetic-field-data-and-state-data",
    "href": "20_candidates.html#combining-magnetic-field-data-and-state-data",
    "title": "Datasets",
    "section": "Combining magnetic field data and state data",
    "text": "Combining magnetic field data and state data\nWith combined dataset, we calculate additional features for each candidate.\n\\[L_{mn} = v_{mn}  T_{duration}\\]\n\\[ j_0 = (\\frac{d B}{d t})_{max} \\frac{1}{v_{mn}}\\]\n\nsource\n\ncombine\n\n combine (candidates:polars.lazyframe.frame.LazyFrame,\n          states_data:polars.lazyframe.frame.LazyFrame)\n\n\n\nCalculating additional features for the combined dataset\n\nsource\n\n\nvector_project_pl\n\n vector_project_pl (df:polars.dataframe.frame.DataFrame, v1_cols, v2_cols,\n                    name=None)\n\n\nsource\n\n\nvector_project\n\n vector_project (v1, v2, dim='v_dim')\n\n\nsource\n\n\ncompute_Alfven_current\n\n compute_Alfven_current (ldf:polars.lazyframe.frame.LazyFrame)\n\n\nsource\n\n\ncompute_Alfven_speed\n\n compute_Alfven_speed (ldf:polars.lazyframe.frame.LazyFrame)\n\n\nsource\n\n\ncompute_inertial_length\n\n compute_inertial_length (ldf:polars.lazyframe.frame.LazyFrame)\n\n\nsource\n\n\ncalc_combined_features\n\n calc_combined_features (df:polars.lazyframe.frame.LazyFrame)\n\n\n\nPipelines\n\nsource\n\n\ncombine_features\n\n combine_features (candidates:polars.lazyframe.frame.LazyFrame,\n                   states_data:polars.lazyframe.frame.LazyFrame)\n\n\nsource\n\n\ncreate_candidate_pipeline\n\n create_candidate_pipeline (sat_id, tau:int=60, ts_mag:int=1,\n                            ts_state:str='1h', **kwargs)"
  },
  {
    "objectID": "20_candidates.html#datasets",
    "href": "20_candidates.html#datasets",
    "title": "Datasets",
    "section": "Datasets",
    "text": "Datasets\nFoundational Dataset Class\n\nsource\n\nIDsDataset\n\n IDsDataset (sat_id:str, tau:datetime.timedelta,\n             ts:datetime.timedelta=datetime.timedelta(seconds=1),\n             candidates:polars.dataframe.frame.DataFrame|None=None,\n             data:polars.lazyframe.frame.LazyFrame|None=None,\n             **extra_data:Any)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model.\nExtended Dataset Class with support for kedro\n\nsource\n\n\ncIDsDataset\n\n cIDsDataset (sat_id:str, tau:datetime.timedelta,\n              ts:datetime.timedelta=datetime.timedelta(seconds=1),\n              candidates:polars.dataframe.frame.DataFrame|None=None,\n              data:polars.lazyframe.frame.LazyFrame|None=None,\n              catalog:kedro.io.data_catalog.DataCatalog,\n              or_df:polars.dataframe.frame.DataFrame|None=None,\n              or_df_normalized:polars.dataframe.frame.DataFrame|None=None,\n              **data_)\n\nCreate a new model by parsing and validating input data from keyword arguments.\nRaises ValidationError if the input data cannot be parsed to form a valid model."
  },
  {
    "objectID": "20_candidates.html#candidate-class",
    "href": "20_candidates.html#candidate-class",
    "title": "Datasets",
    "section": "Candidate class",
    "text": "Candidate class\n\nsource\n\nCandidateID\n\n CandidateID (time, df:polars.dataframe.frame.DataFrame)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\nCode\nsta_candidates_1s: pl.DataFrame = catalog.load('candidates.sta_1s')\njno_candidates_1s = catalog.load('candidates.jno_1s')\n\nsta_mag : pl.LazyFrame = catalog.load('sta.inter_mag_rtn_1s')\njno_mag : pl.LazyFrame = catalog.load('sta.inter_mag_rtn_1s')\n\n\n\n\nCode\nfrom ids_finder.utils.basic import df2ts, pmap\nfrom fastcore.utils import *\n\n\n\n\nCode\ndef plot_candidate(candidate, mag_data: pl.LazyFrame, b_cols = ['BX', 'BY', 'BZ']):\n    temp_tstart = candidate[\"tstart\"]\n    tmep_tstop = candidate[\"tstop\"]\n    tau = tmep_tstop - temp_tstart\n\n    temp_mag_data = (\n        mag_data.filter(pl.col(\"time\").is_between(temp_tstart - tau, tmep_tstop + tau))\n        .with_columns(pl.col(\"time\").dt.cast_time_unit(\"ns\"))\n        .collect()\n    )\n    \n    sat_fgm = df2ts(temp_mag_data, b_cols)\n    plot_candidate_xr(candidate, sat_fgm, tau)\n\n\n\n\nCode\nn = 3\n# list(sta_candidates_1s.sample(n).iter_rows(named=True) | pmap(plot_candidate, mag_data=sta_mag))\n\ncandidates = jno_candidates_1s.sample(n)\nlist(candidates.iter_rows(named=True) | pmap(plot_candidate, mag_data=jno_mag))"
  }
]